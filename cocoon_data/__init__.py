import os
import io
import pandas as pd
import json
import re
import uuid
import numpy as np
import matplotlib.patches as patches
from ipywidgets import *
import ipywidgets as widgets
from graphviz import Digraph
from pygments import highlight
from pygments.lexers import PythonLexer, SqlLexer, YamlLexer
from pygments.formatters import Terminal256Formatter, HtmlFormatter
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
import ast
import faiss
from tqdm import tqdm
import base64
from io import BytesIO
import chardet
import datetime
from functools import partial

import yaml
from bs4 import BeautifulSoup
from collections import OrderedDict
from collections import deque
import warnings
import copy
from pathlib import Path
from ruamel.yaml import YAML
import streamlit as st

from .database import *
from .llm import *
from .utils import *
from .extract import *
from .viz import *
from .constant import *
from .data_type import *
from .widgets import *
from .file_sys import *
from .workflow import *


icon_import = False
MAX_TRIALS = 3
LOG_MESSAGE_HISTORY = False


cocoon_comment_start = "-- COCOON BLOCK START: PLEASE DO NOT MODIFY THIS BLOCK FOR SELF-MAINTENANCE\n"
cocoon_comment_end = "-- COCOON BLOCK END\n"

cocoon_lineage_start = "[COCOON_LINEAGE_START]"
cocoon_lineage_end = "[COCOON_LINEAGE_END]"

sys.setrecursionlimit(50000)

if not cocoon_main_setting['DEBUG_MODE']:
    warnings.filterwarnings("ignore")

def yml_from_name(table_name):

    yaml_dict = {
        "version": 2,
        "models": [{
            "name": table_name
        }]
    }

    return yaml_dict


class QueryWidget:
    def __init__(self, con):
        self.con = con
        self.database_system = get_database_name(con)
        self.current_database, self.default_schema = get_default_database_and_schema(con)
        
        system_label = widgets.HTML(value=f"<b>Engine</b>: {self.database_system}")
        
        self.include_view_checkbox = widgets.Checkbox(
            value=True,
            disabled=False,
            indent=False
        )
        self.include_view_checkbox.observe(self.on_include_view_change, names='value')
        
        include_view_label = widgets.HTML(value="<b>Include Views</b>:")
        
        first_line = widgets.HBox([system_label, include_view_label, self.include_view_checkbox])
        
        database_label = widgets.HTML(value="<b>Database</b>:")
        self.database_dropdown = widgets.Dropdown(
            options=self.get_databases(),
            value=self.current_database,
            layout=widgets.Layout(width='100px')
        )
        self.database_dropdown.observe(self.on_database_change, names='value')
        
        schema_label = widgets.HTML(value="<b>Schema</b>:")
        self.schema_dropdown = widgets.Dropdown(
            options=self.get_schemas(),
            value=self.default_schema,
            layout=widgets.Layout(width='100px')
        )
        self.schema_dropdown.observe(self.on_schema_change, names='value')
        
        table_label = widgets.HTML(value="<b>Table</b>:")
        self.table_dropdown = widgets.Dropdown(
            options=self.get_tables(),
            layout=widgets.Layout(width='150px')
        )
        self.table_dropdown.observe(self.on_table_change, names='value')
        
        second_line = widgets.HBox([database_label, self.database_dropdown, 
                                    schema_label, self.schema_dropdown, 
                                    table_label, self.table_dropdown])
        
        self.dropdown_label = widgets.VBox([first_line, second_line])
        
        self.sql_input = widgets.Textarea(
            value='SELECT * FROM "database_name"."schema_name"."table_name"',
            disabled=False,
            layout={'width': '96%', 'height': '100px'}
        )
        
        self.sql_label = widgets.HTML(value="<b>Your query:</b>")
        self.sql_input.add_class('modern-textarea')
        
        self.context_label = widgets.HTML(value='<b>Cocoon WITH context:</b>')
        self.with_context = widgets.Textarea(
            value='',
            disabled=True,
            layout={'width': '96%', 'height': '100px'}
        )
        
        self.with_context.add_class('modern-textarea')
        
        self.error_msg = widgets.HTML(value='')
        self.result_display = widgets.HTML(layout=widgets.Layout(max_width='96%', overflow='auto'))
        self.submit_button = widgets.Button(description="Submit Query", button_style='success', icon='check')
        
        layout = Layout(display='flex', justify_content='space-between', width='96%')
        database_and_submit = HBox([self.dropdown_label, self.submit_button], layout=layout)
    
        self.vbox = VBox([
            self.context_label, self.with_context, self.sql_label, self.sql_input,
            database_and_submit, self.error_msg, self.result_display
        ])
        self.submit_button.on_click(self.run_query_from_button)
        
        self.update_context_visibility()
        self.update_sql_input()
    
    def get_databases(self):
        return list_databases(self.con)
    
    def get_schemas(self):
        return list_schemas(self.con, self.database_dropdown.value)
    
    def get_tables(self):
        tables = list_tables(self.con, self.schema_dropdown.value, self.database_dropdown.value)
        if self.include_view_checkbox.value:
            tables += list_views(self.con, self.schema_dropdown.value, self.database_dropdown.value)
        return tables
    
    def update_context_value(self, context_value=""):
        self.with_context.value = context_value
        self.update_context_visibility()
    
    def on_include_view_change(self, change):
        self.table_dropdown.options = self.get_tables()
        
    def update_context_visibility(self):
        if self.with_context.value.strip() == '':
            self.context_label.layout.display = 'none'
            self.with_context.layout.display = 'none'
        else:
            self.context_label.layout.display = ''
            self.with_context.layout.display = ''
    
    def on_database_change(self, change):
        schemas = self.get_schemas()
        self.table_dropdown.value = None
        self.table_dropdown.options = []
        self.schema_dropdown.value = None
        self.schema_dropdown.options = schemas
    
    def on_schema_change(self, change):
        selected_schema = change['new']
        if selected_schema:
            tables = self.get_tables()
            self.table_dropdown.value = None
            self.table_dropdown.options = tables
            
        else:
            self.table_dropdown.value = None
            self.table_dropdown.options = []
            
    
    def on_table_change(self, change):
        selected_table = change['new']
        if selected_table:
            self.update_sql_input()
            self.update_context_value()
            self.run_query_from_button(None)
    
    def update_sql_input(self):
        selected_database = self.database_dropdown.value
        selected_schema = self.schema_dropdown.value
        selected_table = self.table_dropdown.value
        
        con = self.con
        
        if selected_database and selected_schema and selected_table:
            self.sql_input.value = f'SELECT * FROM {enclose_table_name(selected_database, con=con)}.{enclose_table_name(selected_schema, con=con)}.{enclose_table_name(selected_table, con=con)}'
        else:
            self.sql_input.value = 'SELECT * FROM "database_name"."schema_name"."table_name"'
   
    def run_query(self, sql_query):
        sample_size = 100
        self.sql_input.value = sql_query
        con = self.con
        with_context = self.with_context.value
        sql_query = sql_query.strip().rstrip(';')
        sample_sql_query = sample_query(con, sql_query, sample_size)
        
        modified_query = f"{with_context} \n {sample_sql_query}"
        try:
            result = run_sql_return_df(self.con, modified_query)
            self.error_msg.value = f"<div style='color: green;'>The query was successful. We only show the first {sample_size} rows.</div>"
            self.result_display.value = f'<div style="width: 80%; min-width: 0px; max-height: 400px;">{result.to_html()}</div>'
        except Exception as e:
            self.error_msg.value = f"<div style='color: red;'>{str(e)}</div>"
            self.result_display.value = ''
   
    def run_query_from_button(self, b):
        self.run_query(self.sql_input.value)
   
    def display(self):
        display(self.vbox)
        




def create_data_type_grid(df, all_data_types = None):
    
    if all_data_types is None:
        type_domain = list(data_types.keys())
    else:
        type_domain = all_data_types

    def update_match_status(change, match_status_label, current_type):
        match_status_label.value = "✔️ Yes" if change['new'] == current_type else "❌ No"

    grid = widgets.GridspecLayout(len(df) + 1, 4)

    grid[0, 0] = widgets.Label('Column')
    grid[0, 1] = widgets.Label('Current Type')
    grid[0, 2] = widgets.Label('Target Type')
    grid[0, 3] = widgets.Label('Matched?')

    for i, row in df.iterrows():
        column_name_label = widgets.Label(row['column_name'])
        current_type_label = widgets.Label(row['current_type'])
        target_type_dropdown = widgets.Dropdown(options=type_domain, value=row['target_type'], description='')
        match_status_label = widgets.Label(value="✔️ Yes" if row['current_type'] == row['target_type'] else "❌ No")

        target_type_dropdown.observe(
            lambda change, current_type=row['current_type'], match_status_label=match_status_label: update_match_status(change, match_status_label, current_type), 
            names='value'
        )
        grid[i + 1, 0] = column_name_label
        grid[i + 1, 1] = current_type_label
        grid[i + 1, 2] = target_type_dropdown
        grid[i + 1, 3] = match_status_label

    return grid

def extract_grid_data_type(grid):

    data = []
    for i in range(1, grid.n_rows):
        column_name = grid[i, 0].value
        current_type = grid[i, 1].value
        target_type = grid[i, 2].value  
        matched = grid[i, 3].value
        
        data.append([column_name, current_type, target_type, matched])
        
    df = pd.DataFrame(data, columns=["Column", "Current Type", "Target Type", "Matched?"])
    return df







def create_html_content_project(title, descriptions, page_no):
    nodes = []
    edges = []
    edge_labels = []

    list_of_descriptions = "<ol>"

    for i in range(page_no):
        description, ers = descriptions[i]
        list_of_descriptions += f"<li>{description}</li>"

        for er in ers:
            node_1, edge, node2 = er

            if node_1 not in nodes:
                nodes.append(node_1)

            if node2 not in nodes:
                nodes.append(node2)

            node_1_idx = nodes.index(node_1)
            node_2_idx = nodes.index(node2)

            edges.append((node_1_idx, node_2_idx))
            edge_labels.append(edge)
    
    description, ers = descriptions[page_no]
    list_of_descriptions += f"<li><b>{description}</b></li>"
    list_of_descriptions += "</ol>"

    highlight_nodes_indices = []
    highlight_edges_indices = []

    for er in ers:
        node_1, edge, node2 = er

        if node_1 not in nodes:
            nodes.append(node_1)

        if node2 not in nodes:
            nodes.append(node2)

        node_1_idx = nodes.index(node_1)
        node_2_idx = nodes.index(node2)

        highlight_nodes_indices.append(node_1_idx)
        highlight_nodes_indices.append(node_2_idx)

        edges.append((node_1_idx, node_2_idx))
        edge_labels.append(edge)
        highlight_edges_indices.append(len(edges) - 1)


    html_content = f"""
    <h3>{title}</h3>
    {list_of_descriptions}
    {generate_workflow_html_multiple(nodes, edges, edge_labels, highlight_nodes_indices, highlight_edges_indices)}
    """
    return html_content




doc_steps = ["Table", "Column", "Missing", "Tests"]
geo_integration_steps = ["Process", "CRS", "Integration"] 
model_steps = ["DW conn", "Clean", "Integration", "Model"]
transform_steps = ["Table Understand", "Table Transform"]
migration_steps = ["DW conn", "Model", "Match", "Execute"]



def create_tabs_with_notifications(tab_data):
    def tab_title_with_emoji_notification(title, notification_count):
        no_break_space = '\u00A0'
        return f"{title}{no_break_space}⚠️{notification_count}"

    tab_children = []
    for title, notifications, content in tab_data:
        tab_content = widgets.HTML(value=content)
        tab_children.append(tab_content)

    tab = widgets.Tab(children=tab_children)

    for index, (title, notifications, _) in enumerate(tab_data):
        if notifications > 0:
            tab.set_title(index, tab_title_with_emoji_notification(title, notifications))
        else:
            tab.set_title(index, title)

    return tab




def create_dropdown_with_content(tab_data):
    dropdown = Dropdown(
        options=[(title, index) for index, (title, _) in enumerate(tab_data)],
    )

    display_area = widgets.HTML(value=tab_data[0][1])

    def on_dropdown_change(change):
        if change['type'] == 'change' and change['name'] == 'value':
            display_area.value = tab_data[change['new']][1]

    dropdown.observe(on_dropdown_change, names='value')

    return VBox([dropdown, display_area])



def transform_string(s):
    """Transforms a string to lowercase and removes special characters."""
    return ''.join(c for c in s if c.isalnum()).lower()

def process_lists(list1, list2):
    transformed_list1 = {s: transform_string(s) for s in list1}

    transformed_list2 = {}
    for s in list2:
        new_string = transform_string(s)
        if new_string in transformed_list2:
            raise ValueError(f"Duplicate transformation: '{transformed_list2[new_string]}' and '{s}' both transform to '{new_string}'")
        transformed_list2[s] = new_string

    for old_string, new_string in transformed_list2.items():
        if new_string not in transformed_list1.values():
            raise ValueError(f"No corresponding string in list1 for '{old_string}'")

    mapping_dict = {}
    for old1, new1 in transformed_list1.items():
        for old2, new2 in transformed_list2.items():
            if new1 == new2:
                mapping_dict[old1] = old2
                break

    return mapping_dict



def image_json_to_base64(json_var, df):
    histogram_imgs = []
    map_imgs = []

    if not isinstance(json_var, list):
        raise ValueError("json_var is not a list")

    for json_entry in json_var:
        if json_entry['name'] == 'Histogram':
            att = json_entry['params']
            histogram_imgs.append(plot_distribution(df, att))
        elif json_entry['name'] == 'Map':
            lon, lat = json_entry['params']
            map_imgs.append(plot_geospatial_data(df, lon, lat))
        else:
            raise ValueError(f"Visualization name {json_entry['name']} is not supported")

    return histogram_imgs, map_imgs
    

def generate_visualization_recommendation(table_summary):    
    template =  f"""Below are table summary, where attributes are in **bold**:
{table_summary}

Help me choose some interesting attributes to generate visualizations for.

Below are visualization APIs:

1. "Histogram"
params: "att" (**atts** are highlighted)
This shows att's distribution. Suitable for categorical attributes.

2. "Map"
params: ["lon_att", "lat_att"] (doesn't suppport address yet)
This shows the geo distribution.

Respond in the following format without reasonings:
```json
[
    {{"name": "Histogram", "params": "age"}},
    {{"name": "Map", "params": ["longitude", "latitude"]}},
    ...
]
```"""
    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']

    json_code = extract_json_code_safe(assistant_message['content'])
    json_code = json_code.replace('\'', '\"')
    json_var = json.loads(json_code)

    messages.append(assistant_message)

    return json_var, messages



def get_source_nodes_ids(edges, node):
    source_nodes = []

    for source, targets in edges.items():
        if node in targets:
            source_nodes.append(source)

    return source_nodes




def json_to_html(json_code):

    json_str = json.dumps(json_code, indent=2)
    
    html_content = f"<pre>{json_str}</pre>"
    
    return html_content




def plot_missing_values_html(df):
    missing_percent = df.isnull().mean() * 100
    missing_percent = missing_percent[missing_percent > 0]

    if len(missing_percent) == 0:
        return

    plot_width = max(4, len(missing_percent) * 0.5)
    sns.set(style="whitegrid")

    plt.figure(figsize=(plot_width, 3), dpi=100)
    bar_plot = sns.barplot(x=missing_percent.index, y=missing_percent, color="lightgreen")

    for index, value in enumerate(missing_percent):
        bar_plot.text(index, value, f'{value:.2f}%', color='black', ha="center", fontsize=8)

    plt.title('Missing % (for Columns with Missing Values)', fontsize=8)
    plt.ylabel('%', fontsize=10)
    plt.xticks(rotation=45, fontsize=6)
    plt.yticks(fontsize=8)
    plt.tight_layout()

    buffer = BytesIO()
    plt.savefig(buffer, format='png')
    plt.close()
    buffer.seek(0)
    image_png = buffer.getvalue()

    image_base64 = base64.b64encode(image_png).decode('utf-8')
    html_str = f'<img src="data:image/png;base64,{image_base64}"/>'

    return html_str


def get_tree_javascript(data):
    def process_data(data):
        keys = list(data.keys())
        if len(keys) == 0:
            return {}

        key = keys[0]
        main_data = data[key]

        children = []
        for sub_key, sub_value in main_data.items():
            if isinstance(sub_value, list):
                items_str = ','.join(map(str, sub_value))

                final_str = '[' + items_str + ']'

                leaf_node = [{"name": final_str}]
                children.append({"name": sub_key, "children": leaf_node})
            else:
                children.append(process_data({sub_key: sub_value}))

        return {"name": key, "children": children}



    def count_leaf_nodes(node):
        """
        Count the number of leaf nodes in a tree represented by nested dictionaries.

        Args:
        node (dict): The tree node, which could be the root of the tree or any sub-node within the tree.

        Returns:
        int: The total number of leaf nodes in the tree from the given node down.
        """
        if not isinstance(node, dict) or 'children' not in node or not node['children']:
            return 1

        leaf_count = 0
        for child in node['children']:
            leaf_count += count_leaf_nodes(child)

        return leaf_count

    def max_path_length(data, ch_size, gap_size):
        def calculate_length(path):
            return sum(len(step) for step in path) * ch_size + (len(path) - 1) * gap_size

        def traverse(node, path=None):
            if path is None:
                path = []
            if isinstance(node, dict):
                for key, value in node.items():
                    yield from traverse(value, path + [key])
            elif isinstance(node, list):
                for item in node:
                    yield calculate_length(path + [item])

        return max(traverse(data))

    processed_data = process_data(data)

    number_of_leaves = count_leaf_nodes(processed_data)
    
    javascript = """
    const structuredData = {{data}};

    const treeLayout = d3.tree().size([{{height1}}, {{width1}}]);

    const root = d3.hierarchy(structuredData);
    root.x0 = 0;
    root.y0 = 0;

    const svg = d3.select('#tree').append('svg')
        .attr('width',  {{width2}})
        .attr('height', {{height2}});

    const g = svg.append('g')
        .attr('transform', 'translate(50,50)');

    update(root);

    function update(source) {
        const duration = 750;

        // Re-compute the tree layout
        treeLayout(root);

        // Determine the leftmost node's y-position to adjust the tree positioning
        let leftmostNode = root;
        root.each(d => {
            if (d.y < leftmostNode.y) {
                leftmostNode = d;
            }
        });
        const shiftX = -leftmostNode.y + 50;

        const nodes = root.descendants();
        const links = root.links();

        const node = g.selectAll('.node')
            .data(nodes, d => d.id || (d.id = Math.random()));

        const nodeEnter = node.enter().append('g')
            .attr('class', 'node')
            .attr('transform', d => `translate(${source.y0 + shiftX},${source.x0})`)
            .on('click', click);

        nodeEnter.append('circle')
            .attr('r', 1e-6)
            .style('fill', d => d._children ? 'lightgreen' : '#fff');

        nodeEnter.append('text')
            .attr('dy', '0.35em')
            .attr('x', d => d.children || d._children ? -13 : 13)
            .style('text-anchor', d => d.children || d._children ? 'end' : 'start')
            .text(d => d.data.name);

        const nodeUpdate = nodeEnter.merge(node);

        nodeUpdate.transition()
            .duration(duration)
            .attr('transform', d => `translate(${d.y + shiftX},${d.x})`);

        nodeUpdate.select('circle')
            .attr('r', 5)
            .style('fill', d => d._children ? 'lightgreen' : '#fff');

        const nodeExit = node.exit().transition()
            .duration(duration)
            .attr('transform', d => `translate(${source.y + shiftX},${source.x})`)
            .remove();

        nodeExit.select('circle')
            .attr('r', 1e-6);

        nodeExit.select('text')
            .style('fill-opacity', 1e-6);

        const link = g.selectAll('.link')
            .data(links, d => d.target.id);

        const linkEnter = link.enter().insert('path', 'g')
            .attr('class', 'link')
            .attr('d', d => {
                const o = { x: source.x0, y: source.y0 };
                return diagonal(o, o);
            });

        const linkUpdate = linkEnter.merge(link);

        linkUpdate.transition()
            .duration(duration)
            .attr('d', d => diagonal(d.source, d.target));

        link.exit().transition()
            .duration(duration)
            .attr('d', d => {
                const o = { x: source.x, y: source.y };
                return diagonal(o, o);
            })
            .remove();


        // Update the positional attributes of each node for the next transition.
        nodes.forEach(d => {
            d.x0 = d.x;
            d.y0 = d.y;
        });

        function diagonal(s, d) {
            const path = `M ${s.y + shiftX} ${s.x}
                C ${(s.y + d.y) / 2 + shiftX} ${s.x},
                ${(s.y + d.y) / 2 + shiftX} ${d.x},
                ${d.y + shiftX} ${d.x}`;

            return path;
        }

    }

    function click(event, d) {
        if (d.children) {
            d._children = d.children;
            d.children = null;
        } else {
            d.children = d._children;
            d._children = null;
        }
        update(d);
    }"""
    
    processed_data_str = json.dumps(processed_data, indent=4)

    height1 = number_of_leaves * 30
    height2 = number_of_leaves * 35 + 50

    width1 = max_path_length(data, 5, 20)
    width2 = max_path_length(data, 10, 50) + 150
    
    html_content_updated = javascript.replace('{{data}}', processed_data_str)
    html_content_updated = html_content_updated.replace('{{height1}}', str(height1))
    html_content_updated = html_content_updated.replace('{{height2}}', str(height2))
    html_content_updated = html_content_updated.replace('{{width1}}', str(width1))
    html_content_updated = html_content_updated.replace('{{width2}}', str(width2))
    
    return html_content_updated, width2, height2
    

def get_tree_html(data):
    javascript_content, width2, height2 = get_tree_javascript(data)

    html_content = f"""<div id="tree" style="overflow: scroll;"></div>
<script src="https://d3js.org/d3.v6.min.js"></script>
<style>
    .link {{
        fill: none;
        stroke: #555;
        stroke-opacity: 0.4;
        stroke-width: 1.5px;
    }}

    .node {{
        cursor: pointer;
    }}

    .node circle {{
        fill: #999;
        stroke: forestgreen;
        stroke-width: 1.5px;
    }}

    .node text {{
        font: 12px sans-serif;
        fill: #555;
    }}
</style>
<script>
{javascript_content}
</script>"""  

    return html_content, width2, height2




def get_rename(meanings):

    column_meanings = '\n'.join([f"  \"{m['column']}\": {m['meaning']}" for m in meanings])

    template = f"""{column_meanings}

    Analyze the column names. The final result as a json file:

    ```json
    [{{
    "column": "column_name" (case sensitive),
    "ambiguous": Given the meaning, is the column name ambiguous? Note that it's fine for the name to be domain specific.
    "rename": "" (empty string if not ambiguous)
    }},...]```"""

    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("---------------------")

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_code = replace_newline(json_code)
    json_code = json.loads(json_code)
            
    def verify_rename(meanings, json_code):

        meanings_columns = set([m['column'] for m in meanings])
        json_code_columns = set([j['column'] for j in json_code])
        if meanings_columns != json_code_columns:
            raise ValueError(f"The set of column names in the meanings and json_code are different: {meanings_columns} vs {json_code_columns}") 

    verify_rename(meanings, json_code)

    return json_code, messages


def topological_sort(nodes, edges):
    in_degree = {i: 0 for i in range(len(nodes))}
    for targets in edges.values():
        for target in targets:
            in_degree[target] += 1

    queue = [i for i in range(len(nodes)) if in_degree[i] == 0]

    order = []

    while queue:
        node_idx = queue.pop(0)
        order.append(node_idx)

        for target_idx in edges.get(node_idx, []):
            in_degree[target_idx] -= 1
            if in_degree[target_idx] == 0:
                queue.append(target_idx)

    if len(order) != len(nodes):
        return "Cycle detected in the graph, topological sort not possible."

    return order



def show_progress(max_value=1, value=1):
    progress = widgets.IntProgress(
        value=value,
        min=0,
        max=max_value+1,  
        step=1,
        description='',
        bar_style='',
        orientation='horizontal'
    )
    
    display(progress)
    return progress


def give_title(task):
    template = f"""Rename the below task into a title that's as short as possible:
{task}"""

    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    title = response['choices'][0]['message']['content']
    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)
    
    for message in messages:
        write_log(message['content'])
        write_log("---------------------")

    return title, messages


def classify_unusual_values(col, unique_values, reason):

    template = f"""Issue: In the '{col}' column, some values are unusual.
Data Values: {unique_values}
Reason: {reason}
Goal: Classify unusual and normal values.

Return the results in json format:
```json
{{
    "reasoning": "The unusual values are ...",
    "unusual_values": ["unusual value",...], (could be empty)
    "normal_values": ["normal value",...] (could be empty)
}}```"""

    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("---------------------")


    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_code = replace_newline(json_code)
    json_code = json.loads(json_code)


    def verify_result(json_code, unique_values):

        unique_values = [str(v) for v in unique_values]

        assert json_code['reasoning'] is not None
        assert json_code['unusual_values'] is not None
        assert json_code['normal_values'] is not None

        assert isinstance(json_code['reasoning'], str)
        assert isinstance(json_code['unusual_values'], list)
        assert isinstance(json_code['normal_values'], list)

        assert len(set(json_code['unusual_values']).intersection(set(json_code['normal_values']))) == 0

        json_codes_all = json_code['unusual_values'] + json_code['normal_values']
        json_codes_all = [str(v) for v in json_codes_all]

        assert len(set(unique_values).difference(set(json_codes_all))) == 0 

    verify_result(json_code, unique_values)

    return json_code

def find_regex_pattern(col, result):

    template = f"""Issue: In the '{col}' column, some values are unusual.
Unusual Values: {result['unusual_values']}
Normal Values: {result['normal_values']}

Goal: Identify if there is regular expression that
(1) matches all the normal values, and
(2) does not match any of the unusual values.

There could be multiple regex patterns that satisfy the goal. 
Please understand the meaning of the column, and find one general regex pattern.

Return the results in json format:
```json
{{
"reasoning": "The patterns in the normal values are ...",
"exists_regex": true/false,
"regex": "..."
}}```"""

    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)


    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("---------------------")

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_code = replace_newline(json_code)
    json_code = json.loads(json_code)



    def verify_json(json_code):
        fields = ['reasoning', 'exists_regex', 'regex']
        for field in fields:
            if field not in json_code:
                raise ValueError(f"Missing field '{field}' in json code.")

        if not isinstance(json_code['reasoning'], str):
            raise ValueError(f"Field 'reasoning' should be string.")
        if not isinstance(json_code['exists_regex'], bool):
            raise ValueError(f"Field 'exists_regex' should be boolean.")
        if not isinstance(json_code['regex'], str):
            raise ValueError(f"Field 'regex' should be string.")

    verify_json(json_code)
    
    if json_code['exists_regex']:

        def verify_regex(regex, normal_values, unusual_values):
            regex = re.compile(regex)
            for value in normal_values:
                if not regex.match(value):
                    raise ValueError(f"Regex does not match normal value: {value}")
            for value in unusual_values:
                if regex.match(value):
                    raise ValueError(f"Regex matches unusual value: {value}")
                
        verify_regex(json_code["regex"], result['normal_values'], result['unusual_values'])

    return json_code



def plot_distribution(df, column_name):
    """
    This function takes a pandas DataFrame and a column name as input.
    It plots a suitable visualization and returns it as a base64-encoded PNG string.
    """
    if column_name not in df.columns:
        raise ValueError(f"Column '{column_name}' not found in the DataFrame.")
        return

    if df[column_name].isnull().all():
        return

    if pd.api.types.is_numeric_dtype(df[column_name]):
        plt.figure(figsize=(4, 2))

        hist_plot = sns.histplot(df[column_name], bins=min(10, len(df[column_name].dropna().unique())), 
                                 kde=False, color="lightgreen")

        hist_plot.xaxis.set_major_formatter(ticker.EngFormatter())

        for p in hist_plot.patches:
            height = p.get_height()
            plt.text(p.get_x() + p.get_width() / 2., height, f'{int(height)}', ha='center', va='bottom', fontsize=8)

        plt.ylabel('Frequency')
        plt.xlabel(column_name)
    else:
        top_categories = df[column_name].value_counts().head(10)
        others_count = df[column_name].value_counts()[10:].sum()

        if others_count > 0:
            other_values_series = pd.Series([others_count], index=['Other Values'])
            top_categories = pd.concat([top_categories, other_values_series])

        plt.figure(figsize=(4, 2))
        
        top_category_str = top_categories.index.astype(str)

        bar_plot = sns.barplot(y=top_category_str, x=top_categories, color="lightgreen", edgecolor="black")

        if 'Other Values' in top_categories:
            bar_plot.patches[-1].set_facecolor('orange')

        for index, value in enumerate(top_categories):
            bar_plot.text(value, index, f'{value}', color='black', va='center', fontsize=8)

        plt.xlabel('Frequency')
        plt.ylabel(column_name)

    plt.tight_layout()
    
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')

    return image_base64


def display_html_iframe(html_content, width="100%", height=None):

    encoded_html = base64.b64encode(html_content.encode()).decode()

    data_uri = f"data:text/html;base64,{encoded_html}"
    
    display(IFrame(src=data_uri, width=width, height=height))


def collect_df_statistics_(df, document=None, sample_distinct=20):

    stats = {}

    for col in df.columns:
        df_col = df[col]
        if isinstance(df_col, pd.DataFrame):
            df_col = df_col.iloc[:, 0]

        dtype = df_col.dtype
        null_percentage = round(df_col.isnull().mean() * 100, 2)

        if np.issubdtype(dtype, np.number):
            unique_values = df_col.nunique()

            hist, bin_edges = np.histogram(df_col.dropna(), bins=sample_distinct)
            histogram = {
                f"[{bin_edges[i]:.2f} - {bin_edges[i + 1]:.2f}]": hist[i] for i in range(sample_distinct)
            }

            stats[col] = {
                "dtype": str(dtype),
                "null_percentage": null_percentage,
                "unique_values": unique_values,
                "histogram": histogram
            }

        else:
            top_values = df_col.value_counts().head(sample_distinct).to_dict()
            other_count = df_col.nunique() - len(top_values)
            if other_count > 0:
                top_values["other"] = other_count

            stats[col] = {
                "dtype": str(dtype),
                "null_percentage": null_percentage,
                "histogram": top_values,
                "unique_values": df_col.nunique()
            }

    if document is not None:
        if "data_type" in document:
            
            for col in df.columns:
                if col in document["data_type"]:
                    data_type = document["data_type"][col]['data_type']
                    stats[col]['data_type'] = data_type

    return stats

def collect_df_statistics(documentdf, sample_distinct=20):
    return collect_df_statistics_(df=documentdf.df, document=documentdf.document, sample_distinct=sample_distinct)


def describe_column(stats, col_name, mention_missing=False, k=10):
    if not col_name in stats:
        raise ValueError(f"Column {col_name} not found in stats.")

    column_stats = stats[col_name]
    description = []

    if pd.api.types.is_numeric_dtype(column_stats["dtype"]) and not pd.api.types.is_bool_dtype(column_stats["dtype"]):
        if "data_type" in column_stats:
            description.append(f"'{col_name}': {column_stats['data_type']}")
        else:
            description.append(f"'{col_name}': numerical")

        min_val = min([float(i.split(' - ')[0][1:]) for i in column_stats["histogram"].keys()])
        max_val = max([float(i.split(' - ')[1][:-1]) for i in column_stats["histogram"].keys()])
        
        description.append(f" with range [{min_val}, {max_val}]")
        

    else:
        if "data_type" in column_stats:
            description.append(f"'{col_name}': {column_stats['data_type']} ")
        else:
            description.append(f"'{col_name}': categorical ")
        num_unique = column_stats['unique_values']
        description.append(f"with {num_unique} unique value{'s' if num_unique > 1 else ''}")
        
        histogram = column_stats["histogram"]
        top_k = 	sorted(histogram.items(), key=lambda x: x[1], reverse=True)[:k]
        top_k_values = ["'" + str(k[0]) + "'" for k in top_k if k[0] != "other"]
        
        if k >= column_stats['unique_values']:
            description.append(f" {', '.join(top_k_values)}.")
        else:
            description.append(f". E.g., {', '.join(top_k_values)}...")

    if mention_missing:
        null_percentage = column_stats['null_percentage']
        if null_percentage > 0:
            description.append(f"% Missing: {null_percentage}%")
            description.append(f"Contains missing values.")
        else:
            description.append(f"No missing values.")
    
    return "".join(description)

def describe_missing_values(stats, df, threshold=50):
    description = ""
    for col_name in df.columns:
        if stats[col_name]['null_percentage'] > threshold:
            description +=  f"{col_name}: {stats[col_name]['null_percentage']}% missing values\n"
    return description



def describe_df_in_natural_language(df, table_name, num_rows_to_show, num_cols_to_show=None):
    num_rows = len(df)
    num_columns = len(df.columns)
    if table_name != "" and table_name is not None:
        description = f"The table '{table_name}' has {num_rows} rows and {num_columns} columns.\n"
    else:
        description = f"The table has {num_rows} rows and {num_columns} columns.\n"

    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)

    if num_cols_to_show is None:
        num_cols_to_show = num_columns

    if num_rows_to_show > 0:
        first_rows_not_null = df[df.notnull().any(axis=1)]

        first_rows = first_rows_not_null.iloc[:num_rows_to_show, :num_cols_to_show]

        first_rows_str = first_rows.to_csv(index=False, quoting=1) 
        description += f"Here are the first {num_rows_to_show} rows:\n{first_rows_str}"
    else:
        description += f"Here are the columns:\n{df.columns.to_list()}"
    
    return description

def replace_asterisks_with_tags(text):
    """
    Replaces all occurrences of words enclosed in double asterisks with the same words enclosed in <u></u> tags.
    """
    import re

    pattern = r'\*\*(.*?)\*\*'
    replaced_text = re.sub(pattern, r'<u>\1</u>', text)

    return replaced_text


def get_table_summary(main_entity, table_sample):
    """
    Generates a summary of the table.
    """
    template = f"""The table is about {main_entity}.
{table_sample} 

- Task: Concisely summarize the table.
-  Highlight: Include and highlight ALL attributes as **Attribute**. 
-  Structure: Start with the big picture, then explain how attributes are related to the big picture.
-  Requirement: ALL attributes must be mentioned and **highlighted**. The attribute name should be exactly the same (case sensitive and no extra space or _).

Example: The table is about ... at **Time**, in **Location**..."""

    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    summary = response['choices'][0]['message']['content']
    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summary, messages

def find_target_table(source_table_description, target_database_description="""The database primarily focuses on healthcare data, structured around several interconnected entities. The central entity is the **PATIENT** table, which contains details about individuals receiving medical care. Their healthcare journey is tracked through the **VISIT_OCCURRENCE** table, which records each visit to a healthcare facility. The **CONDITION_OCCURRENCE** table details any diagnosed conditions during these visits, while the **DRUG_EXPOSURE** table captures information on medications prescribed to the patients.
Procedures performed are logged in the **PROCEDURE_OCCURRENCE** table, and any medical devices used are listed in the **DEVICE** table. The **MEASUREMENT** table records various clinical measurements taken, and the **OBSERVATION** table notes any other relevant clinical observations.
In cases where a patient passes away, the **DEATH** table provides information on the mortality. The **SPECIMEN** table tracks biological samples collected for analysis, and the **COST** table details the financial aspects of the healthcare services.
The **LOCATION**, **CARE_SITE**, and **PROVIDER** tables offer contextual data, respectively detailing the geographical locations, healthcare facilities, and medical professionals involved in patient care. Lastly, the **PAYER_PLAN_PERIOD** table provides information on the patients' insurance coverage details and durations.
"""):
    template = f"""You have a source table. All its attributes are highlighted in **bold**.
{source_table_description}

You have a target database, with tables highlighted in **bold**.
{target_database_description}
Goal: transform source table to target tables.
First repeat source table attributes and reason if they can be mapped to target tables
Then, concisely summarize the targt tables (case sensitive) that can be directly mapped to, and high level reasons (not detailed attributes) in json:
```json
{{  "target table": "xxx concepts in the source matches ...",
...
}}
```"""
    messages = [{"role": "user", "content": template}]
    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    summry = json.loads(json_code)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summry, messages

def decide_one_one(source_table_description, source_table_sample, target_table_description, target_table_sample, transform_reason):
    template = f"""Source table:
{source_table_description}
{source_table_sample}
Target table:
{target_table_description}
{target_table_sample}
The source can be transformed to the target table:
{transform_reason}

Task: Understand the Row Mapping. Is it 1:1 or not?
Example of NOT 1:1: 
a. Source row is about student and teacher relationship. Target row in target table is Person. So one row in source table is mapped to two rows in target table.
b. Source row is about one person info (e.g., height, weight, age). Target row in target table is person's all info. So multiple rows in source table is mapped to one row in target table.
Steps:
1. repeat what each row in the source and target table is about.
2. conclude whether the mapping is 1:1 or not in json:
```json
{{  "1:1": true/false,
    "reason": "why 1:1 or not 1:1"
}}
```"""
    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.95)

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    summry = json.loads(json_code)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summry, messages

def get_concept_mapping(source_table_description, source_table_sample, target_table_description, target_table_sample, transform_reason):
    template = f"""Source table:
{source_table_description}
{source_table_sample}
Target table:
{target_table_description}
{target_table_sample}
The source can be transformed to the target table:
{transform_reason}

Task: Plan for Column Mapping
First repeat each column meaning and find all columns they can be mapped to in target tables.
Then group the similar mapping, and how to map in json:
```json
[  {{"source_columns": ["day", "month", "year"] (case sensitive), 
     "target_columns": ["date"],
     "reason": "Concatenate day, month, year to date"
     }},
]
```"""
    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.95)

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    summry = json.loads(json_code)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summry, messages

def write_code_and_debug(key, source_attributes, source_table_description, target_attributes, df, target_table):

    target_attributes = "\n".join(f"{idx + 1}. {target_attribute}: {attributes_description[target_table][target_attribute]}" for idx, target_attribute in enumerate(target_attributes))

    template =  f"""Transform task: Given Source Table, tansform it to Target Table. 

Source table:
{source_table_description}

Target table needs columns:
{target_attributes}

Do the following:
1. First reason about which target columns are transformable, and how to transform. 
2. Then fill in the python function, with detailed comments. Don't change the function name, first line and the return clause.
If no column is transformable, return an empty dataframe.
```python
def transform(input_df):
    output_df = pd.DataFrame()
    ...
    return output_df
```"""
    messages = [{"role": "user", "content": template}]
    
    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)
    
    python_code = extract_python_code(response['choices'][0]['message']['content'])

    messages = messages + [response['choices'][0]['message']]

    detailed_error_info = None

    max_tries = 2

    while max_tries > 0:
        max_tries -= 1

        detailed_error_info = None

        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(python_code, globals())
            temp_target_df = transform(df)
        except Exception: 
            detailed_error_info = get_detailed_error_info()

        if detailed_error_info is None:
            return python_code, messages


        error_message = f"""There is a bug in the code: {detailed_error_info}.
First, study the error message and point out the problem.
Then, fix the bug and return the codes in the following format:
```python
def transform(input_df):
    output_df = pd.DataFrame()
    ...
    return output_df
```"""
        messages.append({"role": "user", "content": error_message})

        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

        python_code = extract_python_code(response['choices'][0]['message']['content'])
        messages.append(response['choices'][0]['message'])
    
    for message in messages:
        print(message['content'])
        print("---------------------")

    raise Exception("The code is not correct. Please try again.")


def escape_json_string(s):
    s = s.replace("\\", "\\\\")

    s = s.replace("\\\\n", "\\n")
    s = s.replace("\\\\\"", "\\\"")

    return s

def load_template(filename, use_template=True, **kwargs):
    with open(filename, 'r') as file:
        template = file.read()
        if use_template:
            return eval(template, {}, kwargs)
        else:
            return template

def process_dataframe(df, instruction_name, **kwargs):
    message = {
        "role": "user",
        "content": load_template(instruction_name, **kwargs) + 
                   load_template('python_template.txt', use_template=False)
    }
    write_log(message['content'])
    write_log("-----------------------------------")
    
    messages_history = [message]

    max_turns = 10

    while len(messages_history) < max_turns:
        
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)


        assistant_message = response['choices'][0]['message']
        messages_history.append(assistant_message)
        write_log(assistant_message['content'])
        write_log("-----------------------------------")

        processed_string = escape_json_string(assistant_message["content"])
        action_content = json.loads(processed_string)
        action_name = action_content["Action"]["Name"]
        
        if action_name == "Code":
            eda_code = action_content["Action"]["Content"]
            
            exec(eda_code, globals())
            eda_result = code(df)
            
            eda_message = {
                "role": "user",
                "content": f"The EDA codes run successfully:\n\n{eda_result}\n\nPlease keep respond in the required format"
            }
            messages_history.append(eda_message)
            write_log(eda_message['content'])
            write_log("-----------------------------------")

        elif action_name == "Decide":
            continue

        elif action_name == "Conclude":
            return messages_history
        
        else:
            raise Exception("Unknown action")

def read_csv_with_common_encodings(file_path, sep = ","):
    common_encodings = ['utf-8', 'ISO-8859-1', 'cp1252', 'latin1', 'ascii', 'utf-16']

    for encoding in common_encodings:
        try:
            return encoding, pd.read_csv(file_path, encoding=encoding, sep=sep)
        except UnicodeDecodeError:
            continue

    with open(file_path, 'rb') as file:
        sample = file.read()
    detected_encoding = chardet.detect(sample)['encoding']

    try:
        return detected_encoding, pd.read_csv(file_path, encoding=detected_encoding, sep=sep)
    except Exception as e:
        raise e

def detect_separator(filename, potential_separators=[',', '\t', ';', '|', ' '], max_lines=100):
    separator_scores = {sep: 0 for sep in potential_separators}

    with open(filename, 'r') as file:
        for _ in range(max_lines):
            line = file.readline()
            if not line:
                break

            for sep in potential_separators:
                fields = re.split(r'(?<!")' + re.escape(sep) + r'(?!")', line)
                
                if len(fields) > 1:
                    separator_scores[sep] += len(fields) - 1

    likely_separator = max(separator_scores, key=separator_scores.get)
    return likely_separator if separator_scores[likely_separator] > 0 else None



def get_value_from_path(data, path):
    """Recursively extract value from nested dict using the given path."""
    
    if not isinstance(data, dict):
        return data

    if not path:
        return data
    return get_value_from_path(data[path[0]], path[1:])



        
def check_functional_dependency(df, determinant, dependent):
    groups = df.groupby(list(determinant))[dependent].nunique()
    is_functionally_dependent = (groups == 1).all()
    return is_functionally_dependent



def combine_pipelines(pipelines):
    new_steps = []
    new_edges = {}
    offset = 0

    for pipeline in pipelines:
        new_steps.extend(pipeline.steps)

        for source_idx, targets in pipeline.edges.items():
            new_targets = [t + offset for t in targets]
            new_edges[source_idx + offset] = new_targets
        
        offset += len(pipeline.steps)

    return TransformationPipeline(new_steps, new_edges)




class TransformationPipeline:


    def __init__(self, steps, edges):
        if not isinstance(steps, list):
            raise ValueError("Steps must be a list")

        for step in steps:
            if not isinstance(step, TransformationStep):
                raise ValueError("Each step must be an instance of TransformationStep")

        if not isinstance(edges, dict):

            if not isinstance(edges, list):
                raise ValueError("Edges must be a list")

            for edge in edges:
                if not isinstance(edge, tuple) or len(edge) != 2:
                    raise ValueError("Each edge must be a tuple of two elements")
                if not isinstance(edge[0], int) or not isinstance(edge[1], int):
                    raise ValueError("Edge elements must be integers")
                if edge[0] < 0 or edge[0] >= len(steps) or edge[1] < 0 or edge[1] >= len(steps):
                    raise ValueError("Edge indices must be within the range of steps")

            self.edges = {}
            for source_idx, target_idx in edges:
                if source_idx not in self.edges:
                    self.edges[source_idx] = []
                self.edges[source_idx].append(target_idx)
        
        else:
            self.edges = edges

        self.steps = steps

        self.cached_results = {}



    def get_step(self, idx):
        return self.steps[idx]

    def get_step_idx(self, step):
        return find_instance_index(self.steps, step)









    def validate_graph(self):
        if self._is_cyclic():
            raise ValueError("The graph is cyclic.")

        num_steps = len(self.steps)
        for source, targets in self.edges.items():
            if source >= num_steps or any(target >= num_steps for target in targets):
                raise ValueError("Edge indices are out of range.")

    def _is_cyclic(self):
        visited = set()
        rec_stack = set()

        def _is_cyclic_util(v):
            if v not in visited:
                visited.add(v)
                rec_stack.add(v)

                for neighbour in self.edges.get(v, []):
                    if neighbour not in visited and _is_cyclic_util(neighbour):
                        return True
                    elif neighbour in rec_stack:
                        return True

            rec_stack.remove(v)
            return False

        for node in range(len(self.steps)):
            if node not in visited and _is_cyclic_util(node):
                return True

        return False

    def display_workflow(self):
        nodes = [f"{idx + 1}. {step.name}" for idx, step in enumerate(self.steps)]

        display_workflow(nodes, self.edges)

    def get_nodes(self):
        nodes = [f"{idx + 1}. {step.name}" for idx, step in enumerate(self.steps)]
        return nodes

    def display(self, call_back=None):

        if call_back is None:
            call_back = self.display
        

        def create_widget(instances):

            nodes = self.get_nodes()
            
            dropdown = widgets.Dropdown(
                options=nodes,
                disabled=False,
            )

            button1 = widgets.Button(description="View")
            button2 = widgets.Button(description="Edit")

            button3 = widgets.Button(description="Return")

            def on_button_clicked3(b):
                clear_output(wait=True)
                call_back()

            def on_button_clicked(b):
                clear_output(wait=True)

                display_workflow(nodes, self.edges)

                display(dropdown)

                buttons = widgets.HBox([button1, button2, button3])
                display(buttons)

                idx = nodes.index(dropdown.value)

                selected_instance = instances[idx]
                selected_instance.display()

            def on_button_clicked2(b):
                clear_output(wait=True)

                idx = nodes.index(dropdown.value)
                selected_instance = instances[idx]

                def call_back_display(step):
                    clear_output(wait=True)
                    call_back()

                selected_instance.edit_widget(callbackfunc=call_back_display)

            button1.on_click(on_button_clicked)
            button2.on_click(on_button_clicked2)
            button3.on_click(on_button_clicked3)

            display_workflow(nodes, self.edges)

            buttons = widgets.HBox([button1, button2])

            display(dropdown, buttons)


        create_widget(self.steps)




    def get_codes(self):
        sorted_step_idx = topological_sort(self.steps, self.edges)

        codes = "from cocoon_data import *\n\n"

        for step_idx  in sorted_step_idx:

            step = self.steps[step_idx]

            source_ids = get_source_nodes_ids(self.edges, step_idx)

            codes += step.get_codes(target_id=step_idx, source_ids=source_ids)

        return codes

    
    def run_codes(self, use_cache=True):

        sorted_step_idx = topological_sort(self.steps, self.edges)
        
        for step_idx  in sorted_step_idx:
        
            if use_cache and step_idx in self.cached_results:
                continue

            step = self.steps[step_idx]

            source_ids = get_source_nodes_ids(self.edges, step_idx)

            source_dfs = [self.cached_results[source_id] for source_id in source_ids]

            result = step.run_codes(dfs=source_dfs)

            if isinstance(result, str):
                write_log(result)
                raise ValueError(result)
            else:
                self.cached_results[step_idx] = result
        
        final_node_idx = self.find_final_node()
        if final_node_idx is None:
            raise ValueError("No final node to return")
        else:
            return self.cached_results[final_node_idx]


    def print_codes(self):
        codes = self.get_codes()
        
        print(highlight(codes, PythonLexer(), Terminal256Formatter()))

    

    def find_final_node(self):
        return find_final_node(self.steps, self.edges)

    def get_final_step(self):
        final_node = self.find_final_node()
        final_step = self.get_step(final_node)
        return final_step

    def find_source_node(self):
        return find_source_node(self.steps, self.edges)
    
    def get_source_step(self):
        source_node = self.find_source_node()
        source_step = self.get_step(source_node)
        return source_step

    def remove_final_node(self):
        if len(self.steps) <= 1:
            raise ValueError("The graph has less than or equal to 1 step. Cannot remove the final node.")


        final_node = self.find_final_node()
        if final_node is None:
            raise ValueError("No final node to remove")

        self.steps.pop(final_node)

        updated_edges = {}
        for source, targets in self.edges.items():
            adjusted_targets = [t - 1 if t > final_node else t for t in targets if t != final_node]

            adjusted_source = source - 1 if source > final_node else source

            if adjusted_targets:
                updated_edges[adjusted_source] = adjusted_targets

        self.edges = updated_edges

        new_cache = {}
        for key, value in self.cached_results.items():
            new_key = key - 1 if key > final_node else key

            if key != final_node:
                new_cache[new_key] = value

        self.cached_results = new_cache

    def add_step(self, new_step, parent_node_idx=None):

        if len(self.steps) == 0 and parent_node_idx is None:
            self.steps.append(new_step)
            return

        if parent_node_idx is not None and isinstance(parent_node_idx, TransformationStep):
            parent_node_idx = find_instance_index(self.steps, parent_node_idx)

        if parent_node_idx is not None and (parent_node_idx < 0 or parent_node_idx >= len(self.steps)):
            raise ValueError(f"Parent node index is out of range. {parent_node_idx} is not within [0, {len(self.steps) - 1}].")

        if not isinstance(new_step, TransformationStep):
            raise ValueError("The new step must be an instance of TransformationStep.")
        
        self.steps.append(new_step)

        new_step_idx = len(self.steps) - 1

        if parent_node_idx is not None:
            if parent_node_idx not in self.edges:
                self.edges[parent_node_idx] = []
            self.edges[parent_node_idx].append(new_step_idx)
        

    
    def add_step_right_after(self, new_step, parent_node_idx):

        if isinstance(parent_node_idx, TransformationStep):
            parent_node_idx = find_instance_index(self.steps, parent_node_idx)

        if parent_node_idx < 0 or parent_node_idx >= len(self.steps):
            raise ValueError(f"Parent node index is out of range. {parent_node_idx} is not within [0, {len(self.steps) - 1}].")

        if not isinstance(new_step, TransformationStep):
            raise ValueError("The new step must be an instance of TransformationStep.")
        
        self.steps.append(new_step)

        new_step_idx = len(self.steps) - 1


        children = self.edges.get(parent_node_idx, [])
        self.edges[parent_node_idx] = [new_step_idx]
        self.edges[new_step_idx] = children

        self.cached_results = {}




    def add_step_to_final(self, new_step):

        final_node = self.find_final_node()
        if final_node is None:
            raise ValueError("The graph doesn't have a final node.")

        self.add_step(new_step, final_node)




    def add_edge(self, source_step, target_step):
        source_idx = find_instance_index(self.steps, source_step)
        target_idx = find_instance_index(self.steps, target_step)

        if source_idx == -1:
            raise ValueError("The source step is not in the graph.")
        if target_idx == -1:
            raise ValueError("The target step is not in the graph.")
        
        self.add_edge_by_index(source_idx, target_idx)


    
    def add_new_step(self, new_step):
        self.steps.append(new_step)
        return len(self.steps) - 1

    def add_edge_by_index(self, source_idx, target_idx):
        if source_idx not in self.edges:
            self.edges[source_idx] = []
        self.edges[source_idx].append(target_idx)

        self.cached_results = {}


    def start(self):
        pass

    def __repr__(self):
        self.display()
        return ""

class Testing: 
    def __init__(self, name = None, explanation="", codes="", sample_df=None):
        if name is None:
            self.name = "Testing Task"
        else:
            self.name = name
        
        self.codes = codes
        self.explanation = explanation
        self.sample_df = sample_df
            

    def run_codes(self, dfs, codes = None):

        if codes is None:
            codes = self.codes
        
        if isinstance(dfs, list):
            df = dfs[0]
        else:
            df = dfs

        input_df = df.copy()
        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(codes, globals())
            result = test(input_df) 
            return result

        except Exception: 
            detailed_error_info = get_detailed_error_info()
            return detailed_error_info
    
    def generate_codes(self, explanation=None):
        if explanation is None:
            explanation = self.explanation
        template = f"""Testing task: Given input df, write python codes that test df and output True/False
===
Input Df:
{self.sample_df.to_csv()}
===
Testing Requirement
{explanation}

Do the following:
1. First reason about how to test
2. Then fill in the python function, with detailed comments. 
DONT change the function name and the return clause.

{{
    "reason": "To transform, we need to ...",
    "codes": "def test(input_df):\\n   ...\\n    return True/False"
}}
"""
        messages = [{"role": "user", "content": template}]
    
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)
        
        self.reason = json_code["reason"]
        self.codes = json_code["codes"]
    
    def display(self):
        raise NotImplementedError


class TestColumnUnique(Testing):
    def __init__(self, col_name, name = None, explanation="", codes="", sample_df=None):
        super().__init__(name, explanation, codes, sample_df)
        self.col_name = col_name

        if name is None:
            self.name = f"Test if {col_name} is unique"

    def generate_test_unique_codes(self):
        self.codes = """# Concatenate all dataframes horizontally
def test(input_df):
    return input_df["{col_name}"].is_unique"""

class TestColumnNotNull(Testing):
    def __init__(self, col_name, name=None, explanation="", codes="", sample_df=None):
        super().__init__(name, explanation, codes, sample_df)
        self.col_name = col_name

        if name is None:
            self.name = f"Test if {col_name} has no null"

    def generate_test_not_null_codes(self):
        self.codes = f"""def test(input_df):
    return not input_df["{self.col_name}"].isnull().any()"""

class TestColumnAcceptedValues(Testing):
    def __init__(self, col_name, accepted_values, name=None, explanation="", codes="", sample_df=None):
        super().__init__(name, explanation, codes, sample_df)
        self.col_name = col_name
        if not isinstance(accepted_values, list):
            raise ValueError("Accepted values must be a list.")
        self.accepted_values = accepted_values

        if name is None:
            self.name = f"Test {col_name} domain"

    def generate_test_accepted_values_codes(self):
        self.codes = f"""def test(input_df):
    return input_df["{self.col_name}"].isin({self.accepted_values}).all()"""

class TestColumnType(Testing):
    def __init__(self, col_name, expected_type, name=None, explanation="", codes="", sample_df=None):
        super().__init__(name, explanation, codes, sample_df)
        self.col_name = col_name
        self.expected_type = expected_type

        if name is None:
            self.name = f"Test {col_name} type"

    def generate_test_column_type_codes(self):
        self.codes = f"""def test(input_df):
    return input_df["{self.col_name}"].dtype == "{self.expected_type}"""

class TestColumnRange(Testing):
    def __init__(self, col_name, min_value, max_value, name=None, explanation="", codes="", sample_df=None):
        super().__init__(name, explanation, codes, sample_df)
        self.col_name = col_name
        self.min_value = min_value
        self.max_value = max_value

        if name is None:
            self.name = f"Test {col_name} range"

    def generate_test_range_codes(self):
        self.codes = f"""def test(input_df):
    if not pd.api.types.is_numeric_dtype(input_df["{self.col_name}"]):
        return False
    return input_df["{self.col_name}"].between({self.min_value}, {self.max_value}).all()"""


class TestColumnRegex(Testing):
    def __init__(self, col_name, regex_pattern, name=None, explanation="", codes="", sample_df=None):
        super().__init__(name, explanation, codes, sample_df)
        self.col_name = col_name
        self.regex_pattern = regex_pattern

        if name is None:
            self.name = f"Test {col_name} regex pattern"

    def generate_test_regex_codes(self):
        self.codes = f"""def test(input_df):
    return input_df["{self.col_name}"].str.match("{self.regex_pattern}").all()"""



class TransformationStep:

    def __init__(self, name = None, explanation="", codes="", sample_df=None):
        if name is None:
            self.name = "Transformation Task"
        else:
            self.name = name

        self.codes = codes
        self.explanation = explanation

        if sample_df is not None:
            try:
                self.sample_df = sample_df.copy()
            except:
                self.sample_df = sample_df

        self.reason = ""
            

    def verify_infput(self, df):
        if not isinstance(df, pd.DataFrame):
            raise ValueError(f"Input is not a pandas dataframe. It is {type(df)}.")

    def verify_output(self, df):
        if not isinstance(df, pd.DataFrame):
            raise ValueError(f"Output is not a pandas dataframe. It is {type(df)}.")


    def generate_codes(self, explanation=None):
        if explanation is None:
            explanation = self.explanation
            
        template = f"""Transformation task: Given input df, write python codes that transform and ouput df.
===
Input Df:
{self.sample_df.to_csv()}
===
Transformation Requirement
{explanation}

Do the following:
1. First reason about how to transform
2. Then fill in the python function, with detailed comments. 
DONT change the function name, first line and the return clause.

{{
    "reason": "To transform, we need to ...",
    "codes": "def transform(input_df):\\n    output_df = input_df.copy()\\n    ...\\n    return output_df"
}}
"""
        messages = [{"role": "user", "content": template}]
    
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)
        
        self.reason = json_code["reason"]
        self.codes = json_code["codes"]


    def postprocessing(self, df):
        return df

    def run_codes(self, dfs, codes = None):

        if codes is None:
            codes = self.codes
        
        if isinstance(dfs, list):
            df = dfs[0]
        else:
            df = dfs

        input_df = df.copy()


        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(codes, globals())
            temp_target_df = transform(input_df) 
            temp_target_df = self.postprocessing(temp_target_df)
            self.verify_output(temp_target_df)
            return temp_target_df
        except Exception: 
            detailed_error_info = get_detailed_error_info()
            return detailed_error_info




    def edit_widget(self, callbackfunc=None):

        print("\033[91mRemember to save after editing!\033[0m")

        explanation_label = widgets.Label('Task:')
        explanation_text = widgets.Textarea(
            value = self.explanation if self.explanation != "" else "Transform ... For example ...",
            layout=widgets.Layout(width='95%', height='200px')
        )
        submit_button = widgets.Button(description="Submit Task")
        submit_spinner = widgets.HTML()

        def on_submit_clicked(b):
            print("Generating codes...")
            
            submit_spinner.value = spinner_value
            self.generate_codes(explanation = explanation_text.value)

            codes_text.value = self.codes
            reason_label.value = self.reason
            submit_spinner.value = ""
            print("Done")

        submit_button.on_click(on_submit_clicked)
        submit_box = widgets.HBox([submit_button, submit_spinner])

        codes_label = widgets.Label('Codes:')
        codes_text = widgets.Textarea(
            value=self.codes,
            layout=widgets.Layout(width='95%', height='200px')
        )
        run_button = widgets.Button(description="Run Codes")
        run_spinner = widgets.HTML()

        reason_label = widgets.Label(layout=Layout(width='100%', overflow='auto', white_space='pre-wrap'))
        output_label = widgets.Label()

        def on_run_clicked(b):
            print("Running codes...")
            run_spinner.value =  spinner_value
            output = self.run_codes(dfs = self.sample_df, codes = codes_text.value)
            if isinstance(output, str):
                error_label.value = "<span style='color: red;'>" + output.replace("\n", "<br>") + "</span>"
                output_df_widget.value = ""
            else:
                error_label.value = ""
                output_df_widget.value = output.to_html(border=0)
            run_spinner.value = ""
            print("Done")
                

        run_button.on_click(on_run_clicked)
        run_box = widgets.HBox([run_button, run_spinner])


        panel_layout = Layout(width='400px')

        left_panel = widgets.VBox([explanation_label, explanation_text, submit_box], layout=panel_layout)
        right_panel = widgets.VBox([codes_label, codes_text, run_box, output_label], layout=panel_layout)
        display(widgets.HBox([left_panel, right_panel]))
        display(reason_label)

        input_df_label = widgets.Label('Input Table:')
        display(input_df_label)
        display(HTML(self.sample_df.to_html(border=0)))

        output_df_label = widgets.Label('Output Table:')
        output_df_widget = widgets.HTML(
            value='',
            placeholder='Output DataFrame will be shown here',
            description='',
        )
        
        error_label = widgets.HTML(layout=Layout(overflow='auto'))

        save_button = widgets.Button(description="Save the Step")

        def on_save_clicked(b):
            print("Saving ...")
            self.codes = codes_text.value
            self.explanation = explanation_text.value
            self.reason = reason_label.value
            callbackfunc(self)
            print("Done")

        save_button.on_click(on_save_clicked)

        display(widgets.VBox([
                                output_df_label, 
                                output_df_widget, 
                                error_label]))

        display(save_button)

        if self.codes != "":
            on_run_clicked(run_button)

    def get_sample_output(self):
        result = self.run_codes(self.sample_df)
        if isinstance(result, str):
            raise ValueError(f"The codes for step {self.name} is not correct: {result} \n Please edit the codes.")
        elif isinstance(result, pd.DataFrame):
            return result
        else:
            raise ValueError("Output is neither a string nor a pandas dataframe.")

    def display(self):
        print(f"{BOLD}{self.name}{END}: {self.explanation}")
        display(HTML(f"<hr>"))
        print(f"{BOLD}Codes{END}:")
        print(highlight(self.codes, PythonLexer(), Terminal256Formatter()))

        if hasattr(self, 'sample_df'):
            display(HTML(f"<hr><b>Example Input</b>: {self.sample_df.to_html()}"))
            
            if not hasattr(self, 'output_sample_df'):
                self.output_sample_df = self.run_codes(self.sample_df)
            
            display(HTML(f"<hr><b>Example Output</b>: {self.output_sample_df.to_html()}"))

    def __repr__(self):
        return self.name

    def rename_based_on_explanation(self):
        title, message = give_title(self.explanation)
        self.name = title

    def get_codes(self, target_id=0, source_ids=None):
        if source_ids is None:
            source_ids = []
        source_ids_str = ", ".join([f"df_{source_id}" for source_id in source_ids])
        return self.codes + "\n\n" + f"df_{target_id} = transform({source_ids_str})\n\n"



class GeoAggregationStep(TransformationStep):

    def __init__(self, shape_data, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.shape_data = shape_data
        self.agg_cols = [shape_data.x_att, shape_data.y_att]

        self.explanation = f"""Aggregate {self.agg_cols} by ..."""

        if 'name' not in kwargs:
            self.name = f"Aggregation"

    def verify_input(self, df):
        if not set(self.agg_cols).issubset(df.columns):
            raise ValueError(f"Columns {self.agg_cols} are not in the input dataframe.")

    def postprocessing(self, df):
        if not isinstance(df, pd.DataFrame):
            raise ValueError("Output is not a pandas dataframe.")

        if not set(self.agg_cols).issubset(df.columns):
            raise ValueError(f"Columns {self.agg_cols} are not in the output dataframe.")
        
        if df.index.name is not None:
            df = df.reset_index()
        return df

    def generate_codes(self, explanation=None):
        if explanation is None:
            explanation = self.explanation
            
        template = f"""Transformation task: Given input df, write python codes that aggregate and ouput df.
===
Input Df:
{self.sample_df.df[:2].to_csv()}
===
Transformation Requirement:
Aggregate {self.agg_cols}.
{explanation}

Do the following:
1. First reason about how to transform. The final output df should group by only attributes: {self.agg_cols}
2. Then fill in the python function, with detailed comments. 
DONT change the function name, first line and the return clause.

{{
    "reason": "To transform, we need to ...",
    "codes": "def transform(input_df):\\n    output_df = input_df.copy()\\n    ...\\n    return output_df"
}}
"""
        messages = [{"role": "user", "content": template}]
    
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)
        
        self.reason = json_code["reason"]
        self.codes = json_code["codes"]

    
    def run_codes(self, dfs, codes = None):

        if codes is None:
            codes = self.codes
        
        if isinstance(dfs, list):
            shape_data = dfs[0]
        else:
            shape_data = dfs

        data = shape_data.get_data()
        meta = shape_data.meta.copy()
        meta['aggregated'] = True


        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(codes, globals())
            temp_output_df = transform(data)
            temp_output_df = self.postprocessing(temp_output_df)

            return shape_manufacturing(shape_data = temp_output_df, x_att=self.agg_cols[0], y_att=self.agg_cols[1], meta=meta)
        
        except Exception: 
            detailed_error_info = get_detailed_error_info()
            return detailed_error_info


class MultiDFTransformationStep(TransformationStep):
    def __init__(self, name=None, explanation="", codes="", sample_dfs=None):
        super().__init__(name, explanation, codes)
        if sample_dfs is not None:
            self.sample_dfs = [df.copy() for df in sample_dfs]
        else:
            self.sample_dfs = []

    def run_codes(self, dfs):
        input_dfs = [df.copy() for df in dfs]
        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(self.codes, globals())
            temp_target_df = transform(*input_dfs)
            return temp_target_df
        except Exception:
            detailed_error_info = get_detailed_error_info()
            return detailed_error_info
        
        

    def display(self):
        display(HTML(f"<b>{self.name}</b>: {self.explanation} <hr> <b>Codes</b>:"))
        print(highlight(self.codes, PythonLexer(), Terminal256Formatter()))

        if hasattr(self, 'sample_dfs'):
            for i, df in enumerate(self.sample_dfs):
                display(HTML(f"<hr><b>Example Input {i+1}</b>: {df.to_html()}"))

            if not hasattr(self, 'output_sample_dfs'):
                self.output_sample_dfs = self.run_codes(self.sample_dfs)

            for i, df in enumerate(self.output_sample_dfs):
                display(HTML(f"<hr><b>Example Output {i+1}</b>: {df.to_html()}"))
    
    def generate_codes(self):
        raise NotImplementedError("generate_codes() is not implemented for MultiDFTransformationStep.")


class ConcatenateHorizontalStep(MultiDFTransformationStep):
    def __init__(self, name=None, explanation="", codes="", sample_dfs=None):
        super().__init__(name, explanation, codes, sample_dfs)
        self.generate_concatenate_horizontal_codes()

        self.explanation = """Concatenate all dataframes horizontally"""

        if name is None:
            self.name = "Concatenate Horizontal"

    def generate_concatenate_horizontal_codes(self):
        self.codes = """# Concatenate all dataframes horizontally
def transform(*dfs):
    return pd.concat(dfs, axis=1)"""



class SourceStep(TransformationStep):
    def __init__(self, doc_df, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.explanation = ""
        self.doc_df = doc_df
        self.name = doc_df.table_name

    def generate_codes(self):
        pass

    def run_codes(self, dfs):
        return self.doc_df.original_df

    def display(self):
        display(HTML(self.doc_df.original_df[:5].to_html()))

    def edit_widget(self, callbackfunc=None):
        self.display()
        print("\033[91mEdit Source File is under development!\033[0m")

        return_button = widgets.Button(description="Return")

        def on_return_clicked(b):
            clear_output(wait=True)
            callbackfunc(self)
        
        return_button.on_click(on_return_clicked)

        display(return_button)
    
    def get_codes(self, target_id=0, source_ids=None):
        if source_ids is None:
            source_ids = []
            
        if self.doc_df.file_path is None:
            return f"df_{str(target_id)} = pd.read_csv(ADD_YOUR_SOURCE_FILE_PATH_HERE) \n\n"
        else:
            sep = self.doc_df.sep
            encoding = self.doc_df.encoding
            return f"df_{str(target_id)} = pd.read_csv('{self.doc_df.file_path}', sep='{sep}', encoding='{encoding}') \n\n"

class RemoveMissingValueStep(TransformationStep):

    def __init__(self, col, reason, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.col = col
        self.explanation = f"""Remove the rows with missing values in column {col}"""
        self.generate_remove_missing_value_codes()
        self.generate_missing_value_samples()

        if 'name' not in kwargs:
            self.name = f"Remove NULL for {col}"


    def generate_remove_missing_value_codes(self):
        self.codes = f"""# Remove the rows with missing values in column {self.col}
def transform(df):
    output_df = df.copy()
    output_df = output_df.dropna(subset=["{self.col}"])
    return output_df"""


    def generate_missing_value_samples(self):
        sample_df1 = self.sample_df[self.sample_df[self.col].notnull()][:2]
        sample_df2 = self.sample_df[self.sample_df[self.col].isnull()][:2]
        sample_df = pd.concat([sample_df1, sample_df2])
        self.sample_df = sample_df







    


















        

    






    






        




        









        



        













    






    






    


class GeoShapeCustomTransformStep(TransformationStep):

    def generate_codes(self, explanation=None):

        data_type = self.sample_df.get_type()

        if data_type == "raster":
            raise NotImplementedError("generate_codes() is not implemented for Raster.")
        elif data_type == "df":
            raise NotImplementedError("generate_codes() is not implemented for df.")

        if explanation is None:
            explanation = self.explanation
            
        template = f"""Transformation task: Given input {data_type}, write python codes that transform and ouput {data_type}.
===
Input {data_type}:
{self.sample_df.get_summary()}
===
Transformation Requirement
{explanation}

Do the following:
1. First reason about how to transform
2. Then fill in the python function, with detailed comments. 
DONT change the function name and the return clause.

{{
    "reason": "To transform, we need to ...",
    "codes": "def transform(input_{data_type}):\\n    ...\\n    return output_{data_type}"
}}
"""
        messages = [{"role": "user", "content": template}]
    
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)
        
        self.reason = json_code["reason"]
        self.codes = json_code["codes"]
    
    def run_codes(self, dfs, codes = None):

        if codes is None:
            codes = self.codes
        
        if isinstance(dfs, list):
            shape_data = dfs[0]
        else:
            shape_data = dfs

        data = shape_data.get_data()
        meta = shape_data.meta


        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(codes, globals())
            temp_output_df = transform(data)
            return shape_manufacturing(shape_data = temp_output_df, meta = meta)
        except Exception: 
            detailed_error_info = get_detailed_error_info()
            return detailed_error_info

class GeoShapeStep(TransformationStep):
    def generate_codes(self):
        self.codes = f"""# Reproject to {self.target_crs}
NOT IMPLEMENTED YET!"""

    def verify_input(self, shape_data):
        if not isinstance(shape_data, ShapeData):
            raise TypeError("shape_data must be of type ShapeData.")
    
    def verify_output(self, shape_data):
        if not isinstance(shape_data, ShapeData):
            raise TypeError("shape_data must be of type ShapeData.")

    def edit_widget(self, callbackfunc=None):
        print("🚧 under development")
        self.display()

        return_button = widgets.Button(description="Return")

        def on_return_clicked(b):
            clear_output(wait=True)
            callbackfunc(self)
        
        return_button.on_click(on_return_clicked)

        display(return_button)

    def get_sample_output(self):
        result = self.run_codes(self.sample_df)

    def display(self):
        if hasattr(self, 'sample_df'):
            display(HTML(f"<hr><b>Example Input</b>:"))
            self.sample_df.__repr__()
            
            if not hasattr(self, 'output_sample_df'):
                self.output_sample_df = self.run_codes(self.sample_df)
            
            display(HTML(f"<hr><b>Example Output</b>:"))
            self.output_sample_df.__repr__()


class ReprojectStep(GeoShapeStep):

    def __init__(self, target_crs=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.target_crs = target_crs
        
        if 'name' not in kwargs:
            self.name = f"Reproject to {target_crs}"

    def run_codes(self, dfs):
        if isinstance(dfs, list):
            shape_data = dfs[0]
        else:
            shape_data = dfs
        return shape_data.project_to_target_crs(target_crs=self.target_crs)

class ResampleStep(GeoShapeStep):
    def __init__(self, geo_transform=None, resolution=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.geo_transform = geo_transform
        self.resolution = resolution

        if 'name' not in kwargs:
            self.name = f"Rasample"

        if geo_transform is not None:
            self.name += f" to {geo_transform}"
        
        if resolution is not None:
            self.name += f" to resolution {resolution}"
    
    def run_codes(self, dfs):
        if isinstance(dfs, list):
            shape_data = dfs[0]
        else:
            shape_data = dfs
        return shape_data.resample_to_target_transform(geo_transform=self.geo_transform, resolution=self.resolution)

class NumpyToDfStep(GeoShapeStep):
    def __init__(self,  *args, **kwargs):
        super().__init__(*args, **kwargs)

        if 'name' not in kwargs:
            self.name = f"NumPy to Pandas DataFrame"
    
    def run_codes(self, dfs):
        if isinstance(dfs, list):
            shape_data = dfs[0]
        else:
            shape_data = dfs
        return shape_data.to_df()


class SourceShapeStep(GeoShapeStep):
    def __init__(self, shape_data, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.explanation = ""
        self.shape_data = shape_data
        
        if "table_name" in shape_data.meta:
            self.name = shape_data.meta["table_name"]
        else:
            self.name = "Source Geographical Shape"

    def generate_codes(self):
        pass

    def run_codes(self, dfs):
        return self.shape_data
    
    def display(self):
        self.shape_data.display()
    
    def edit_widget(self, callbackfunc=None):
        self.display()
        print("\033[91mEdit Source File is under development!\033[0m")

        return_button = widgets.Button(description="Return")

        def on_return_clicked(b):
            clear_output(wait=True)
            callbackfunc(self)
        
        return_button.on_click(on_return_clicked)

        display(return_button)

    def get_codes(self, target_id=0, source_ids=None):
        if source_ids is None:
            source_ids = []
            
        if hasattr(self, 'gdf'):
            return f"df_{str(target_id)} = gpd.read_file(ADD_YOUR_SOURCE_FILE_PATH_HERE) \n\n"
        elif hasattr(self, 'raster_path'):
            return f"df_{str(target_id)} = rasterio.open('{self.raster_path}').read() \n\n"
        elif hasattr(self, 'np_array'):
            return f"df_{str(target_id)} = np.load(ADD_YOUR_SOURCE_FILE_PATH_HERE) \n\n"
        elif hasattr(self, 'df'):
            return f"df_{str(target_id)} = pd.read_csv(ADD_YOUR_SOURCE_FILE_PATH_HERE) \n\n"



class RemoveColumnsStep(TransformationStep):

    def __init__(self, col_indices, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.col_indices = col_indices

        if self.explanation == "":
            self.explanation = f"""Remove columns"""
        
        self.generate_remove_columns_codes()
        self.generate_remove_columns_samples()
        
        if 'name' not in kwargs:
            self.name = f"Remove Columns"
        
    
    def generate_remove_columns_codes(self):
        if isinstance(self.col_indices, list):
            cols_indices_str = str(self.col_indices)
        else:
            cols_indices_str = f"[{self.col_indices}]"

        self.codes = f"""# Remove columns by indices {cols_indices_str}
def transform(df):
    # Create a boolean mask for all columns
    mask = np.full(df.shape[1], False)
    # Set True for columns to be dropped
    mask[{cols_indices_str}] = True
    # Drop columns based on the mask
    output_df = df.loc[:, ~mask]
    return output_df"""

    def generate_remove_columns_samples(self):
        sample_df = self.sample_df[:4]
        self.sample_df = sample_df




class RemoveRowsStep(TransformationStep):

    def __init__(self, rows, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.rows = rows

        if self.explanation == "":
            self.explanation = f"""Remove rows {rows}"""
        
        self.generate_remove_rows_codes()
        self.generate_remove_rows_samples()

        if 'name' not in kwargs:
            self.name = f"Remove Rows"

    def generate_remove_rows_codes(self):
        if isinstance(self.rows, list):
            rows_str = str(self.rows)
        else:
            rows_str = f"[{self.rows}]"

        self.codes = f"""# Remove rows {rows_str}
def transform(df):
    output_df = df.drop(index={rows_str})
    return output_df"""

    def generate_remove_rows_samples(self):
        sample_df1 = self.sample_df[~self.sample_df.index.isin(self.rows)][:2]
        sample_df2 = self.sample_df[self.sample_df.index.isin(self.rows)][:2]
        sample_df = pd.concat([sample_df1, sample_df2])
        self.sample_df = sample_df

class RemoveDuplicatesStep(TransformationStep):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        if self.explanation == "":
            self.explanation = f"""Remove duplicate rows"""
        
        self.generate_remove_duplicates_codes()
        self.generate_remove_duplicates_samples()

        if 'name' not in kwargs:
            self.name = f"Remove Duplicates"
    
    def generate_remove_duplicates_codes(self):
        self.codes = f"""# Remove duplicate rows
def transform(df):
    output_df = df.drop_duplicates()
    return output_df"""

    def generate_remove_duplicates_samples(self):
        duplicate_mask = self.sample_df.duplicated(keep=False)
        duplicated_rows = self.sample_df[duplicate_mask]

        sample_duplicated_rows = duplicated_rows[:4]
        self.sample_df = sample_duplicated_rows

class CleanDataType(TransformationStep):

    def __init__(self, column_data_type_dict , *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.column_data_type_dict = column_data_type_dict

        if self.explanation == "":
            self.explanation = f"""Remove rows that don't match the data type"""

        self.generate_clean_data_type_codes()

        if 'name' not in kwargs:
            self.name = f"Clean Data Type"
    
    def generate_clean_data_type_codes(self):
        self.codes = f"""# Clean data type
def transform(df):
    data_type_dict = {self.column_data_type_dict}
    for column_name, data_type in data_type_dict.items():
        mask = select_invalid_data_type(df, column_name, data_type)
        df = df[~mask]
    return df"""


class ProjectionStep(TransformationStep):

    def __init__(self, cols, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cols = cols

        all_cols = ", ".join(self.cols)

        self.explanation = f"""Keep only column {all_cols}"""
        self.generate_projection_codes()

        if hasattr(self, 'sample_df'):
            self.generate_projection_samples()

        if 'name' not in kwargs:
            self.name = f"Project {all_cols}"

    def generate_projection_codes(self):
        all_cols = ', '.join([f'"{col}"' for col in self.cols])

        self.codes = f"""# Keep only specified columns
def transform(df):
    output_df = df.copy()
    output_df = output_df[[{all_cols}]]
    return output_df"""

    def generate_projection_samples(self):
        sample_df = self.sample_df[:4]
        self.sample_df = sample_df



class RegexTransformationStep(TransformationStep):

    def __init__(self, col, unique_values, reason, *args, **kwargs):

        super().__init__(*args, **kwargs)

        if 'name' not in kwargs:
            self.name = f"Clean {col}"

        self.explanation = f"""Remove the unusual values from column {col}: {reason}"""

        self.col = col
        self.unique_values = unique_values
        self.reason = reason

    def generate_regex_codes(self, explanation=None):
        if not hasattr(self, 'labeled_result'):
            raise ValueError("Please run the edit_widget() function first.")

        unusual_values = self.labeled_result["unusual_values"]
        normal_values = self.labeled_result["normal_values"]


        self.sample_df[self.col] = self.sample_df[self.col].astype(str)

        sample_df1 = self.sample_df[self.sample_df[self.col].isin(normal_values)][:2]
        sample_df2 = self.sample_df[self.sample_df[self.col].isin(unusual_values)][:2]
        sample_df = pd.concat([sample_df1, sample_df2])
        self.sample_df = sample_df
        
        print("Generating codes...")

        progress = show_progress(1)
        try:
            regex_result = find_regex_pattern(self.col, self.labeled_result)
        except Exception as e:
            write_log(e)
            regex_result = {"exists_regex": False}

        progress.value += 1

        if regex_result["exists_regex"]:
            regex_pattern = regex_result["regex"]
            self.codes = f"""# Clean column {self.col} by removing the unusual rows that don't match the regex pattern
def transform(df):
    output_df = df.copy()

    # Transform the column to string type
    output_df["{self.col}"] = output_df["{self.col}"].astype(str)

    output_df = output_df[output_df["{self.col}"].str.match(r"{regex_pattern}")]
    return output_df"""

            self.explanation = f"""Remove the unusual values from column {self.col} by removing the rows that don't match the regex pattern: {regex_pattern}"""
        
        else:
            unusual_values = self.labeled_result["unusual_values"]
            self.codes = f"""# Clean column {self.col} by removing the unusual rows with unusual values
def transform(df):
    output_df = df.copy()

    # Transform the column to string type
    output_df["{self.col}"] = output_df["{self.col}"].astype(str)

    output_df = output_df[~output_df["{self.col}"].isin({unusual_values})]
    return output_df"""

            self.explanation = f"""Remove the unusual values from column {self.col} by removing the rows with unusual values: {unusual_values}"""

    def parent_edit_widget(self, callbackfunc=None):
        super().edit_widget(callbackfunc)


    def edit_widget(self, callbackfunc=None):

        print("Analyzing the unusual values...")

        if not hasattr(self, 'labeled_result'):
            progress = show_progress(1)
            result = classify_unusual_values(self.col, self.unique_values, self.reason)
            self.labeled_result = result
            progress.value += 1

        def create_tabular_widgets_toggle(result):
            grid_items = []

            grid_items.append(widgets.Label(value=''))
            grid_items.append(widgets.Label(value='Keep (Normal)'))
            grid_items.append(widgets.Label(value='Remove (Unusual)'))

            checkbox_widgets = {}

            for email in result['normal_values'] + result['unusual_values']:
                is_normal = email in result['normal_values']
                
                label = widgets.Label(value=email)
                grid_items.append(label)

                keep_checkbox = widgets.Checkbox(value=is_normal)
                grid_items.append(keep_checkbox)

                remove_checkbox = widgets.Checkbox(value=not is_normal)
                grid_items.append(remove_checkbox)

                checkbox_widgets[email] = (keep_checkbox, remove_checkbox)

                keep_checkbox.observe(lambda change, email=email: on_checkbox_toggle(change, email, 'keep'), names='value')
                remove_checkbox.observe(lambda change, email=email: on_checkbox_toggle(change, email, 'remove'), names='value')

            def on_checkbox_toggle(change, email, action):
                if change['new']:
                    if action == 'keep':
                        checkbox_widgets[email][1].value = False
                    else:
                        checkbox_widgets[email][0].value = False

            grid_layout = widgets.Layout(grid_template_columns="repeat(3, 33%)",
                                        align_items='center')
            grid_box = widgets.GridBox(children=grid_items, layout=grid_layout)

            submit_button = widgets.Button(description="Submit")

            def on_submit_button_clicked(b):
                clear_output(wait=True)

                updated_result = {'unusual_values': [], 'normal_values': []}
                for email, (keep_checkbox, remove_checkbox) in checkbox_widgets.items():
                    if keep_checkbox.value:
                        updated_result['normal_values'].append(email)
                    else:
                        updated_result['unusual_values'].append(email)
                
                self.labeled_result = updated_result
                
                self.generate_regex_codes()

                print("Done")

                self.parent_edit_widget(callbackfunc)


            submit_button.on_click(on_submit_button_clicked)

            display(grid_box, submit_button)

        print("Please verify the unusual values...")

        create_tabular_widgets_toggle(self.labeled_result)



class ColumnRename(TransformationStep):

    def __init__(self, rename_map, *args, **kwargs):

        super().__init__(*args, **kwargs)

        if 'name' not in kwargs:
            self.name = "Rename Columns"

        self.rename_map = rename_map
        self.explanation = f"""Rename columns in the DataFrame based on: \n"""

        valid_renaming = {}

        for old_name, new_name in self.rename_map.items():
            if old_name != new_name:
                valid_renaming[old_name] = new_name

        rename_map_str = "{\n"
        rename_map_str += "\n".join([f"    '{old}': '{new}'," for old, new in valid_renaming.items()])
        rename_map_str += "\n    }"

        self.explanation += rename_map_str
        
        self.generate_rename_codes()

    def verify_input(self, df):
        super().verify_input(df)

        if len(self.rename_map) != len(set(self.rename_map.keys())):
            raise ValueError("Some old column names are duplicated.")
        
        if len(self.rename_map) != len(set(self.rename_map.values())):
            raise ValueError("Some new column names are duplicated.")

        for old_name in self.rename_map.keys():
            if old_name not in df.columns:
                raise ValueError(f"Column {old_name} does not exist in the DataFrame.")

    def generate_rename_codes(self):

        valid_renaming = {}
        
        for old_name, new_name in self.rename_map.items():
            if old_name != new_name:
                valid_renaming[old_name] = new_name

        rename_map_str = "{\n"
        rename_map_str += "\n".join([f"        '{old}': '{new}'," for old, new in valid_renaming.items()])
        rename_map_str += "\n    }"

        self.codes = f"""# Rename columns in the DataFrame based on column indices to avoid circular issues
def transform(df):
    rename_map = {rename_map_str}
    # Create a list of the current column names
    new_column_names = list(df.columns)

    # Find the indices of the columns to be renamed and update their names
    for old_name, new_name in rename_map.items():
        if old_name in df.columns:
            index = df.columns.get_loc(old_name)
            new_column_names[index] = new_name
        else:
            raise ValueError(f'Column {{{{old_name}}}} not found in DataFrame')

    # Assign the new names to the DataFrame's columns
    df.columns = new_column_names
    return df
"""



        



class DocumentedDatabase:
    def __init__(self, doc_dfs):
        self.doc_dfs = doc_dfs
        
    def generate_pipeline(self):
        pipelines = []

        for doc_df in self.doc_dfs:
            pipelines.append(doc_df.generate_pipeline())

        final_pipeline = combine_pipelines(pipelines)
        return final_pipeline

class DataCleaning:
    def __init__(self, doc_df):
        self.doc_df = doc_df
        self.pipeline = doc_df.generate_pipeline()
    
    def __repr__(self):
        self.display()
        return ""

    def run_codes(self):
        return self.pipeline.run_codes()

    def display(self):
        

        self.pipeline.display(call_back=self.display)

        attributes = []
        issues = {}

        document = self.doc_df.document

        if "missing_value" in document:
            issues["missing_value"] = []
            for attribute in document["missing_value"]:
                item = document["missing_value"][attribute]
                if item:
                    attributes.append(attribute)
                    issues["missing_value"].append((attribute, item["summary"]))

        if "unusual" in document:
            issues["unusual"] = []
            for attribute in document["unusual"]:
                item = document["unusual"][attribute]["summary"]
                if item["Unusualness"]:
                    attributes.append(attribute)
                    issues["unusual"].append((attribute, item["Examples"]))

        self.recommends_transformation(issues)

    def recommends_transformation(self, issues):

        def on_button_clicked_missing(b):
            clear_output(wait=True)
            self.create_remove_missing_value_step(b.attribute, b.issue)

        boxes = []
        for attribute, issue in issues["missing_value"]:
            label = widgets.HTML(value=f"❓ <b>{attribute}</b> has missing values: {issue}")

            button = widgets.Button(description=f"Remove them")
            button.attribute = attribute
            button.issue = issue
            button.on_click(on_button_clicked_missing)

            box = widgets.VBox([label, button])
            boxes.append(box)

        def on_button_clicked(b):
            clear_output(wait=True)
            unique_values = self.doc_df.df[b.attribute].dropna().unique()[:20]
            self.create_regex_step(b.attribute, b.issue, unique_values)
            
        for attribute, issue in issues["unusual"]:
            label = widgets.HTML(value=f"🤔 <b>{attribute}</b> has unusual value: {issue}")

            button = widgets.Button(description=f"Remove them")
            button.attribute = attribute
            button.issue = issue
            button.on_click(on_button_clicked)

            box = widgets.VBox([label, button])
            boxes.append(box)

        def on_remove_last_clicked(b):
            clear_output(wait=True)
            self.add_remove_last_step()

        remove_last_button = widgets.Button(description="⚠️ Remove Last")
        remove_last_button.on_click(on_remove_last_clicked)
        box = widgets.VBox([remove_last_button])
        boxes.append(box)


        label = widgets.HTML(value=f"🎲 Want to perform an ad hoc transformation?")

        def on_ad_hoc_clicked(b):
            clear_output(wait=True)
            self.create_ad_hoc_step()

        adhoc_button = widgets.Button(description="Ad hoc")
        adhoc_button.on_click(on_ad_hoc_clicked)
        box = widgets.VBox([label, adhoc_button])
        boxes.append(box)


        display(widgets.VBox(boxes))

    
    def add_remove_last_step(self):
        self.pipeline.remove_final_node()
        self.final_shape = self.pipeline.run_codes()
        clear_output(wait=True)
        self.display()

    def create_remove_missing_value_step(self, col, reason):


        sample_output = self.doc_df.df

        remove_missing_value_step = RemoveMissingValueStep(col=col, reason=reason, sample_df=sample_output)

        def callback(remove_missing_value_step):
            if remove_missing_value_step.explanation != "" or remove_missing_value_step.codes != "":
                self.pipeline.add_step_to_final(remove_missing_value_step)
            clear_output(wait=True)
            self.display()

        callbackfunc = callback
        
        remove_missing_value_step.edit_widget(callbackfunc=callbackfunc)


    def create_regex_step(self, col, reason, unique_values):


        sample_output = self.doc_df.df

        regex_step = RegexTransformationStep(col=col, unique_values=unique_values, reason=reason, sample_df=sample_output)

        def callback(regex_step):
            if regex_step.explanation != "" or regex_step.codes != "":
                
                self.pipeline.add_step_to_final(regex_step)

            clear_output(wait=True)
            self.display()

        callbackfunc = callback

        regex_step.edit_widget(callbackfunc=callbackfunc)

    def create_ad_hoc_step(self):

        final_node = self.pipeline.find_final_node()
        final_step = self.pipeline.get_step(final_node)
        sample_output = final_step.get_sample_output()

        add_hoc_step = TransformationStep(name="Ad hoc", sample_df=sample_output)

        def callback(add_hoc_step):
            if add_hoc_step.explanation != "" or add_hoc_step.codes != "":
                add_hoc_step.rename_based_on_explanation()
                self.pipeline.add_step_to_final(add_hoc_step)
            clear_output(wait=True)
            self.display()

        callbackfunc = callback

        add_hoc_step.edit_widget(callbackfunc=callbackfunc)
    
    def print_codes(self):
        self.pipeline.print_codes()

    def generate_pipeline(self):
        return self.pipeline

def replace_nan(df):
    for col in df.columns:
        df[col] = df[col].fillna("")
    return df

def embed_string(string, engine=None):
    max_retries = 3
    retry_count = 0
    last_exception = None

    while retry_count < max_retries:
        try:
            response = call_embed(string)
            embeddings = response['data'][0]['embedding']
            return embeddings
        except Exception as e:
            retry_count += 1
            print(f"Retry {retry_count}: An error occurred while embedding: {e}")
            last_exception = e
            
            if retry_count < max_retries:
                print(f"Waiting for 5 seconds before retrying...")
                time.sleep(5)

    raise last_exception

def initialize_output_csv(df, output_csv_address, label='label'):
    try:
        df_output = pd.read_csv(output_csv_address)
    except FileNotFoundError:
        df_output = get_unique_labels_with_ids(df, label='label')
        df_output.to_csv(output_csv_address, index=False)
    return df_output

def find_first_nan_index(df, column_name):
    """
    Finds the first index in a DataFrame column that is NaN.
    
    :param df: pandas DataFrame object
    :param column_name: String name of the column to search for NaN
    :return: The index of the first NaN value, or None if no NaN found
    """
    nan_index = df[column_name].isna().idxmax()
    if pd.isna(df[column_name][nan_index]):
        return nan_index
    else:
        return None

def embed_labels(df, output_csv_address, chunk_size=1000, 
                 label='label', embedding_col='embedding'):
    df_output = initialize_output_csv(df, output_csv_address, label='label')
    df[embedding_col] = df[embedding_col].astype('object')
    
    start_index = find_first_nan_index(df_output, embedding_col)

    if start_index is None:
        print("All labels already embedded.")
        return df_output

    pbar = tqdm(total=len(df_output), desc="Embedding Labels", unit="label")

    pbar.update(start_index)

    for chunk_start in range(start_index, len(df_output), chunk_size):
        chunk_end = min(chunk_start + chunk_size, len(df_output))
        labels_chunk = df_output[label][chunk_start:chunk_end]

        for i, label_value in enumerate(labels_chunk):
            if pd.isna(df_output.at[chunk_start + i, embedding_col]):
                embeddings = embed_string(label_value)
                if embeddings is not None:
                    df_output.at[chunk_start + i, embedding_col] = embeddings
            pbar.update(1)

        df_output.to_csv(output_csv_address, index=False)

    pbar.close()
    print("All labels embedded and CSV updated.")

    return df_output

def get_unique_labels(df, label='label'):
    unique_labels = df[label].dropna().unique()
    return unique_labels


def parse_json_col(df, col='embedding'):
    df[col] = df[col].apply(ast.literal_eval)
    return df


def load_embedding(df, label_embedding='embedding', dim=1536):
    
    if not isinstance(df[label_embedding].iloc[0], list):
        df = parse_json_col(df, col=label_embedding)

    embeddings_array = np.array(list(df[label_embedding]), dtype=np.float32)
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings_array)
    return index

def adhoc_search(string, index, topk=10):
    adhoc_embed = embed_string(string)
    D, I = index.search(np.array([adhoc_embed], dtype=np.float32), topk) 
    return D, I

def df_search(df, index, label_embedding='embedding', topk=10):

    if not isinstance(df[label_embedding].iloc[0], list):
        df = parse_json_col(df, col=label_embedding)

    embeddings = list(df[label_embedding])
    D, I = index.search(np.array(embeddings, dtype=np.float32), topk) 
    return D, I

def flatten_append(df, std_df, D, I, topk=10):
    df = df.loc[df.index.repeat(topk)].reset_index(drop=True)
    df['D_values'] = D.flatten()
    df['I_values'] = I.flatten()
    df['rank'] = (df.index % topk) + 1

    for col in std_df.columns:
        df[col] = std_df.iloc[df['I_values'].values][col].values

    return df

def display_matches(reference_df, 
                    input_df, 
                    I, 
                    exclude_columns=['label', 'index_ids', 'embedding'],
                    label_col = 'label'):

    
    current_page = 0

    def create_html_content(page_no):
        label = input_df[label_col][page_no]
        results_indices = I[page_no]
        results = reference_df.iloc[results_indices][label_col]
        results = [result.replace('\n', '<br>') for result in results]
        results_html = "<ul>" + "".join(f"<li>{result}</li>" for result in results) + "</ul>"
        html_content = f"<b>Input Label</b>: {df_row_to_column_value(input_df, idx=page_no, exclude_columns=exclude_columns).to_html()}<br><b>Top 10 Matches</b>:{results_html}"
        return html_content

    def update_html_display(page_no):
        html_display.value = create_html_content(page_no)
        page_label.value = f'Page {page_no + 1} of {len(input_df)}'
    
    def on_prev_clicked(b):
        nonlocal current_page
        if current_page > 0:
            current_page -= 1
            update_html_display(current_page)

    def on_next_clicked(b):
        nonlocal current_page
        if current_page < len(input_df) - 1:
            current_page += 1
            update_html_display(current_page)
    
    html_display = widgets.HTML(value=create_html_content(current_page))
    
    btn_prev = widgets.Button(description='Previous Page')
    btn_next = widgets.Button(description='Next Page')
    
    btn_prev.on_click(on_prev_clicked)
    btn_next.on_click(on_next_clicked)

    page_label = widgets.Label(value=f'Page {current_page + 1} of {len(input_df)}')
    
    navigation_bar = widgets.HBox([btn_prev, page_label, btn_next])
    
    display(navigation_bar, html_display)


def check_functional_dependency(df, determinant, dependent):
    groups = df.groupby(list(determinant))[dependent].nunique()
    is_functionally_dependent = (groups == 1).all()
    return is_functionally_dependent


def get_unique_labels_with_ids(df, label = 'label'):
    df_cleaned = df.dropna(subset=['label'])

    grouped = df_cleaned.groupby('label')

    label_id_df = grouped.apply(lambda x: x.index.tolist()).reset_index(name='index_ids')

    dependent_columns = []

    for column in df_cleaned.columns:
        if column != 'label':
            if grouped[column].nunique().eq(1).all():
                dependent_columns.append(column)
    
    if dependent_columns:
        attributes_data = grouped[dependent_columns].first().reset_index()
        label_id_df = pd.merge(label_id_df, attributes_data, on='label', how='left')

    label_id_df['embedding'] = None

    return label_id_df


def df_row_to_column_value(df, idx=0, exclude_columns=None):
    """
    Create a DataFrame of [column, value] pairs from a specified row index,
    excluding specified columns.

    Parameters:
    - df: The input DataFrame.
    - idx: The index of the row to use.
    - exclude_columns: A set or list of columns to exclude.

    Returns:
    - A new DataFrame with two columns: 'Column' and 'Value'.
    """
    
    if exclude_columns is None:
        exclude_columns = []
        
    df_reduced = df.drop(columns=exclude_columns, errors='ignore')

    if idx not in df_reduced.index:
        raise IndexError(f"Index {idx} is out of bounds for the DataFrame.")

    row_transposed = df_reduced.iloc[idx].transpose()

    new_df = pd.DataFrame(row_transposed)
    new_df.reset_index(inplace=True)
    new_df.columns = ['Column', 'Value']

    return new_df

def extract_json_code_safe(s):
    s_stripped = s.strip()
    if (s_stripped.startswith('{') and s_stripped.endswith('}')) or \
       (s_stripped.startswith('[') and s_stripped.endswith(']')):
        return s_stripped
    return extract_json_code(s_stripped)

def extract_json_code(s):
    import re
    pattern = r"```json(.*?)```"
    match = re.search(pattern, s, re.DOTALL)
    return match.group(1).strip() if match else None

def compute_cluster(df, match="matches"):
    clusters = {}

    for idx, row in df.iterrows():
        entry = row[match]
        if 'similar_to' not in entry:
            clusters[idx] = []

    for idx, row in df.iterrows():
        entry = row[match]
        if 'similar_to' in entry:
            similar_to_idx = entry['similar_to']
            if similar_to_idx in clusters:
                clusters[similar_to_idx].append(idx)
            elif similar_to_idx not in clusters:
                clusters[similar_to_idx] = [idx]
    return clusters

def generate_report_for_cluster(df, clusters, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):
    middle_html = ""

    for i in clusters:

        js = clusters[i] 
        middle_html += f"""<h1>{i+1}</h1>
{generate_page_clusters(df=df, clusters=clusters, i=i, js=js, exclude_columns=exclude_columns, match_col=match_col)}"""

    title_html = f'<div style="display: flex; align-items: center;">' \
        f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
        f'<div style="margin: 0; padding: 0;">' \
        f'<h1 style="margin: 0; padding: 0;">Table Standardization</h1>' \
        f'<p style="margin: 0; padding: 0;">Powered by cocoon</p>' \
        f'</div>' \
        f'</div><hr>'


    full_html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Cocoon Standardization</title>
    <head>
        <style>
    /* CSS for the overall appearance: fonts, spacing, etc. */
    body {{
        font-family: "Arial";
        margin: 40px;
        background-color: #d9ead3;  /* Matching the light green background of the icon */
    }}

    .container {{
        max-width: 900px;  /* Setting a maximum width for the content */
        margin: 0 auto;   /* Centering the container */
        background-color: #ffffff; /* White background for the main content */
        padding: 20px;
        border-radius: 5px; /* Rounded corners */
        box-shadow: 0 0 10px rgba(0,0,0,0.1); /* A subtle shadow to lift the container */
    }}

    h1 {{
        font-size: 24px;
        color: #274e13; /* Dark green color matching the icon */
        padding-bottom: 10px;
        margin-bottom: 20px;
    }}

    h2 {{
        font-size: 20px;
        margin-top: 20px;
        margin-bottom: 15px;
    }}

    ul, ol {{
        padding-left: 20px;
    }}

    li {{
        margin-bottom: 10px;    }}

    p {{        margin-bottom: 20px;
        text-align: justify; /* To align the text on both sides */
    }}

    /* CSS specific to the table elements */
    table {{
        font-family: Arial, sans-serif;
        border-collapse: collapse; /* Ensures there are no double borders */
        width: 100%;
        table-layout: auto;
    }}

    th, td {{
        border: 1px solid #dddddd; /* Light grey borders */
        text-align: left;
        padding: 8px; /* Make text not touch the borders */
    }}

    th {{
        background-color: #b6d7a8; /* Light green background matching the icon */
    }}

    tr:nth-child(even) {{
        background-color: #d9ead3; /* Light green background matching the icon */
    }}

    tr:hover {{
        background-color: #c9d9b3; /* A slightly darker green for hover effect */
    }}
    </style>
</head>
<body>
    
    <div class="container">
        {title_html}
        {middle_html}
    </div>
</body>
</html>
"""
    return full_html

def entity_relation_match(input_df, I, refernece_df, attributes=None, label = "label", match="matches"):
    
    if match not in input_df:
        input_df[match] = None

    if attributes is None:
        attributes = input_df.columns.tolist()
        attributes.remove("label")
        attributes.remove("index_ids")
        attributes.remove("embedding")

    for idx in range(len(input_df)):
        print(f"💪 Working on the row {idx+1} ...")

        if input_df[match][idx] is not None:
            continue

        input_desc = ""
        for attribute in attributes:
            input_desc += (attribute + ": " + input_df.iloc[idx][attribute] + "\n")

        refernece_desc = ""
        for i, output in enumerate(refernece_df[label].iloc[I[idx]]): 
            refernece_desc += (str(i+1) + ". " + output + "\n")

        template = f"""Your goal is to build relations between input and reference entities.

The input entity has the following attributes:
{input_desc}
Below are reference entities:
{refernece_desc}
Do the following:
1. Read input entity attributes and guess what it is about.

2. Go through each output entity. Describe what it is and reason its relation.
For instance, given the input entity "small car":
if the same entity then EXACT_MATCH. 
    E.g., "small automobile"
else if has assumptions that is clearly wrong then CONFLICTED_ASSUMPTION
    E.g., "big car" is wrong because input entity clearly specify size as "small"
else if additional assumptions that can't be verified then ADDITIONAL_ASSUMPTION
    E.g., "electronic car" is additional battery assumption can't be verified
else if it is general super class then GENERAL 
    E.g., "small vehicle" and "car" are general category of "small car"
else it is irrelavent entity then NOT_RELATED
    E.g., "cloth" is a irrelavent

The list is prioritized. Choose the first one that applies. Provide your answer as json:
```json
{{       
        "Input Entity Guess": "...",
        "EXACT_MATCH": {{
                "reason": "The input entity is ... which matches ...",
                "entity": [...] (A list of reference entity names)

        }},
        "CONFLICTED_ASSUMPTION": {{
                "reason": "The (what specific) details are conflicted",
                "entity": [...]

        }},
        "ADDITIONAL_ASSUMPTION": {{
                "reason": "The (what specific) details are not mentioned",
                "entity": [...]

        }},
        "GENERAL": {{
                "reason": "How these entites are more general",
                "entity": [...]
        }}
        (DON'T include NOT_RELATED!)
}}
```
"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_var = json.loads(json_code)
        input_df.at[idx, match] = json_var

def entity_relation_match_cluster(input_df, I, refernece_df, attributes=None, label = "label", match="matches", verbose=False):

    emdeb_searcher = EmbeddingSearcher(input_df)

    if match not in input_df:
        input_df[match] = None


    for idx in range(len(input_df)):
        if input_df[match][idx] is not None:
            emdeb_searcher.remove_rows(idx)

    if attributes is None:
        attributes = input_df.columns.tolist()
        attributes.remove("label")
        attributes.remove("index_ids")
        attributes.remove("embedding")

    while not emdeb_searcher.is_index_empty():
        print(f"👉 {emdeb_searcher.get_size()} rows remain...")
        

        idx = emdeb_searcher.get_valid_id()

        emdeb_searcher.remove_rows(idx)
            
        input_desc = ""
        for attribute in attributes:
            input_desc += (attribute + ": " + input_df.iloc[idx][attribute] + "\n")
        
        reference_entities = list(refernece_df[label].iloc[I[idx]])

        refernece_desc = ""
        for i, output in enumerate(refernece_df[label].iloc[I[idx]]): 
            refernece_desc += (str(i+1) + ". " + output + "\n")
        if verbose:
            print(f"👉 Input: {input_desc}")
            print(f"👉 Reference: {refernece_desc}")

        json_var = entity_relation_match_one(input_desc, refernece_desc)

        def replace_indices_with_entities(json_var, reference_entities):
            for category in json_var:
                if isinstance(json_var[category], dict) and "entity" in json_var[category]:
                    json_var[category]["entity"] = [reference_entities[int(idx) - 1] for idx in json_var[category]["entity"]]
            return json_var

        json_var = replace_indices_with_entities(json_var, reference_entities)

        if verbose:        
            print(f"👉 Match: {json.dumps(json_var, indent=4)}")

        all_indicies = []



        related_rows = emdeb_searcher.search_by_row_index(idx, k=30)

        if len(related_rows) > 0:





            entity_desc = json_var["Summary of Relations"]
            refernece_desc = related_rows[attributes].reset_index(drop=True).to_csv(quoting=1)

            json_var2 = find_relation_satisfy_description(entity_desc=entity_desc, 
                                                        related_rows_desc_str=refernece_desc)
            
            indicies = json_var2["indices"]

            ids = []
            for index in indicies:
                ids.append(list(related_rows.index)[index])
            
            all_indicies += ids
            emdeb_searcher.remove_rows(ids)


            json_var3 = {"similar_to": idx}
            for i, index in enumerate(all_indicies):
                input_df.at[index, match] = json_var3
        
        input_df.at[idx, match] = json_var

def generate_html_from_json_entity(json_var):
    html_output = f"<p>&#x1F913; <b>This input entity is about:</b> <i>{json_var['Input Entity Guess']}</i><p>"
    if json_var['EXACT_MATCH']['entity']:
        html_output += "<p>&#x1F600; We find <u>exactly matched</u> entities:</p>"
        html_output += "<ul>"
        for entity in json_var['EXACT_MATCH']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['EXACT_MATCH']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['EXACT_MATCH']['reason']}</i></p>"
        html_output += "</body></html>"
    else:
        html_output += "<p>&#x1F641; We can't find <u>exactly matched</u> entities.</p>"

    if json_var['GENERAL']['entity']:
        html_output += "<p>&#x1F600; We find entities that are <u>more general</u>:</p>"
        html_output += "<ul>"
        for entity in json_var['GENERAL']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['GENERAL']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['GENERAL']['reason']}</i></p>"

    if json_var['ADDITIONAL_ASSUMPTION']['entity']:
        html_output += "<p>&#x1F600; We find entities but with <u>additional details</u> that need verification:</p>"
        html_output += "<ul>"
        for entity in json_var['ADDITIONAL_ASSUMPTION']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['ADDITIONAL_ASSUMPTION']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['ADDITIONAL_ASSUMPTION']['reason']}</i></p>"

    if json_var['CONFLICTED_ASSUMPTION']['entity']:
        html_output += "<p>&#x1F600; We find entities that are <u>similar, but details are different</u>:</p>"
        html_output += "<ul>"
        for entity in json_var['CONFLICTED_ASSUMPTION']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['CONFLICTED_ASSUMPTION']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['CONFLICTED_ASSUMPTION']['reason']}</i></p>"

    return html_output


def generate_entity_page(df, i=0, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col="matches"):

    if isinstance(df[match_col].iloc[0], str):
        df = parse_json_col(df, col=match_col)
    
    input_data_html = df_row_to_column_value(df, idx=i, exclude_columns=exclude_columns).to_html(index=False)
    json_var = df[match_col][i]
    solution_output_html = generate_html_from_json_entity(json_var)

    return f"""<h1>Input Data</h1>
{input_data_html}
<h1>Match Result</h1>
{solution_output_html}"""

def display_entity_matches(df, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col="matches"):
    if isinstance(df[match_col].iloc[0], str):
        df = parse_json_col(df, col=match_col)

    current_page = 0

    def create_html_content(page_no):
        return generate_entity_page(df, page_no, exclude_columns, match_col)

    def update_html_display(page_no):
        html_display.value = create_html_content(page_no)
        page_label.value = f'Page {page_no + 1} of {len(df)}'
    
    def on_prev_clicked(b):
        nonlocal current_page
        if current_page > 0:
            current_page -= 1
            update_html_display(current_page)

    def on_next_clicked(b):
        nonlocal current_page
        if current_page < len(df) - 1:
            current_page += 1
            update_html_display(current_page)
    
    html_display = widgets.HTML(value=create_html_content(current_page))
    
    btn_prev = widgets.Button(description='Previous Page')
    btn_next = widgets.Button(description='Next Page')
    
    btn_prev.on_click(on_prev_clicked)
    btn_next.on_click(on_next_clicked)

    page_label = widgets.Label(value=f'Page {current_page + 1} of {len(df)}')
    
    navigation_bar = widgets.HBox([btn_prev, page_label, btn_next])
    
    display(navigation_bar, html_display)

def generate_html_from_json(json_var):
    
    html_output = ""
    
    if 'Input Entity Guess' in json_var:
        html_output += f"<p>&#x1F913; <b>This input entity is about:</b> <i>{json_var.get('Input Entity Guess', 'Unknown')}</i></p>"

    exact_matches = json_var.get('EXACT_MATCH', {})
    if exact_matches.get('entity'):
        html_output += "<p>&#x1F600; We find <u>exactly matched</u> entities:</p><ul>"
        for entity in exact_matches['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if exact_matches.get('reason'):
            html_output += f"<p><b>Reason:</b> <i>{exact_matches['reason']}</i></p>"
    else:
        html_output += "<p>&#x1F641; We can't find <u>exactly matched</u> entities.</p>"

    general_matches = json_var.get('GENERAL', {})
    if general_matches.get('entity'):
        html_output += "<p>&#x1F600; We find entities that are <u>more general</u>:</p><ul>"
        for entity in general_matches['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if general_matches.get('reason'):
            html_output += f"<p><b>Reason:</b> <i>{general_matches['reason']}</i></p>"

    additional_assumptions = json_var.get('ADDITIONAL_ASSUMPTION', {})
    if additional_assumptions.get('entity'):
        html_output += "<p>&#x1F600; We find entities but with <u>additional details</u> that need verification:</p><ul>"
        for entity in additional_assumptions['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if additional_assumptions.get('reason'):
            html_output += f"<p><b>Reason:</b> <i>{additional_assumptions['reason']}</i></p>"

    conflicted_assumptions = json_var.get('CONFLICTED_ASSUMPTION', {})
    if conflicted_assumptions.get('entity'):
        html_output += "<p>&#x1F600; We find entities that are <u>similar, but details are different</u>:</p><ul>"
        for entity in conflicted_assumptions['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if conflicted_assumptions.get('reason'):
            html_output += f"<p><b>Reason:</b> <i>{conflicted_assumptions['reason']}</i></p>"

    return html_output




def generate_page_clusters(df, clusters, i=0, js=None, exclude_columns=['label', 'index_ids', 'embedding', 'matches'], match_col='matches'):
    if js is None:
        js = []
    input_data_html = df_row_to_column_value(df, idx=i, exclude_columns=exclude_columns).to_html(index=False)
    similar_data_html = ""
    for j in clusters[i]:
        similar_data_html += df_row_to_column_value(df, idx=j, exclude_columns=exclude_columns).to_html(index=False)
    json_var = df[match_col][i]
    solution_output_html = generate_html_from_json(json_var)

    similar_data_section = ""
    if len(js) > 0:
        similar_data_section = f"<h1>Similar Data</h1>\n{similar_data_html}"

    return f"""<h1>Input Data</h1>
{input_data_html}
{similar_data_section}
<h1>Match Result</h1>
{solution_output_html}"""


def generate_page(df, i=0, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):
    input_data_html = df_row_to_column_value(df, idx=i, exclude_columns=exclude_columns).to_html(index=False)
    json_var = df[match_col][i]
    solution_output_html = generate_html_from_json(json_var)

    return f"""<h1>Input Data</h1>
{input_data_html}
<h1>Match Result</h1>
{solution_output_html}"""

def write_report_to_file(df, output_html_address, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):

    full_html = generate_report(df, exclude_columns=exclude_columns)

    with open(output_html_address, 'w') as f:
        f.write(full_html)

def print_report(df, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):
    
    full_html = generate_report(df, exclude_columns=exclude_columns)

    display(HTML(full_html))

def generate_report(df, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):
    middle_html = ""

    for i in range(len(df)):
        json_var = df[match_col][i]

        middle_html += f"""<h1>{i+1}</h1>
{generate_page(df=df, i=i, exclude_columns=exclude_columns, match_col=match_col)}"""

    title_html = f'<div style="display: flex; align-items: center;">' \
        f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
        f'<div style="margin: 0; padding: 0;">' \
        f'<h1 style="margin: 0; padding: 0;">Fuzzy Join</h1>' \
        f'<p style="margin: 0; padding: 0;">Powered by cocoon</p>' \
        f'</div>' \
        f'</div><hr>'


    full_html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Cocoon Standardization</title>
    <head>
        <style>
    /* CSS for the overall appearance: fonts, spacing, etc. */
    body {{
        font-family: "Arial";
        margin: 40px;
        background-color: #d9ead3;  /* Matching the light green background of the icon */
    }}

    .container {{
        max-width: 900px;  /* Setting a maximum width for the content */
        margin: 0 auto;   /* Centering the container */
        background-color: #ffffff; /* White background for the main content */
        padding: 20px;
        border-radius: 5px; /* Rounded corners */
        box-shadow: 0 0 10px rgba(0,0,0,0.1); /* A subtle shadow to lift the container */
    }}

    h1 {{
        font-size: 24px;
        color: #274e13; /* Dark green color matching the icon */
        padding-bottom: 10px;
        margin-bottom: 20px;
    }}

    h2 {{
        font-size: 20px;
        margin-top: 20px;
        margin-bottom: 15px;
    }}

    ul, ol {{
        padding-left: 20px;
    }}

    li {{
        margin-bottom: 10px;    }}

    p {{        margin-bottom: 20px;
        text-align: justify; /* To align the text on both sides */
    }}

    /* CSS specific to the table elements */
    table {{
        font-family: Arial, sans-serif;
        border-collapse: collapse; /* Ensures there are no double borders */
        width: 100%;
        table-layout: auto;
    }}

    th, td {{
        border: 1px solid #dddddd; /* Light grey borders */
        text-align: left;
        padding: 8px; /* Make text not touch the borders */
    }}

    th {{
        background-color: #b6d7a8; /* Light green background matching the icon */
    }}

    tr:nth-child(even) {{
        background-color: #d9ead3; /* Light green background matching the icon */
    }}

    tr:hover {{
        background-color: #c9d9b3; /* A slightly darker green for hover effect */
    }}
    </style>
</head>
<body>
    
    <div class="container">
        {title_html}
        {middle_html}
    </div>
</body>
</html>
"""
    return full_html


def CRS_select(callbackfunc=None):

    checkbox1 = widgets.Checkbox(description='String', value=True, indent=False)
    checkbox2 = widgets.Checkbox(description='Parameters', value=False, indent=False)

    text_input1 = widgets.Text(description='', indent=False)

    box1 = widgets.VBox([text_input1])
    
    projection_label = widgets.Label(value='Projection Parameters', style={'font_weight': 'bold'})
    projection_description = widgets.Label(value='Select the type of map projection.')

    proj_options = [
        ('Albers Equal Area', 'aea'), 
        ('Mercator', 'merc'), 
        ('Lambert Conformal Conic', 'lcc'), 
        ('Universal Transverse Mercator', 'utm'), 
        ('Longitude/Latitude', 'longlat'),
        ('Stereographic', 'stere'),
        ('Gnomonic', 'gnom'),
        ('Lambert Azimuthal Equal Area', 'laea'),
        ('Transverse Mercator', 'tmerc'),
        ('Robinson', 'robin')
    ]
    proj_dropdown = widgets.Dropdown(options=proj_options, value='aea', description='Projection:')

    def update_visibility(*args):
        projection_uses = {
            'aea': {'lat_lon': True, 'standard_parallels': True, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'merc': {'lat_lon': True, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'lcc': {'lat_lon': True, 'standard_parallels': True, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'utm': {'lat_lon': False, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'longlat': {'lat_lon': False, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': False, 'scale_factor': False},
            'stere': {'lat_lon': True, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'gnom': {'lat_lon': True, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'laea': {'lat_lon': True, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'tmerc': {'lat_lon': True, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True},
            'robin': {'lat_lon': True, 'standard_parallels': False, 'ellipsoid_datum': True, 'false_easting_northing': True, 'scale_factor': True}
        }

        uses = projection_uses[proj_dropdown.value]
        lat_0.disabled = not uses['lat_lon']
        lon_0.disabled = not uses['lat_lon']
        lat_1.disabled = not uses['standard_parallels']
        lat_2.disabled = not uses['standard_parallels']
        ellps_dropdown.disabled = not uses['ellipsoid_datum']
        datum_dropdown.disabled = not uses['ellipsoid_datum']
        false_easting.disabled = not uses['false_easting_northing']
        false_northing.disabled = not uses['false_easting_northing']
        scale_factor.disabled = not uses['scale_factor']


    proj_dropdown.observe(update_visibility, 'value')

    projection_group = widgets.VBox([projection_label, projection_description, proj_dropdown])

    central_meridian_label = widgets.Label(value='Central Meridian and Latitude of Origin', style={'font_weight': 'bold'})
    central_meridian_description = widgets.Label(value='Define the central point of the projection.')

    lat_0 = widgets.FloatText(value=0, description='lat_0:')
    lon_0 = widgets.FloatText(value=0, description='lon_0:')

    central_meridian_group = widgets.VBox([central_meridian_label, central_meridian_description, lat_0, lon_0])



    standard_parallels_label = widgets.Label(value='Standard Parallels', style={'font_weight': 'bold'})
    standard_parallels_description = widgets.Label(value='Standard parallels for accurate map scale.')

    lat_1 = widgets.FloatText(value=20, description='lat_1:')
    lat_2 = widgets.FloatText(value=60, description='lat_2:')

    standard_parallels_group = widgets.VBox([standard_parallels_label, standard_parallels_description, lat_1, lat_2])

    ellipsoid_datum_label = widgets.Label(value='Ellipsoid and Datum Parameters', style={'font_weight': 'bold'})
    ellipsoid_datum_description = widgets.Label(value='Choose the ellipsoid model and datum.')

    ellps_options = [
        'WGS84', 'GRS80', 'Airy', 'Bessel', 'Clarke1866', 'Clarke1880', 
        'International', 'Krassovsky', 'GRS67', 'Australian', 'Hayford', 
        'Helmert1906', 'IERS1989', 'IERS2003', 'SouthAmerican1969'
    ]
    ellps_dropdown = widgets.Dropdown(options=ellps_options, value='WGS84', description='Ellipsoid:')


    datum_options = [
        'WGS84', 'NAD83', 'NAD27', 'ED50', 'OSGB36', 'GDA94', 'Pulkovo1942', 
        'AGD66', 'AGD84', 'SAD69', 'Ireland1965', 'NZGD49', 'ATS77', 'JGD2000'
    ]
    datum_dropdown = widgets.Dropdown(options=datum_options, value='WGS84', description='Datum:')


    ellipsoid_datum_group = widgets.VBox([ellipsoid_datum_label, ellipsoid_datum_description, ellps_dropdown, datum_dropdown])

    false_easting_northing_label = widgets.Label(value='False Easting and Northing', style={'font_weight': 'bold'})
    false_easting_northing_description = widgets.Label(value='Shifts the origin of the map by specific units.')

    false_easting = widgets.FloatText(value=0, description='+x_0:')
    false_northing = widgets.FloatText(value=0, description='+y_0:')

    false_easting_northing_group = widgets.VBox([false_easting_northing_label, false_easting_northing_description, false_easting, false_northing])


    scale_factor_label = widgets.Label(value='Scale Factor', style={'font_weight': 'bold'})
    scale_factor_description = widgets.Label(value='Multiplier to reduce or increase the scale of the projection.')

    scale_factor = widgets.FloatText(value=1.0, description='+k:')

    scale_factor_group = widgets.VBox([scale_factor_label, scale_factor_description, scale_factor])


    submit_button = widgets.Button(description='Create CRS')

    def on_submit_clicked(b):
        crs_string = None
        if checkbox1.value and not checkbox2.value:
            crs_string = text_input1.value
        elif checkbox2.value and not checkbox1.value:
            parts = [f"+proj={proj_dropdown.value}"]

            if not lat_0.disabled:
                parts.append(f"+lat_0={lat_0.value}")
            if not lon_0.disabled:
                parts.append(f"+lon_0={lon_0.value}")
            if not lat_1.disabled:
                parts.append(f"+lat_1={lat_1.value}")
            if not lat_2.disabled:
                parts.append(f"+lat_2={lat_2.value}")
            if not ellps_dropdown.disabled:
                parts.append(f"+ellps={ellps_dropdown.value}")
            if not datum_dropdown.disabled:
                parts.append(f"+datum={datum_dropdown.value}")
            if not false_easting.disabled:
                parts.append(f"+x_0={false_easting.value}")
            if not false_northing.disabled:
                parts.append(f"+y_0={false_northing.value}")
            if not scale_factor.disabled:
                parts.append(f"+k={scale_factor.value}")

            crs_string = " ".join(parts)

        callbackfunc(crs_string)

    submit_button.on_click(on_submit_clicked)


    update_visibility()

    box2 = widgets.VBox([projection_group, 
    central_meridian_group, 
    standard_parallels_group, 
    ellipsoid_datum_group, 
    false_easting_northing_group,
    scale_factor_group
    ])

    box1.layout.display = 'flex'
    box2.layout.display = 'none'

    def update_widgets(change):
        
        if change['owner'] == checkbox1:
            checkbox2.value = not checkbox1.value
            box1.layout.display = 'none' if not checkbox1.value else 'flex'
            box2.layout.display = 'none' if checkbox1.value else 'flex'
        elif change['owner'] == checkbox2:
            checkbox1.value = not checkbox2.value
            box1.layout.display = 'none' if checkbox2.value else 'flex'
            box2.layout.display = 'none' if not checkbox2.value else 'flex'

    checkbox1.observe(update_widgets, names='value')
    checkbox2.observe(update_widgets, names='value')

    display(checkbox1, box1, checkbox2, box2, submit_button)




def lightweight_copy(df, columns_to_copy=None):
    
    
    
    
    
    if columns_to_copy is None:
        columns_to_copy = []
        
    new_df = pd.DataFrame(index=df.index, columns=df.columns)
    
    for col in df.columns:
        if col in columns_to_copy:
            new_df[col] = df[col].copy()
        else:
            new_df[col] = df[col]

    return new_df



def plot_efficient_grid(df, x_col, y_col, value_col=None):

    if value_col:
        value_columns = [value_col]
    else:
        numeric_columns = df.select_dtypes(include=np.number).columns
        value_columns = [col for col in numeric_columns if col not in [x_col, y_col]]
        value_columns = value_columns[:5]

    n_rows = len(value_columns)
    fig, axs = plt.subplots(n_rows, 1, figsize=(6, 3 * n_rows))

    if n_rows == 1:
        axs = [axs]

    for i, value_col in enumerate(value_columns):
        pivot_table = df.pivot_table(index=y_col, columns=x_col, values=value_col, aggfunc='sum', fill_value=0)
        if isinstance(pivot_table.columns, pd.MultiIndex):
            pivot_table.columns = pivot_table.columns.get_level_values(1)

        pivot_table = pivot_table.where(pd.notnull(pivot_table), np.nan)

        cmap = plt.cm.viridis
        cmap.set_bad(color='white')

        im = axs[i].imshow(pivot_table.values, cmap=cmap)
        axs[i].set_title(f"Heatmap for {value_col}")

        plt.colorbar(im, ax=axs[i], fraction=0.046, pad=0.04, aspect=10)

    plt.tight_layout()
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
    return image_base64

def are_crs_equivalent(crs_string1, crs_string2):

    crs1 = CRS(crs_string1)
    crs2 = CRS(crs_string2)
    return crs1 == crs2

def apply_affine_transform(affine_matrix, x, y, offset='ul'):
    homogeneous_coords = np.vstack([x, y, np.ones_like(x)])

    transformed_x, transformed_y, _ = np.dot(affine_matrix, homogeneous_coords)

    if offset == 'center':
        transformed_x += affine_matrix[0, 2] / 2
        transformed_y += affine_matrix[1, 2] / 2
    elif offset == 'll':
        transformed_y += affine_matrix[1, 2]

    return transformed_x, transformed_y

def xy_to_colrow(affine_matrix, x_coords, y_coords, offset='ul'):
    if offset == 'center':
        x_coords -= affine_matrix[0, 0] / 2
        y_coords -= affine_matrix[1, 1] / 2
    elif offset == 'll':
        y_coords -= affine_matrix[1, 1]

    homogeneous_coords = np.vstack([x_coords, y_coords, np.ones_like(x_coords)])

    inverse_affine = np.linalg.inv(affine_matrix)
    col, row, _ = np.dot(inverse_affine, homogeneous_coords)

    return np.round(col).astype(int), np.round(row).astype(int)

def plot_np_array(np_array, meta, band_names=None):
    if 'nodata' in meta.keys():
        np_array = np.ma.masked_equal(np_array, meta['nodata'])

    if np_array.ndim == 2:
        plt.figure(figsize=(6,3))

        im = plt.imshow(np_array, cmap='viridis')
        plt.colorbar(im, fraction=0.046, pad=0.04, aspect=10)
        
    elif np_array.ndim == 3:
        plt.figure(figsize=(6,3* np_array.shape[0]))
        fig, axs = plt.subplots(np_array.shape[0], 1, figsize=(6, 3 * np_array.shape[0]))
        for i in range(np_array.shape[0]):
            ax = axs[i] if np_array.shape[0] > 1 else axs
            im = ax.imshow(np_array[i], cmap='viridis')
            if band_names is not None:
                ax.set_title(band_names[i])
            else:
                ax.set_title(f"Band {i+1}")
            fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04, aspect=10)
            
    plt.tight_layout()
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')

    return image_base64


def plot_coarse_raster_file(file_path):
    
    with rasterio.open(file_path) as src:
        max_dimension = max(src.width, src.height)
        downscale_factor = max(1, max_dimension // 1000)

        out_shape = (
            src.count,
            int(src.height // downscale_factor),
            int(src.width // downscale_factor)
        )

        image = src.read(out_shape=out_shape)

        if src.count == 3:
            plt.imshow(np.rollaxis(image), cmap='viridis')
        else:
            fig, axs = plt.subplots(1, src.count)
            for i in range(src.count):
                if src.count == 1:
                    ax = axs
                else:
                    ax = axs[i]
                ax.imshow(image[i], cmap='viridis')
                ax.set_title(f"Band {i+1}")
            plt.tight_layout()

    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')

    return image_base64


def plot_coarse_raster(raster):
    max_dimension = max(raster.width, raster.height)
    downscale_factor = max(1, max_dimension // 1000)

    out_shape = (
        raster.count,
        int(raster.height // downscale_factor),
        int(raster.width // downscale_factor)
    )

    image = raster.read(out_shape=out_shape)

    nodata_value = raster.nodata

    if nodata_value is not None:
        image = np.ma.masked_equal(image, nodata_value)

    if raster.count == 3:
        img_plot = plt.imshow(np.rollaxis(image, 0, 3), cmap='viridis')
        _set_original_scale_ticks(img_plot, raster.width, raster.height, downscale_factor)
    else:
        fig, axs = plt.subplots(1, raster.count)
        for i in range(raster.count):
            ax = axs if raster.count == 1 else axs[i]
            img_plot = ax.imshow(image[i], cmap='viridis')
            ax.set_title(f"Band {i+1}")
            _set_original_scale_ticks(img_plot, raster.width, raster.height, downscale_factor)
        plt.tight_layout()

    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')

    return image_base64

def _set_original_scale_ticks(img_plot, original_width, original_height, downscale_factor):
    xticks = np.linspace(0, img_plot.get_array().shape[1], num=5)
    xtick_labels = [f"{int(x * downscale_factor)}" for x in xticks]
    plt.xticks(xticks, xtick_labels)

    yticks = np.linspace(0, img_plot.get_array().shape[0], num=5)
    ytick_labels = [f"{int(y * downscale_factor)}" for y in yticks]
    plt.yticks(yticks, ytick_labels)
    plt.xlabel('x')
    plt.ylabel('y')

def plot_gdf(gdf):
    if not isinstance(gdf, gpd.GeoDataFrame):
        raise TypeError("Input must be a geopandas GeoDataFrame")

    gdf.plot(figsize=(6, 3))

    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')

    return image_base64


def create_geotransform(bounds, resolution):

    width = int((bounds[2] - bounds[0]) / resolution)
    height = int((bounds[3] - bounds[1]) / resolution)

    if width == 0 or height == 0:
        raise ValueError(f"""Resolution is too large for the given geometry. 
Current resolution: {resolution}
Geometry bounds: {bounds}                   
The width and height of the raster must be at least 1 pixel.""")
    
    transform = from_origin(bounds[0], bounds[3], resolution, resolution)
    return transform




def rasterize_gdf(gdf, transform=None, resolution=None, dtype='uint8', nodata=0):
    
    if transform is None:
        if resolution is None:
            raise ValueError("Either transform or resolution must be provided.")
        bounds = gdf.total_bounds
        width = int((bounds[2] - bounds[0]) / resolution)
        height = int((bounds[3] - bounds[1]) / resolution)
        
        transform = create_geotransform(bounds, resolution)

    else:
        if not isinstance(transform, Affine):
            raise TypeError("Transform must be of type Affine.")
        bounds = gdf.total_bounds
        width = int((bounds[2] - bounds[0]) / transform.a)
        height = int((bounds[3] - bounds[1]) / -transform.e)

        if width == 0 or height == 0:
            raise ValueError(f"""Transform is too large for the given geometry.
Current transform: {transform}
Geometry bounds: {bounds}
The width and height of the raster must be at least 1 pixel.""")

    rasterized = rasterize(
        ((geometry, 1) for geometry in gdf.geometry),
        out_shape=(height, width),
        transform=transform,
        dtype=dtype,
        fill=nodata
    )

    meta = {
        'driver': 'GTiff',
        'height': height,
        'width': width,
        'count': 1,
        'dtype': dtype,
        'crs': gdf.crs,
        'transform': transform,
        'nodata': nodata
    }

    return rasterized, meta

















def standardize_crs_bbox(target_transform, bbox):
    left, bottom, right, top = bbox

    window = from_bounds(left, bottom, right, top, target_transform)

    return window

def standardize_crs_geo_df(target_crs, gdf):
    gdf = gdf.to_crs(target_crs)
    return gdf

def read_raster_to_numpy(raster, window=None):

    all_bands = raster.read(window=window)

    return all_bands



def get_meta_from_gdf(gdf):
    meta = {
        'crs': gdf.crs.to_string(),
        'bounds': gdf.total_bounds
    }

    width = int((meta['bounds'][2] - meta['bounds'][0]) / meta['resolution'][0])
    height = int((meta['bounds'][3] - meta['bounds'][1]) / meta['resolution'][1])

    meta['width'] = width
    meta['height'] = height

    meta['transform'] = from_bounds(*meta['bounds'], width, height)

    return meta

def write_raster_to_disk(self, meta, output_path):
    with rasterio.open(output_path, 'w', **meta) as dst:
        for i in range(1, self.raster.count + 1):
            data = self.raster.read(i)
            dst.write(data, i)



def resample_raster_to_transform(raster, target_transform, resampling_method=None):
    if resampling_method is None:
        resampling_method = Resampling.bilinear
        
    src_meta = raster.meta.copy()
    
    scale_x = target_transform.a
    scale_y = -target_transform.e
    new_width = int(raster.width * abs(raster.transform.a / scale_x))
    new_height = int(raster.height * abs(raster.transform.e / scale_y))

    resampled_meta = src_meta.copy()
    resampled_meta.update({
        'transform': target_transform,
        'width': new_width,
        'height': new_height
    })

    resampled_raster = rasterio.open('./tmp/temp.tif', 'w+', **resampled_meta)
    for i in range(1, raster.count + 1):
        reproject(
            source=rasterio.band(raster, i),
            destination=rasterio.band(resampled_raster, i),
            src_transform=raster.transform,
            src_crs=raster.crs,
            dst_transform=target_transform,
            dst_crs=raster.crs,
            resampling=resampling_method)

    resampled_raster_data = resampled_raster.read()
    resampled_raster.close()

    return resampled_raster_data, resampled_meta


def resample_array(np_array, bounds, src_meta, target_transform=None, resolution=None, resampling_method=None):
    if resampling_method is None:
        resampling_method = Resampling.bilinear
    
    if target_transform is None:
        if resolution is None:
            raise ValueError("Either target_transform or resolution must be provided.")

        width = int((bounds[2] - bounds[0]) / resolution)
        height = int((bounds[3] - bounds[1]) / resolution)

        target_transform = create_geotransform(bounds, resolution)

    nodata = src_meta.get('nodata', -9999)

    np_array = np_array.astype(np.float32)

    if np_array.ndim == 2:
        return resample_2d_array(np_array, src_meta, target_transform, resampling_method, nodata)
    elif np_array.ndim == 3:
        return resample_3d_array(np_array, src_meta, target_transform, resampling_method, nodata)
    else:
        raise ValueError("Array must be either 2D or 3D")

def resample_2d_array(np_array, src_meta, target_transform, resampling_method, nodata):
    scale_x, scale_y = target_transform.a, -target_transform.e
    new_width = int(src_meta['width'] * abs(src_meta['transform'].a / scale_x))
    new_height = int(src_meta['height'] * abs(src_meta['transform'].e / scale_y))

    resampled_array = np.full((new_height, new_width), nodata, dtype=np_array.dtype)

    resampled_meta = src_meta.copy()
    resampled_meta.update({
        'transform': target_transform,
        'width': new_width,
        'height': new_height,
        'nodata': nodata
    })

    reproject(
        source=np_array,
        destination=resampled_array,
        src_transform=src_meta['transform'],
        src_crs=src_meta['crs'],
        dst_transform=target_transform,
        dst_crs=src_meta['crs'],
        resampling=resampling_method,
        src_nodata=nodata,
        dst_nodata=nodata)

    return resampled_array, resampled_meta

def resample_3d_array(np_array, src_meta, target_transform, resampling_method, nodata):
    num_bands, src_height, src_width = np_array.shape

    scale_x, scale_y = target_transform.a, -target_transform.e
    new_width = int(src_width * abs(src_meta['transform'].a / scale_x))
    new_height = int(src_height * abs(src_meta['transform'].e / scale_y))

    resampled_array = np.full((num_bands, new_height, new_width), nodata, dtype=np_array.dtype)

    resampled_meta = src_meta.copy()
    resampled_meta.update({
        'transform': target_transform,
        'width': new_width,
        'height': new_height,
        'nodata': nodata
    })

    for band in range(num_bands):
        reproject(
            source=np_array[band, :, :],
            destination=resampled_array[band, :, :],
            src_transform=src_meta['transform'],
            src_crs=src_meta['crs'],
            dst_transform=target_transform,
            dst_crs=src_meta['crs'],
            resampling=resampling_method,
            src_nodata=nodata,
            dst_nodata=nodata)

    return resampled_array, resampled_meta



def reproject_raster(raster, meta, target_crs):

    src_crs = meta['crs']
    src_width = meta['width']
    src_height = meta['height']
    src_transform = meta['transform']

    left, bottom, right, top = array_bounds(src_height, src_width, src_transform)

    new_transform, new_width, new_height = calculate_default_transform(
        src_crs, target_crs, src_width, src_height, left=left, bottom=bottom, right=right, top=top)

    new_meta = meta.copy()
    new_meta.update({
        'crs': target_crs,
        'transform': new_transform,
        'width': new_width,
        'height': new_height
    })

    new_raster = rasterio.MemoryFile().open(**new_meta)

    for band in range(1, raster.count + 1):
        reproject(
            source=rasterio.band(raster, band),
            destination=rasterio.band(new_raster, band),
            src_transform=src_transform,
            src_crs=src_crs,
            dst_transform=new_transform,
            dst_crs=target_crs,
            resampling=Resampling.nearest)
    
    return new_raster, new_meta



def reproject_array(np_array, meta, target_crs):

    src_crs = meta['crs']
    src_width = meta['width']
    src_height = meta['height']
    src_transform = meta['transform']

    left, bottom, right, top = array_bounds(src_height, src_width, src_transform)

    new_transform, new_width, new_height = calculate_default_transform(
        src_crs, target_crs, src_width, src_height, left=left, bottom=bottom, right=right, top=top)


    new_meta = meta.copy()
    new_meta.update({
        'crs': target_crs,
        'transform': new_transform,
        'width': new_width,
        'height': new_height
    })

    reprojected_array = np.empty((new_height, new_width), dtype=np_array.dtype)
    
    reproject(
        source=np_array,
        destination=reprojected_array,
        src_transform=src_transform,
        src_crs=src_crs,
        dst_transform=new_transform,
        dst_crs=target_crs,
        resampling=Resampling.nearest)

    return reprojected_array, new_meta


def get_bounding_box(raster):

    width, height = raster.width, raster.height

    transform = raster.transform

    min_x, min_y = transform * (0, height)
    max_x, max_y = transform * (width, 0)

    return min_x, min_y, max_x, max_y

def get_raster_bounding_box(meta):
    transform = meta['transform']
    width = meta['width']
    height = meta['height']

    top_left_x, top_left_y = transform * (0, 0)
    bottom_right_x, bottom_right_y = transform * (width, height)

    return (top_left_x, bottom_right_y, bottom_right_x, top_left_y)


def raster_numpy_to_df(np_array, table_name="raster", no_data=None):
    if np_array.ndim == 2:
        rows, cols = np.indices(np_array.shape)
        data = {
            'row': rows.ravel(),
            'col': cols.ravel(),
            table_name: np_array.ravel()
        }
        df = pd.DataFrame(data)
    else:
        rows, cols = np.indices(np_array[0].shape)
        data = {
            'row': rows.ravel(),
            'col': cols.ravel()
        }
        for band_num, band in enumerate(np_array, start=1):
            data[f'{table_name}_{band_num}'] = band.ravel()
        df = pd.DataFrame(data)

    if no_data is not None:
        df.replace(no_data, np.nan, inplace=True)
    
    return df

def shape_manufacturing(shape_data, x_att = None, y_att = None, meta={}):
    if isinstance(shape_data, str):
        return ShapeData(raster_path=shape_data, meta=meta)
    if isinstance(shape_data, rasterio.DatasetReader):
        return ShapeData(raster=shape_data, meta=meta)
    if isinstance(shape_data, gpd.GeoDataFrame):
        return ShapeData(gdf=shape_data, meta=meta)
    if isinstance(shape_data, np.ndarray):
        return ShapeData(np_array=shape_data, meta=meta)
    if isinstance(shape_data, pd.DataFrame):
        return ShapeData(df=shape_data, x_att=x_att, y_att=y_att, meta=meta)

class ShapeData:

    def __init__(self, raster_path=None, raster=None, gdf=None, np_array=None, df=None, x_att=None, y_att=None, meta={}):
        if raster_path is not None:
            self.load_raster(raster_path)
            self.raster_path = raster_path

        elif raster is not None:
            self.raster = raster
            self.meta = meta

        elif gdf is not None:
            self.gdf = gdf
            self.meta = meta
            if "crs" not in self.meta.keys():
                self.meta["crs"] = self.gdf.crs.to_string()

        elif np_array is not None:
            self.np_array = np_array
            self.meta = meta

        elif df is not None:
            if x_att is None or y_att is None:
                raise ValueError("x_att and y_att must be provided if df is provided")
            
            self.df = lightweight_copy(df)
            self.x_att = x_att
            self.y_att = y_att
            self.meta = meta

    def get_nodata(self):
        if "nodata" in self.meta.keys():
            return self.meta["nodata"]
        else:
            return None

    def to_df(self, xcol = "x", ycol = "y"):
        if hasattr(self, 'np_array'):
            df = create_df_from_np_array(np_array=self.np_array, 
                        column_names=self.get_band_names(),
                        nodata=self.get_nodata(),
                        x_col=xcol, y_col=ycol)
            meta = self.meta.copy()
            meta["aggregated"] = True
            return ShapeData(df=df, x_att=xcol, y_att=ycol, meta=meta)
        else:
            raise ValueError("Not implemented yet")

    def get_band_names(self):
        
        if "band_names" in self.meta.keys():
            return self.meta["band_names"]

        table_name = self.get_name()
        
        if hasattr(self, 'raster'):
            return [f"{table_name}_band_{i+1}" for i in range(self.raster.count)]
        
        if hasattr(self, 'np_array'):
            if self.np_array.ndim == 2:
                return [table_name]
            else:
                return [f"{table_name}_band_{i+1}" for i in range(self.np_array.shape[0])]

    def set_band_names(self, new_names):
        self.meta["band_names"] = new_names

    def get_name(self):

        if "table_name" in self.meta:
            return self.meta["table_name"]

        else:
            return self.get_type()

    def get_summary(self):
        if hasattr(self, 'gdf'):
            return f"""gdf has {len(self.gdf)} rows and {len(self.gdf.columns)} columns.
Below are the first 2 rows of the gdf:
{self.gdf.head(2)}"""

        elif hasattr(self, 'raster'):
            return f"""raster is rasterio.DatasetReader. It has {self.raster.count} bands, {self.raster.width} columns, and {self.raster.height} rows.
Its nodata value is {self.raster.nodata}. Its crs is {self.raster.crs.to_string()}. Its transform is {self.raster.transform}."""

        elif hasattr(self, 'np_array'):
            nodata_summary = ""
            if "nodata" in self.meta.keys():
                nodata_summary = f"Its nodata value is {self.meta['nodata']}."
            
            np_array = self.np_array
            min_value = np.nanmin(np_array)
            max_value = np.nanmax(np_array)

            if "nodata" in self.meta.keys():
                filtered_array = np_array[np_array != self.meta['nodata']]
                min_value = np.nanmin(filtered_array)
                max_value = np.nanmax(filtered_array)

            return f"""np_array has shape {self.np_array.shape}. 
{nodata_summary}. Its normal value range is from {min_value} to {max_value}.
Its crs is {self.meta['crs']}. Its transform is {self.meta['transform']}."""

        elif hasattr(self, 'df'):
            if "crs" in self.meta.keys():
                crs_summary = f"\nIts crs is {self.meta['crs']}."
            if "transform" in self.meta.keys():
                transform_summary = f"\nIts transform is {self.meta['transform']}."

            return f"""df has {len(self.df)} rows and {len(self.df.columns)} columns.{crs_summary}{transform_summary}
The attribute for x is {self.x_att}. The attribute for y is {self.y_att}.
Below are the first 2 rows of the df:
{self.df.head(2)}"""


    def get_type(self):
        if hasattr(self, 'gdf'):
            return "gdf"
        
        elif hasattr(self, 'raster'):
            return "raster"
        
        elif hasattr(self, 'np_array'):
            return "np_array"
        
        elif hasattr(self, 'df'):
            return "df"

    def get_post_fix(self):
        if hasattr(self, 'gdf'):
            return "shp"
        elif hasattr(self, 'raster'):
            return "tif"
        elif hasattr(self, 'np_array'):
            return "tif"
        elif hasattr(self, 'df'):
            return "csv"

    def save_file(self, output_path):
        if hasattr(self, 'gdf'):
            gdf = self.gdf
            meta = self.meta
            self.gdf.to_file(output_path)

        elif hasattr(self, 'raster'):
            raster = self.raster
            meta = self.meta
            with rasterio.open(output_path, 'w', **meta) as dst:
                for i in range(1, raster.count + 1):
                    dst.write(raster.read(i), i)

        elif hasattr(self, 'np_array'):
            np_array = self.np_array
            meta = self.meta
            save_np_array_as_raster(np_array, output_path, meta)


        elif hasattr(self, 'df'):
            df = self.df
            meta = self.meta
            df.to_csv(output_path, index=False)

        


    def get_data(self):
        if hasattr(self, 'gdf'):
            return self.gdf
        elif hasattr(self, 'raster'):
            return self.raster
        elif hasattr(self, 'np_array'):
            return self.np_array
        elif hasattr(self, 'df'):
            return self.df

    
    def apply_window(self, window):

        if hasattr(self, 'gdf'):
            pass
        
        elif hasattr(self, 'raster'):
            np_array = read_raster_to_numpy(self.raster, window=window)
            return ShapeData(np_array=np_array, meta=self.meta)

        elif hasattr(self, 'np_array'):
            pass

        elif  hasattr(self, 'df'):
            pass

    def get_crs(self):
        if hasattr(self, 'gdf'):
            return self.gdf.crs.to_string()
        
        elif hasattr(self, 'raster'):
            return self.raster.crs.to_string()
        
        elif hasattr(self, 'np_array'):
            if "crs" not in self.meta.keys():
                return ""
            else:
                crs = self.meta['crs']
                if isinstance(crs, str):
                    return crs
                else:
                    return crs.to_string()
                
        elif hasattr(self, 'df'):
            if "crs" not in self.meta.keys():
                return ""
            else:
                crs = self.meta['crs']
                if isinstance(crs, str):
                    return crs
                else:
                    return crs.to_string()

    def project_to_target_crs(self, target_crs):

        source_crs = self.get_crs()

        if are_crs_equivalent(source_crs, target_crs):
            return self
        
        if hasattr(self, 'gdf'):
            gdf = standardize_crs_geo_df(target_crs, self.gdf)
            return ShapeData(gdf=gdf, meta=self.meta)
        
        elif hasattr(self, 'raster'):
            new_raster, new_meta = reproject_raster(self.raster, self.meta, target_crs)
            return ShapeData(raster=new_raster, meta=new_meta)

        elif hasattr(self, 'np_array'):
            new_np_array, new_meta = reproject_array(self.np_array, self.meta, target_crs)
            return ShapeData(np_array=new_np_array, meta=new_meta)

        elif hasattr(self, 'df'):
            
            source_transform = self.get_geo_transform()

            new_shape_data = self.row_column_to_x_y()

            source_crs = self.get_crs()

            transformer = Transformer.from_crs(source_crs, target_crs, always_xy=True)
            
            new_shape_data.df[new_shape_data.x_att], new_shape_data.df[new_shape_data.y_att] = transformer.transform(new_shape_data.df[new_shape_data.x_att].values, new_shape_data.df[new_shape_data.y_att].values)

            new_shape_data.meta['crs'] = target_crs

            
            return new_shape_data


    def row_column_to_x_y(self):

        if hasattr(self, 'gdf'):
            raise ValueError("not implemented yet")
        
        elif hasattr(self, 'raster'):
            raise ValueError("this is only for vector data")

        elif hasattr(self, 'np_array'):
            raise ValueError("this is only for vector data")

        elif hasattr(self, 'df'):
            source_transform = self.get_geo_transform()

            if source_transform is None:
                return self
            
            x_att = self.df[self.x_att]
            y_att = self.df[self.y_att]

            source_matrix = np.array([[source_transform.a, source_transform.b, source_transform.c],
                                        [source_transform.d, source_transform.e, source_transform.f],
                                        [0, 0, 1]])
            
            x_att, y_att = apply_affine_transform(source_matrix, x=x_att, y=y_att)

            new_df = lightweight_copy(self.df)

            new_df[self.x_att] = x_att
            new_df[self.y_att] = y_att

            new_meta = self.meta.copy()
            new_meta['transform'] = None

            return ShapeData(df=new_df, x_att=self.x_att, y_att=self.y_att, meta=new_meta)

    def is_aggregated(self):
        if hasattr(self, 'gdf'):
            raise ValueError("not implemented yet")
        
        elif hasattr(self, 'raster'):
            return True

        elif hasattr(self, 'np_array'):
            return True

        elif hasattr(self, 'df'):
            if "aggregated" in self.meta.keys():
                return self.meta["aggregated"]
            else:
                return False

    def set_aggregated(self, aggregated):
        if hasattr(self, 'gdf'):
            raise ValueError("not implemented yet")
        
        elif hasattr(self, 'raster'):
            raise ValueError("this is only for df")
        
        elif hasattr(self, 'np_array'):
            raise ValueError("this is only for df")

        elif hasattr(self, 'df'):
            self.meta["aggregated"] = aggregated

    def apply_aggregation(self, agg_dict=None, rename = None):
        group_by_key = [self.x_att, self.y_att]

        if agg_dict is None:
            agg_dict = {self.x_att: 'count'}
            if rename is None:
                rename = ['count']

        grouped_df = self.df.groupby(group_by_key).agg(agg_dict)

        if rename is not None:
            grouped_df.columns = rename

        grouped_df.reset_index(inplace=True)

        new_shape_data = ShapeData(df=grouped_df, x_att=self.x_att, y_att=self.y_att, meta=self.meta)

        new_shape_data.set_aggregated(True)

        return new_shape_data

    def x_y_to_row_column(self, resolution=None, target_geo_transform=None):

        if resolution is not None and target_geo_transform is not None:
            raise ValueError("resolution and target_geo_transform cannot be both provided")
        
        if resolution is None and target_geo_transform is None:
            raise ValueError("resolution and target_geo_transform cannot be both None")

        if resolution is not None:
            bounding_box = self.get_bounding_box()
            target_geo_transform = create_geotransform(bounding_box, resolution)

        if hasattr(self, 'gdf'):
            raise ValueError("not implemented yet")
        
        elif hasattr(self, 'raster'):
            raise ValueError("this is only for vector data")

        elif hasattr(self, 'np_array'):
            raise ValueError("this is only for vector data")

        elif hasattr(self, 'df'):

            source_transform = self.get_geo_transform()

            if source_transform is not None:
                raise ValueError("Please convert row and column to x and y first")

            affine_matrix = np.array([[target_geo_transform.a, target_geo_transform.b, target_geo_transform.c],
                                        [target_geo_transform.d, target_geo_transform.e, target_geo_transform.f],
                                        [0, 0, 1]])

            x_att = self.df[self.x_att]
            y_att = self.df[self.y_att]

            converted_cols, converted_rows = xy_to_colrow(affine_matrix, df_x, df_y)

            new_df = lightweight_copy(self.df)
            new_df[self.x_att] = converted_cols
            new_df[self.y_att] = converted_rows

            new_meta = self.meta.copy()
            new_meta['transform'] = target_geo_transform

            return ShapeData(df=new_df, x_att=self.x_att, y_att=self.y_att, meta=new_meta)
            
    
    def resample_to_target_transform(self, geo_transform = None, resolution = None):

        source_transform = self.get_geo_transform()

        if geo_transform is None and resolution is None:
            return self

        if source_transform is not None and geo_transform is not None:
            if source_transform == geo_transform:
                return self

        if hasattr(self, 'gdf'):
            np_array, new_meta = rasterize_gdf(self.gdf, transform=geo_transform, resolution=resolution)
            return ShapeData(np_array=np_array, meta=new_meta)
        
        elif hasattr(self, 'raster'):
            np_array,  new_meta = resample_raster_to_transform(self.raster, geo_transform)
            return ShapeData(np_array=np_array, meta=new_meta)

        elif hasattr(self, 'np_array'):
            bounds = self.get_bounding_box()
            new_np_array, new_meta = resample_array(self.np_array, bounds, self.meta, target_transform=geo_transform, resolution=resolution)
            return ShapeData(np_array=new_np_array, meta=new_meta)

        elif hasattr(self, 'df'):
            
            x_att = self.df[self.x_att]
            y_att = self.df[self.y_att]

            if source_transform is not None:
                source_matrix = np.array([[source_transform.a, source_transform.b, source_transform.c],
                                        [source_transform.d, source_transform.e, source_transform.f],
                                        [0, 0, 1]])
                x_att, y_att = apply_affine_transform(source_matrix, x=x_att, y=y_att)

            if geo_transform is not None:
                target_transform = geo_transform
            else:
                bounds = self.get_bounding_box()
                width = int((bounds[2] - bounds[0]) / resolution)
                height = int((bounds[3] - bounds[1]) / resolution)
                target_transform = create_geotransform(bounds, resolution)
   
            target_matrix = np.array([[target_transform.a, target_transform.b, target_transform.c],
                                    [target_transform.d, target_transform.e, target_transform.f],
                                    [0, 0, 1]])

            x_att, y_att = xy_to_colrow(target_matrix, x_att, y_att)

            new_df = lightweight_copy(self.df)

            new_df[self.x_att] = x_att
            new_df[self.y_att] = y_att

            new_meta = self.meta.copy()
            new_meta['transform'] = target_transform


            new_shape_data = ShapeData(df=new_df, x_att=self.x_att, y_att=self.y_att, meta=new_meta)
            if self.is_aggregated():
                print("⚠️ Warning: resampling aggregated data is not implemented yet")
                print("😊 Please send a feature request!")
                new_shape_data.set_aggregated(False)
            return new_shape_data

    def load_raster(self, raster_path):


        raster = rasterio.open(raster_path)

        self.np_array = read_raster_to_numpy(raster)

        self.meta = raster.meta

        self.meta["table_name"] = os.path.basename(raster_path).split('.')[0]

    def display_html(self, value_att=None):
        image_b64 = ""
        if hasattr(self, 'gdf'):
            image_b64 = plot_gdf(self.gdf)

        elif hasattr(self, 'raster'):
            image_b64 = plot_coarse_raster(self.raster)

        elif hasattr(self, 'np_array'):
            band_names = self.get_band_names()
            image_b64 = plot_np_array(self.np_array, self.meta, band_names=band_names)

        elif hasattr(self, 'df'):

            if self.is_aggregated():
                image_b64 = plot_efficient_grid(self.df, self.x_att, self.y_att, value_col=value_att)
            else:
                transform = self.get_geo_transform()

                invert = True
                if transform is None:
                    invert = False

                image_b64 = plot_df_with_geo(self.df, self.x_att, self.y_att, invert=invert, value_att=value_att)
            
        html_img = f'<img src="data:image/png;base64,{image_b64}" width="400"/>'
        return html_img

    def to_html(self, value_att=None, **kwargs):
        html = ""
        

        html += self.display_html(value_att=value_att)
        return html

    def display(self, value_att=None):
        html_img = self.display_html(value_att=value_att)
        if hasattr(self, 'gdf'):
            display(self.gdf.head())
        if hasattr(self, 'df'):
            display(self.df.head())
        
        display(HTML(html_img))

    def get_geo_transform(self):

        if hasattr(self, 'gdf'):
            if 'transform' not in self.meta.keys():
                return None
            return self.meta['transform']
        
        elif hasattr(self, 'raster'):
            return self.raster.transform
        
        elif hasattr(self, 'np_array'):
            if 'transform' not in self.meta.keys():
                return None
            return self.meta['transform']
        
        elif hasattr(self, 'df'):
            if 'transform' not in self.meta.keys():
                return None
            return self.meta['transform']

    def get_bounding_box(self):

        if hasattr(self, 'gdf'):
            return get_geodataframe_bbox(self.gdf)
        
        elif hasattr(self, 'raster'):
            return get_bounding_box(self.raster)
        
        elif hasattr(self, 'np_array'):
            return get_raster_bounding_box(self.meta)
        
        elif hasattr(self, 'df'):
            min_x = self.df[self.x_att].min()
            max_x = self.df[self.x_att].max()
            min_y = self.df[self.y_att].min()
            max_y = self.df[self.y_att].max()

            transform = self.get_geo_transform()

            if transform is not None:
                source_matrix = np.array([[transform.a, transform.b, transform.c],
                                        [transform.d, transform.e, transform.f],
                                        [0, 0, 1]])
                min_x, min_y = apply_affine_transform(source_matrix, x=min_x, y=min_y)
                max_x, max_y = apply_affine_transform(source_matrix, x=max_x, y=max_y)

                min_x, max_x = min(min_x, max_x), max(min_x, max_x)
                min_y, max_y = min(min_y, max_y), max(min_y, max_y)

            return (min_x, min_y, max_x, max_y)

    def get_height_width(self):

        if hasattr(self, 'gdf'):
            return self.gdf.height, self.gdf.width
        
        elif hasattr(self, 'raster'):
            return self.raster.height, self.raster.width
        
        elif hasattr(self, 'np_array'):
            return self.meta['height'], self.meta['width']
        
        elif hasattr(self, 'df'):
            pass

    def crop_by_bounding_box(self, bbox):
            
        if hasattr(self, 'gdf'):
            raise ValueError("Not implemented yet")
        
        elif hasattr(self, 'raster'):
            window = standardize_crs_bbox(self.raster.transform, bbox)
            np_array, new_meta = crop_raster_by_window(self.raster, self.meta, window)
            return ShapeData(np_array=np_array, meta=new_meta)
        
        elif hasattr(self, 'np_array'):
            window = standardize_crs_bbox(self.meta['transform'], bbox)
            np_array, new_meta = crop_np_array_by_window(self.np_array, self.meta, window)
            return ShapeData(np_array=np_array, meta=new_meta)
        
        elif hasattr(self, 'df'):
            new_shape = self.row_column_to_x_y()

            new_df, new_meta = crop_df_based_on_bbox(new_shape.df, new_shape.meta, new_shape.x_att, new_shape.y_att, bbox)

            return ShapeData(df=new_df, x_att=new_shape.x_att, y_att=new_shape.y_att, meta=new_meta)

    
    def crop(self, height, width):

        if hasattr(self, 'gdf'):
            raise ValueError("Not implemented yet")
        
        elif hasattr(self, 'raster'):
            np_array, new_meta = crop_raster(self.raster, self.meta, height, width)
            return ShapeData(np_array=np_array, meta=new_meta)
        
        elif hasattr(self, 'np_array'):
            new_np_array = crop_np_array(self.np_array, height, width)
            new_meta = self.meta.copy()
            new_meta['height'] = height
            new_meta['width'] = width

            return ShapeData(np_array=new_np_array, meta=new_meta)

        elif hasattr(self, 'df'):
            pass

    def get_np_array(self):
        if hasattr(self, 'gdf'):
            raise ValueError("Can't get np_array from gdf")
        
        elif hasattr(self, 'raster'):
            return read_raster_to_numpy(self.raster)
        
        elif hasattr(self, 'np_array'):
            return self.np_array

        elif hasattr(self, 'df'):
            raise ValueError("Can't get np_array from df")

    def to_np_array(self):
        if hasattr(self, 'gdf'):
            raise ValueError("Can't convert gdf to np_array")
        elif hasattr(self, 'raster'):
            np_array = read_raster_to_numpy(self.raster)
            return ShapeData(np_array=np_array, meta=self.meta)
        elif hasattr(self, 'np_array'):
            return self
        elif hasattr(self, 'df'):
            raise ValueError("Can't convert df to np_array")

    def __repr__(self):
        if hasattr(self, 'gdf'):
            print("This is a GeoDataFrame")
        
        elif hasattr(self, 'raster'):
            print("This is a raster of shape", self.raster.shape)
        
        elif hasattr(self, 'np_array'):
            print("This is a numpy array of shape", self.np_array.shape)

        elif hasattr(self, 'df'):
            print("This is a DataFrame")

        if hasattr(self, 'meta'):
            print("Meta: ", self.meta)
        
        self.display()

        return ""

    def transform(self):
        if not hasattr(self, 'cleaner'):
            self.cleaner = GeoDataCleaning(self)
        self.cleaner.display()

    def get_cleaner(self):
        if not hasattr(self, 'cleaner'):
            self.cleaner = GeoDataCleaning(self)
        return self.cleaner

    def get_shape(self):
        if hasattr(self, 'cleaner'):
            return self.cleaner.run_codes()
        
        return self
    

def crop_df_based_on_bbox(df, meta, x_att, y_att, bbox):

    min_x, min_y, max_x, max_y = bbox

    cropped_df = df[(df[x_att] >= min_x) & (df[x_att] <= max_x) &
                    (df[y_att] >= min_y) & (df[y_att] <= max_y)]

    new_meta = meta
    return cropped_df, new_meta

def plot_df_with_geo(df, x_att, y_att, value_att=None, invert=False):

    if len(df) > 10000:
        df = df.sample(10000)

    plt.figure(figsize=(5, 3))

    if value_att is not None:
        plt.scatter(df[x_att], df[y_att], c=df[value_att], cmap='viridis', alpha=0.2)
    else:
        plt.scatter(df[x_att], df[y_att], alpha=0.2)

    if invert:
        plt.gca().invert_yaxis()
        
    plt.grid(True)
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')

    return image_base64

def crop_raster(raster, meta, height, width):
    window = Window(0, 0, width, height)

    cropped_data = raster.read(window=window)

    new_transform = raster.window_transform(window)

    new_meta = meta.copy()
    new_meta.update({
        "height": height,
        "width": width,
        "transform": new_transform
    })

    return cropped_data, new_meta



def crop_raster_by_window(raster, meta, window):

    cropped_data = raster.read(window=window)

    new_transform = raster.window_transform(window)

    new_meta = meta.copy()
    new_meta.update({
        "height": window.height,
        "width": window.width,
        "transform": new_transform
    })

    return cropped_data, new_meta


def update_transform(original_transform, col_start, row_start):
    new_origin_x = original_transform.c + col_start * original_transform.a
    new_origin_y = original_transform.f + row_start * original_transform.e

    new_transform = Affine(original_transform.a, 0.0, new_origin_x,
                           0.0, original_transform.e, new_origin_y)
    return new_transform

def crop_np_array_by_window(np_array, meta, window):
    row_start = max(0, min(int(window.row_off), np_array.shape[-2] - 1))
    row_stop = max(0, min(int(window.row_off + window.height), np_array.shape[-2]))
    col_start = max(0, min(int(window.col_off), np_array.shape[-1] - 1))
    col_stop = max(0, min(int(window.col_off + window.width), np_array.shape[-1]))

    if np_array.ndim == 3:
        cropped_array = np_array[:, row_start:row_stop, col_start:col_stop]
    elif np_array.ndim == 2:
        cropped_array = np_array[row_start:row_stop, col_start:col_stop]
    else:
        raise ValueError("Input array must be either 2D or 3D.")

    new_meta = meta.copy()
    new_meta.update({
        "height": row_stop - row_start,
        "width": col_stop - col_start,
        "transform": update_transform(meta['transform'], col_start, row_start)
    })

    return cropped_array, new_meta



def crop_np_array(np_array, height, width):
    if np_array.ndim == 2:
        return np_array[:height, :width]
    else:
        return np_array[:, :height, :width]


def get_geodataframe_bbox(geodf):
    bounds = geodf.total_bounds

    return bounds


def extract_values_by_indices(main_array, row_indices, col_indices):
    valid_rows = (row_indices >= 0) & (row_indices < main_array.shape[1])
    valid_cols = (col_indices >= 0) & (col_indices < main_array.shape[2])
    valid_indices = valid_rows & valid_cols

    if main_array.ndim == 2:
        result = np.full((1, len(row_indices)), np.nan)
        result[0, valid_indices] = main_array[row_indices[valid_indices], col_indices[valid_indices]]
        return result
    
    elif main_array.ndim == 3:
        result = np.full((main_array.shape[0], len(row_indices)), np.nan)

        for band in range(main_array.shape[0]):
            result[band, valid_indices] = main_array[band, row_indices[valid_indices], col_indices[valid_indices]]

        return result
    else: 
        raise ValueError("The input array must be 2D or 3D")






    


def to_target_shape_data(source_shape_data, target_shape_data, semi_join=True):
    shape_data_crs = target_shape_data.get_crs()
    shape_data_transform = target_shape_data.get_geo_transform()

    bounding_box = target_shape_data.get_bounding_box()

    return to_target_crs_and_transform(source_shape_data, 
                                        target_crs=shape_data_crs, 
                                        target_transform=shape_data_transform, 
                                        semi_join=semi_join,
                                        target_bounding_box=bounding_box)

def to_target_crs_and_transform(shape_data, target_crs, target_transform=None, resolution=None, semi_join=False, target_bounding_box=None):
    

    new_shape_data = shape_data

    if semi_join and target_bounding_box is None:
        raise ValueError("target_bounding_box cannot be None for semi_join")

    if hasattr(new_shape_data, 'gdf'):

        new_shape_data = new_shape_data.project_to_target_crs(target_crs)

        if semi_join:
            new_shape_data = new_shape_data.crop_by_bounding_box(target_bounding_box)

        new_shape_data = new_shape_data.resample_to_target_transform(resolution=resolution, geo_transform=target_transform)
    
    elif hasattr(new_shape_data, 'raster'):

        new_shape_data = new_shape_data.project_to_target_crs(target_crs)

        if semi_join:
            new_shape_data = new_shape_data.crop_by_bounding_box(target_bounding_box)

        new_shape_data = new_shape_data.resample_to_target_transform(resolution=resolution, geo_transform=target_transform)

    elif hasattr(new_shape_data, 'np_array'):
        new_shape_data = new_shape_data.project_to_target_crs(target_crs)

        if semi_join:
            new_shape_data = new_shape_data.crop_by_bounding_box(target_bounding_box)


        new_shape_data = new_shape_data.resample_to_target_transform(resolution=resolution, geo_transform=target_transform)

    elif hasattr(new_shape_data, 'df'):
        
        new_shape_data = new_shape_data.row_column_to_x_y()

        new_shape_data = new_shape_data.project_to_target_crs(target_crs)

        if semi_join:
            new_shape_data = new_shape_data.crop_by_bounding_box(target_bounding_box)

        new_shape_data = new_shape_data.resample_to_target_transform(resolution=resolution, geo_transform=target_transform)

    return new_shape_data

def plot_bounding_boxes(bounding_boxes):
    fig, ax = plt.subplots(1, figsize=(4, 2))

    if not isinstance(bounding_boxes, list):
        bounding_boxes = [bounding_boxes]

    colors = generate_seaborn_palette(len(bounding_boxes))
    line_styles = ['-', '--', '-.', ':']

    min_x, min_y, max_x, max_y = float('inf'), float('inf'), float('-inf'), float('-inf')

    for i, box in enumerate(bounding_boxes):
        left, bottom, right, top = box

        min_x = min(min_x, left)
        min_y = min(min_y, bottom)
        max_x = max(max_x, right)
        max_y = max(max_y, top)

        rect = patches.Rectangle((left, bottom), right - left, top - bottom, linewidth=2,
                                 edgecolor=colors[i % len(colors)], linestyle=line_styles[i % len(line_styles)],
                                 facecolor='none', alpha=0.7, label=f'Box {i+1}')

        ax.add_patch(rect)

    range_x = max_x - min_x
    range_y = max_y - min_y

    padding_x = range_x * 0.1
    padding_y = range_y * 0.1

    ax.set_xlim(min_x - padding_x, max_x + padding_x)
    ax.set_ylim(min_y - padding_y, max_y + padding_y)

    plt.grid(True, which='both', linestyle='--', linewidth=0.5)

    plt.xlabel('X', fontsize=8)
    plt.ylabel('Y', fontsize=8)
    plt.title('Bounding Boxes', fontsize=10)
    plt.legend(fontsize=8)

    ax.tick_params(axis='both', which='major', labelsize=8)

    plt.show()



def validate_bounding_box(bbox, is_longlat=False):

    min_x, min_y, max_x, max_y = bbox

    if is_longlat:
        lon_bounds = (-180, 180)
        lat_bounds = (-90, 90)

        if not (lon_bounds[0] <= min_x <= lon_bounds[1]):
            raise ValueError(f"Minimum longitude {min_x} is out of bounds {lon_bounds}.")
        if not (lon_bounds[0] <= max_x <= lon_bounds[1]):
            raise ValueError(f"Maximum longitude {max_x} is out of bounds {lon_bounds}.")
        if not (lat_bounds[0] <= min_y <= lat_bounds[1]):
            raise ValueError(f"Minimum latitude {min_y} is out of bounds {lat_bounds}.")
        if not (lat_bounds[0] <= max_y <= lat_bounds[1]):
            raise ValueError(f"Maximum latitude {max_y} is out of bounds {lat_bounds}.")
    else:
        if min_x >= max_x:
            raise ValueError(f"Minimum x-coordinate {min_x} is not less than maximum x-coordinate {max_x}.")
        if min_y >= max_y:
            raise ValueError(f"Minimum y-coordinate {min_y} is not less than maximum y-coordinate {max_y}.")



def is_valid_geo_transform(original_transform, new_transform, data_bounds=None):

    if not all(isinstance(t, Affine) for t in [original_transform, new_transform]):
        return False

    if (original_transform.a * new_transform.a < 0) or (original_transform.e * new_transform.e < 0):
        return False

    if data_bounds:
        min_x, min_y, max_x, max_y = data_bounds
        if not (min_x <= new_transform.c <= max_x) or not (min_y <= new_transform.f <= max_y):
            return False

    if original_transform.b != new_transform.b or original_transform.d != new_transform.d:
        return False

    return True






class EmbeddingSearcher:
    def __init__(self, df, embedding_col='embedding'):
        self.df = df
        self.embedding_col = embedding_col
        self.index = None
        self._create_index()

    def _create_index(self):
        embeddings = np.vstack(self.df[self.embedding_col].values)
        
        d = embeddings.shape[1]

        base_index = faiss.IndexFlatL2(d)

        self.index = faiss.IndexIDMap(base_index)

        ids = np.array(range(len(self.df))).astype(np.int64)
        self.index.add_with_ids(embeddings, ids)

    def remove_rows(self, row_indices):
        if np.isscalar(row_indices):
            row_indices = [row_indices]

        ids_to_remove = np.array(row_indices, dtype=np.int64)

        self.index.remove_ids(ids_to_remove)

    def get_closest_indices(self, query_embedding, k=5):
        if not isinstance(query_embedding, np.ndarray):
            query_embedding = np.array(query_embedding)

        if query_embedding.ndim == 1:
            query_embedding = np.expand_dims(query_embedding, axis=0)

        if query_embedding.shape[1] != self.index.d:
            raise ValueError("Dimension of query_embedding does not match the index dimension.")

        distances, indices = self.index.search(query_embedding, k)

        valid_indices = indices[0][indices[0] != -1]

        return valid_indices

    def get_closest_rows(self, query_embedding, k=5):
        closest_indices = self.get_closest_indices(query_embedding, k)

        return self.df.iloc[closest_indices]

    
    def search_by_row_index(self, row_idx, k=10):
        if row_idx < 0 or row_idx >= len(self.df):
            raise IndexError("Row index is out of bounds.")

        query_embedding = self.df.iloc[row_idx][self.embedding_col]
        
        search_results = self.get_closest_rows(query_embedding, k + 1)
        
        if row_idx in search_results.index:
            filtered_results = search_results.drop(index=row_idx)
        else:
            filtered_results = search_results

        return filtered_results.head(k)

    def is_index_empty(self):

        return self.index.ntotal == 0

    def get_valid_id(self):
        if self.is_index_empty():
            raise ValueError("The index is empty. No valid ID can be returned.")

        ids = faiss.vector_to_array(self.index.id_map)

        return ids[0]

    def get_size(self):
        return self.index.ntotal



def entity_relation_match_one(input_desc, refernece_desc):
    template = f"""Your goal is to build relations between input and reference entities.
The input entity has the following attributes:
{input_desc}
Below are reference entities:
{refernece_desc}
Do the following:
1. Read input entity attributes and guess what it is about.

2. Go through each output entity. Describe what it is and reason its relation.
For instance, given the input entity "small car":
if the same entity then EXACT_MATCH. 
E.g., "small automobile"
else if has assumptions that is clearly wrong then CONFLICTED_ASSUMPTION
E.g., "big car" is wrong because input entity clearly specify size as "small"
else if additional assumptions that can't be verified then ADDITIONAL_ASSUMPTION
E.g., "electronic car" is additional battery assumption can't be verified
else if it is general super class then GENERAL 
E.g., "small vehicle" and "car" are general category of "small car"
else it is irrelavent entity then NOT_RELATED
E.g., "cloth" is a irrelavent

Provide your answer as json:
```json
{{
    "Input Entity Guess": "...",
    "EXACT_MATCH": {{
        "reason": "The input entity is ... which matches ...",
        "entity": [...] (a list of index numbers. Note that each entity appears only once in one of the categories)
    }},
    "CONFLICTED_ASSUMPTION": {{
        "reason": "The (what specific) details are conflicted",
        "entity": [...]
    }},
    "ADDITIONAL_ASSUMPTION": {{
        "reason": "The (what specific) details are not mentioned",
        "entity": [...]
    }},
    "GENERAL": {{
        "reason": "...",
        "entity": [...],
    }},
    "NOT_RELATED": {{
        "reason": "...",
        "entity": [...],
    }},
    "Summary of Relations": "The input entity is ... (desrcibe its properties) It doesn't make assumptions about ... It is different from ..."
}}
```"""

    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("-----------------------------------")

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_var = json.loads(json_code)
    
    return json_var

def find_similar_relation_match(input_desc, refernece_desc, related_rows_desc_str):

    template = f"""The input entity has the following attributes:
{input_desc}

Below are the previous matching results:
{refernece_desc}

Below are similar entities:
{related_rows_desc_str}

Now, go through each similar entity. Answer:
1. How they differ to the input entity?
2. Are they totally different entities, or entity with minor detail difference?

Conclude with the list of indices of entities with minor differences:
```json
{{
    "indices": [1, 2 ...]
}}
``` """
    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("-----------------------------------")


    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_var = json.loads(json_code)
    
    return json_var

def find_relation_satisfy_description(entity_desc, related_rows_desc_str):

    template = f"""Find entities that satisfy the description:
Below are entities:
{related_rows_desc_str}
Below are the description:
{entity_desc}

Now, find entities that satisfy the description. Provide your answer as json:
```json
{{
    "reasoning": "The entities are about ...",
    "indices": [...] (list of index numbers, could be empty)
}}
``` """
    messages = [{"role": "user", "content": template}]

    response = call_llm_chat(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("-----------------------------------")


    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_var = json.loads(json_code)
    
    return json_var
    
def parse_match_result(json_var, reference_entities):

    def replace_indices_with_entities(json_var, reference_entities):
        for category in json_var:
            if isinstance(json_var[category], dict) and "entity" in json_var[category]:
                json_var[category]["entity"] = [reference_entities[int(idx) - 1] for idx in json_var[category]["entity"]]
        return json_var

    updated_json_var = replace_indices_with_entities(json_var, reference_entities)

    
    def parse_json_to_string(json_var):
        parsed_string = ""
        if json_var['EXACT_MATCH']['entity']:
            parsed_string += f"The input is exactly matched to: {', '.join(json_var['EXACT_MATCH']['entity'])}.\n" if json_var['EXACT_MATCH']['entity'] else ""
            parsed_string += f"The reason is: {json_var['EXACT_MATCH']['reason']}.\n" if not json_var['EXACT_MATCH']['entity'] else ""

        if json_var['CONFLICTED_ASSUMPTION']['entity']:
            parsed_string += f"The input has conflicted assumption to: {', '.join(json_var['CONFLICTED_ASSUMPTION']['entity'])}.\n"
            parsed_string += f"The reason is: {json_var['CONFLICTED_ASSUMPTION']['reason']}.\n"

        if json_var['ADDITIONAL_ASSUMPTION']['entity']:
            parsed_string += f"The input has additional assumption to: {', '.join(json_var['ADDITIONAL_ASSUMPTION']['entity'])}.\n"
            parsed_string += f"The reason is: {json_var['ADDITIONAL_ASSUMPTION']['reason']}.\n"

        if json_var['GENERAL']['entity']:
            parsed_string += f"The input is generally related to: {', '.join(json_var['GENERAL']['entity'])}.\n"
            parsed_string += f"The reason is: {json_var['GENERAL']['reason']}.\n"

        return parsed_string

    parsed_json_string = parse_json_to_string(updated_json_var)

    return parsed_json_string





def create_geo_transform_widget(callback):
    def on_radio_button_change(change):
        if change['new'] == 'Geo Transform':
            geo_transform_widgets.layout.display = 'block'
            resolution_widget.layout.display = 'none'
        else:
            geo_transform_widgets.layout.display = 'none'
            resolution_widget.layout.display = 'block'

    def on_button_clicked(b):
        if radio_buttons.value == 'Geo Transform':
            geo_transform = Affine(pixel_size_x.value, rotation_x.value, origin_x.value,
                                   rotation_y.value, -pixel_size_y.value, origin_y.value)
            callback(geo_transform, None)
        else:
            callback(None, resolution.value)

    radio_buttons = widgets.RadioButtons(
        options=['Geo Transform', 'Resolution'],
        description='Select:',
        disabled=False
    )
    radio_buttons.observe(on_radio_button_change, 'value')

    origin_x = widgets.FloatText(value=0.0, description='Origin X:')
    origin_y = widgets.FloatText(value=0.0, description='Origin Y:')
    pixel_size_x = widgets.FloatText(value=1.0, description='Pixel Size X:')
    pixel_size_y = widgets.FloatText(value=1.0, description='Pixel Size Y:')
    rotation_x = widgets.FloatText(value=0.0, description='Rotation X:')
    rotation_y = widgets.FloatText(value=0.0, description='Rotation Y:')
    geo_transform_widgets = widgets.VBox([origin_x, origin_y, pixel_size_x, pixel_size_y, rotation_x, rotation_y])
    geo_transform_widgets.layout.display = 'block'

    resolution = widgets.FloatText(value=300.0, description='Resolution:')
    resolution_widget = widgets.VBox([resolution])
    resolution_widget.layout.display = 'none'

    button = widgets.Button(description="Submit")
    button.on_click(on_button_clicked)

    display(radio_buttons, geo_transform_widgets, resolution_widget, button)




def truncate_html_td(html, max_length=30):
    pattern = r'(<td[^>]*>)(.*?)(</td>)'
    
    def truncate_match(match):
        content = match.group(2).strip()
        if len(content) > max_length:
            content = content[:max_length] + '...'
        return match.group(1) + content + match.group(3)

    return re.sub(pattern, truncate_match, html, flags=re.DOTALL)



def save_np_array_as_raster(np_array, output_path, meta):
    if np_array.ndim == 2:
        height, width = np_array.shape
        count = 1
    elif np_array.ndim == 3:
        count, height, width = np_array.shape
    else:
        raise ValueError("NumPy array must be either 2D or 3D")

    meta.update({
        'height': height,
        'width': width,
        'count': count,
        'dtype': np_array.dtype
    })

    with rasterio.open(output_path, 'w', **meta) as dst:
        if np_array.ndim == 2:
            dst.write(np_array, 1)
        else:
            for i in range(count):
                dst.write(np_array[i, :, :], i + 1)


class GeoDataCleaning(DataCleaning):
    def __init__(self, shape_data):
        self.shape_data = shape_data
        source = SourceShapeStep(shape_data)
        self.pipeline = TransformationPipeline(steps = [source], edges=[])
        self.final_shape = self.pipeline.run_codes()
    
    def display(self):
        self.pipeline.display(call_back=self.display)
        display(HTML(f"<hr> <h3>🤓 Result Data</h3>"))
        self.final_shape.__repr__()
        display(HTML(f"<hr> <h3>🧹 Data Transformation</h3>"))
        data_type = self.final_shape.get_type()

        boxes = []
        
        def on_reproject_clicked(b):
            clear_output(wait=True)
            self.add_reproject_step()
        
        reproject_button = widgets.Button(description="Reproject")
        reproject_button.on_click(on_reproject_clicked)
        box = widgets.VBox([reproject_button])
        boxes.append(box)

        
        def on_resample_clicked(b):
            clear_output(wait=True)
            self.add_resample_step()

        resample_button = widgets.Button(description="Rasterize")
            
        if data_type == "raster" or data_type == "np_array":
            resample_button = widgets.Button(description="Resample")

        resample_button.on_click(on_resample_clicked)
        box = widgets.VBox([resample_button])
        boxes.append(box)

        def on_save_clicked(b):
            clear_output(wait=True)
            self.save()
        
        save_button = widgets.Button(description="💾 Save Result")
        save_button.on_click(on_save_clicked)
        box = widgets.VBox([save_button])
        boxes.append(box)

        def on_remove_last_clicked(b):
            clear_output(wait=True)
            self.add_remove_last_step()

        remove_last_button = widgets.Button(description="⚠️ Remove Last")
        remove_last_button.on_click(on_remove_last_clicked)
        box = widgets.VBox([remove_last_button])
        boxes.append(box)

        if data_type == "np_array":
            def on_to_df_clicked(b):
                clear_output(wait=True)
                self.add_to_df_step()
            
            to_df_button = widgets.Button(description="To DataFrame")

            to_df_button.on_click(on_to_df_clicked)
            box = widgets.VBox([to_df_button])
            boxes.append(box)

        label = widgets.HTML(value=f"🎲 Want to perform an ad hoc transformation?\n⚠️ Currently, ad hoc transformation can't change CRS or file type.")

        def on_ad_hoc_clicked(b):
            clear_output(wait=True)
            self.create_ad_hoc_step()

        adhoc_button = widgets.Button(description="Ad hoc")
        adhoc_button.on_click(on_ad_hoc_clicked)
        box = widgets.VBox([label, adhoc_button])
        boxes.append(box)

        display(widgets.VBox(boxes))


    def save(self):
        post_fix = self.final_shape.get_post_fix()

        file_name = f"cocoon_geo_result.{post_fix}"

        print(f"🤓 Do you want to save the result?")
        print(f"Note that only the given file type can be saved.")
        print("😊 More save options are under development. Please send a feature request!")

        def save_file(b):
            updated_file_name = file_name_input.value

            if file_exists(updated_file_name):
                print("Failed to save: File already exists.")
            else:
                self.final_shape.save_file(updated_file_name)
                print(f"🎉 File saved successfully as {updated_file_name}")

        file_name_input = Text(value=file_name, description='File Name:')
        
        save_button = Button(description="Save File")
        save_button.on_click(save_file)

        display(file_name_input, save_button)
        

    def add_reproject_step(self):

        print("🧐 Please specify the target CRS:")

        def call_back(crs_string):
            step = ReprojectStep(crs_string, sample_df = self.final_shape)

            try:
                step.run_codes(self.final_shape)
            except Exception as e:
                print("☹️ There is an error in the step you added:")
                print(e)
                return

            self.pipeline.add_step_to_final(step)
            self.final_shape = self.pipeline.run_codes()
            clear_output(wait=True)
            self.display()
        
        CRS_select(call_back)

    def add_remove_last_step(self):
        self.pipeline.remove_final_node()
        self.final_shape = self.pipeline.run_codes()
        clear_output(wait=True)
        self.display()


    def add_to_df_step(self):
        step = NumpyToDfStep(sample_df = self.final_shape)

        try:
            step.run_codes(self.final_shape)
        except Exception as e:
            print("☹️ There is an error in the step you added:")
            print(e)
            return
        
        self.pipeline.add_step_to_final(step)
        self.final_shape = self.pipeline.run_codes()
        clear_output(wait=True)
        self.display()

    def add_resample_step(self):

        print("🧐 Bounding box for the data")

        bounding_box = self.final_shape.get_bounding_box()
        plot_bounding_boxes(bounding_box)

        print("🧐 Please specify the target transform/resolution:")
        
        def call_back(geo_transform, resolution):
            if geo_transform:
                print("Geo Transform:", geo_transform)
            if resolution:
                print("Resolution:", resolution)
            
            step = ResampleStep(geo_transform=geo_transform, resolution=resolution, sample_df = self.final_shape)

            step.run_codes(self.final_shape)

            self.pipeline.add_step_to_final(step)
            self.final_shape = self.pipeline.run_codes()
            clear_output(wait=True)
            self.display()
        
        create_geo_transform_widget(call_back)

    def create_ad_hoc_step(self):
        sample_output = self.final_shape

        add_hoc_step = GeoShapeCustomTransformStep(name="Ad hoc", sample_df=sample_output)

        def callback(add_hoc_step):
            if add_hoc_step.explanation != "" or add_hoc_step.codes != "":
                add_hoc_step.rename_based_on_explanation()
                self.pipeline.add_step_to_final(add_hoc_step)
                self.final_shape = self.pipeline.run_codes()

            clear_output(wait=True)
            self.display()

        callbackfunc = callback

        add_hoc_step.edit_widget(callbackfunc=callbackfunc)
    

            





            

    



            
        







def create_np_array_from_df(df, x_col, y_col, height, width, nodata=0):

    included_columns = [col for col in df.columns if col not in [x_col, y_col]]

    output_array = np.full((len(included_columns), height, width), nodata, dtype=float)

    for i, col in enumerate(included_columns):
        valid_rows = df[(df[x_col] < width) & (df[y_col] < height)]
        x_values = valid_rows[x_col].astype(int).values
        y_values = valid_rows[y_col].astype(int).values
        values = valid_rows[col].values

        output_array[i, y_values, x_values] = values

    return output_array, included_columns




def create_df_from_np_array(np_array, column_names, nodata=0, x_col="x", y_col="y"):

    df = pd.DataFrame()

    for i, name in enumerate(column_names):
        if np_array.ndim == 3:
            slice_2d = np_array[i, :, :]
        elif np_array.ndim == 2:
            slice_2d = np_array[:, :]
        
        y_indices, x_indices = np.where(slice_2d != nodata)
        values = slice_2d[y_indices, x_indices]

        temp_df = pd.DataFrame({x_col: x_indices, y_col: y_indices, name: values})
        
        if df.empty:
            df = temp_df
        else:
            df = pd.merge(df, temp_df, on=[x_col, y_col], how='outer')

    return df



def parse_raster_to_dataframe(raster, window=None, table_name=""):
        
    all_bands = read_raster_to_numpy(raster, window=window)

    num_bands, num_rows, num_cols = all_bands.shape

    x_coords, y_coords = np.meshgrid(np.arange(num_cols), np.arange(num_rows))

    bands_reshaped = all_bands.reshape(num_bands, -1).T
    x_reshaped = x_coords.ravel()
    y_reshaped = y_coords.ravel()

    data = np.column_stack([x_reshaped, y_reshaped, bands_reshaped])
    data = np.column_stack([bands_reshaped])
    if table_name != "":
        table_name += "_"
    columns = ['x', 'y'] + [f'{table_name}band_{i+1}' for i in range(num_bands)]
    df = pd.DataFrame(data, columns=columns)

    return df

def crop_np_array(np_array_to_crop, target_shape, nodata=0):

    original_shape = np_array_to_crop.shape[1:]

    crop_height = min(original_shape[0], target_shape[0])
    crop_width = min(original_shape[1], target_shape[1])

    cropped_array = np_array_to_crop[:, :crop_height, :crop_width]

    if cropped_array.shape[1] < target_shape[0] or cropped_array.shape[2] < target_shape[1]:
        padding = (
            (0, 0),
            (0, target_shape[0] - cropped_array.shape[1]),
            (0, target_shape[1] - cropped_array.shape[2])
        )
        cropped_array = np.pad(cropped_array, padding, mode='constant', constant_values=nodata)

    return cropped_array

def integrate_np_array(np_array_1, np_array_2, nodata=0):

    if len(np_array_1.shape) == 2:
        np_array_1 = np.expand_dims(np_array_1, axis=0)
    if len(np_array_2.shape) == 2:
        np_array_2 = np.expand_dims(np_array_2, axis=0)

    if np_array_1.shape[1:] != np_array_2.shape[1:]:
        np_array_2 = crop_np_array(np_array_2, np_array_1.shape[1:], nodata=nodata)

    integrated_np_array = np.concatenate((np_array_1, np_array_2), axis=0)
    return integrated_np_array

def refill_nodata(np_array, old_no_data, new_no_data):
    output_array = np.copy(np_array)

    output_array[output_array == old_no_data] = new_no_data

    return output_array

def integrate_geo(main_geo_data, geo_data_arr):
    main_data_type = main_geo_data.get_type()
   
    if main_data_type == "df":


        main_df = main_geo_data.df
        main_x_att = main_geo_data.x_att
        main_y_att = main_geo_data.y_att

        for geo_data in geo_data_arr:

            data_type = geo_data.get_type()

            if data_type == "df":

                df = geo_data.df
                df_x_att = geo_data.x_att
                df_y_att = geo_data.y_att

                main_df = main_df.merge(df, left_on=[main_x_att, main_y_att], right_on=[df_x_att, df_y_att], how="left")
                
                if df_x_att != main_x_att:
                    main_df.drop(columns=[df_x_att], inplace=True)
                if df_y_att != main_y_att:
                    main_df.drop(columns=[df_y_att], inplace=True)

            elif data_type == "np_array":

                band_names = geo_data.get_band_names()

                main_geo_data_transformed = to_target_shape_data(main_geo_data, geo_data, semi_join=False)

                extracted_np_array = join_raster_to_df_points(raster_shape = geo_data, df_shape=main_geo_data_transformed)
                extracted_np_array = extracted_np_array.T
                print("Shape of extracted_np_array:", extracted_np_array.shape)
                print("Length of band_names:", len(band_names))
                print(band_names)
                print("Shape of main_df:", main_df.shape)
                
                main_df = pd.concat([main_df, pd.DataFrame(extracted_np_array, columns=band_names)], axis=1)

        
        new_shape_data = ShapeData(df=main_df, x_att=main_x_att, y_att=main_y_att, meta=main_geo_data.meta)
        return new_shape_data


    elif main_data_type == "np_array":
        main_no_data = main_geo_data.get_nodata()
        main_band_names = main_geo_data.get_band_names()
        main_np_array = main_geo_data.np_array

        if len(main_np_array.shape) == 3:
            height = main_np_array.shape[1]
            width = main_np_array.shape[2]
        elif len(main_np_array.shape) == 2:
            height = main_np_array.shape[0]
            width = main_np_array.shape[1]

        for geo_data in geo_data_arr:
            
            data_type = geo_data.get_type()

            if data_type == "df":

                df = geo_data.df
                x_col =  geo_data.x_att
                y_col = geo_data.y_att

                output_array, included_columns = create_np_array_from_df(df, x_col, y_col, height, width, nodata=main_no_data)

                main_np_array = integrate_np_array(main_np_array, output_array, nodata=main_no_data)

                main_band_names = main_band_names + included_columns

            elif data_type == "np_array":

                no_data = geo_data.get_nodata()
                np_array = geo_data.np_array
                band_names = geo_data.get_band_names()
                
                if main_no_data is not None and no_data is not None and main_no_data != no_data:
                    np_array = refill_nodata(np_array, no_data, main_no_data)

                main_np_array = integrate_np_array(main_np_array, np_array, nodata=main_no_data)
                
                main_band_names = main_band_names + band_names
        
        new_meta = main_geo_data.meta.copy()
        new_shape_data = ShapeData(np_array=main_np_array, meta=new_meta)
        new_shape_data.set_band_names(main_band_names)
        return new_shape_data

        

        
class GeoIntegration:
    
    def __init__(self, doc_dfs):
        if not isinstance(doc_dfs, list):
            raise ValueError("doc_dfs should be a list")

        self.doc_dfs = doc_dfs
        
        self.shape_data_arr = [None] * len(doc_dfs)

        self.document = {}

    def start(self):
        self.process_data()

    def get_result(self):
        return self.result

    def process_data(self):
        next_step = self.ask_for_attributes

        if "candidate_longitude_latitude" not in self.document:
            self.document["candidate_longitude_latitude"] = {}            
        
        create_progress_bar_with_numbers(0, geo_integration_steps)

        print("💡 Processing the data...")
        
        for i in range(len(self.doc_dfs)):
            id_str = f"{i}"
            
            if id_str in self.document["candidate_longitude_latitude"]:
                continue
            
            doc_df = self.doc_dfs[i]
            if isinstance(doc_df, pd.DataFrame):
                raise ValueError("""☹️ Cannot process DataFrame. Please read_data(df) first.
😊 Support for dataFrame is under development. Please send a feature request!""")
                
            elif isinstance(doc_df, DocumentedData):
                source_table_description = doc_df.get_summary()
                json_code =  identify_longitude_latitude(source_table_description)
                self.document["candidate_longitude_latitude"][id_str] = json_code
            
            elif isinstance(doc_df, ShapeData):
                data_type = doc_df.get_type()
                if data_type == "gdf":
                    print("☹️ Warning: GeoDataFrame is not well supported.")
                self.shape_data_arr[i] = doc_df.get_shape()
            
            else:
                raise ValueError(f"☹️ Cannot process {type(doc_df)}. Please read_data() first.")
        
        clear_output(wait=True)
        next_step()
    
    def ask_for_attributes(self):
        
        next_step = self.display_results

        if "decided_longitude_latitude" not in self.document:
            self.document["decided_longitude_latitude"] = {}

        for id_str in self.document["candidate_longitude_latitude"]:
            i = int(id_str)
            if id_str in self.document["decided_longitude_latitude"]:
                if self.shape_data_arr[i] is None:
                    x_att, y_att = self.document["decided_longitude_latitude"][id_str]
                    self.shape_data_arr[i] = ShapeData(df = self.doc_dfs[i].get_df(), x_att=x_att, y_att=y_att, meta={"crs": "epsg:4326"})
                continue


            df = self.doc_dfs[i].df
            json_code = self.document["candidate_longitude_latitude"][id_str]

            column_indices = [[df.columns.get_loc(pair["longitude_name"]), df.columns.get_loc(pair["latitude_name"])] for pair in json_code]

            colors = generate_seaborn_palette(len(column_indices))
            styled_df = color_columns_multiple(df.head(), colors, column_indices)
            display(HTML(wrap_in_scrollable_div(truncate_html_td(styled_df.to_html()))))
            
            
            def call_back(longitude, latitude):
                self.document["decided_longitude_latitude"][id_str] = [longitude, latitude]
                self.ask_for_attributes()
            
            display_longitude_latitude(json_code, list(df.columns), call_back)

            break
        
        if len(self.document["decided_longitude_latitude"]) == len(self.document["candidate_longitude_latitude"]):
            next_step()
    
    def display_results(self):
        create_progress_bar_with_numbers(1, geo_integration_steps)

        print("💡 Preview the data...")

        tab_data = [(f"{i}: {shapedata.get_name()}", 0, shapedata.to_html()) for i, shapedata in enumerate(self.shape_data_arr)]
        tabs = create_tabs_with_notifications(tab_data)
        display(tabs)

        print("💡 Please select the main CRS from one data")
        print("  We will transform all the other tables to the same CRS as the main table.")

        tables = [f"{i}: {shapedata.get_name()}" for i, shapedata in enumerate(self.shape_data_arr)]

        def my_callback(selected_item):
            index = tables.index(selected_item)
            clear_output(wait=True)
            self.transform_to_crs(index)

        create_select_widget(tables, my_callback)

    def transform_to_crs(self, crs_table_index):
        next_step = self.select_main_table

        self.crs_table_index = crs_table_index

        create_progress_bar_with_numbers(1, geo_integration_steps)
        print("💡 Transforming the data...")

        main_shape_data = self.shape_data_arr[crs_table_index]
        
        progress = show_progress(len(self.shape_data_arr) - 1)

        for i in range(len(self.shape_data_arr)):
            if i == crs_table_index:
                continue
            source_shape_data = self.shape_data_arr[i]
            result_shape_data = to_target_shape_data(source_shape_data, main_shape_data, semi_join=False)
            self.shape_data_arr[i] = result_shape_data
            progress.value += 1

        print("🎉 CRS has successfully aligned...")

        tab_data = [(f"{i}: {shapedata.get_name()}" + ("(main)" if i == crs_table_index else ""), 0, shapedata.to_html()) for i, shapedata in enumerate(self.shape_data_arr)]
        tabs = create_tabs_with_notifications(tab_data)
        display(tabs)

        print("🤓 Next we will integrate them")

        button = widgets.Button(description="Continue")

        def on_click(b):
            clear_output(wait=True)
            next_step()

        button.on_click(on_click)
        display(button)

    def select_main_table(self):
        next_step = self.aggregate_table

        create_progress_bar_with_numbers(2, geo_integration_steps)

        print("💡 Please select the main table")
        print(" We will integrate all the other tables to the main table.")

        tables = [f"{i}: {shapedata.get_name()}" for i, shapedata in enumerate(self.shape_data_arr)]

        def my_callback(selected_item):
            main_table_index = tables.index(selected_item)
            self.main_table_index = main_table_index
            clear_output(wait=True)
            self.aggregate_table()

        create_select_widget(tables, my_callback)
            
    def aggregate_table(self):
        create_progress_bar_with_numbers(2, geo_integration_steps)
        next_step = self.join_to_main_table

        main_table_index = self.main_table_index

        main_shape_data = self.shape_data_arr[main_table_index]


        for i in range(len(self.shape_data_arr)):
            if i == main_table_index:
                continue
            
            source_shape_data = self.shape_data_arr[i]

            data_type = source_shape_data.get_type()

            if data_type == "df":
                if source_shape_data.is_aggregated():
                    continue
                else:
                    print("💡 The following dataframe is not aggregated. Please aggregate and extract features.")
                    aggregate_step = GeoAggregationStep(shape_data=source_shape_data, sample_df=source_shape_data)

                    def callback(aggregate_step):
                        clear_output(wait=True)
                        final_shape = aggregate_step.run_codes(source_shape_data)
                        self.shape_data_arr[i] = final_shape
                        self.aggregate_table()
                    
                    aggregate_step.edit_widget(callbackfunc=callback)
                    return

        clear_output(wait=True)                      
        next_step()

    def join_to_main_table(self):

        print("💡 Integrating the data...")
        
        progress = show_progress(1)
        main_geo_data = self.shape_data_arr[self.main_table_index]
        geo_data_arr = [self.shape_data_arr[i] for i in range(len(self.shape_data_arr)) if i != self.main_table_index]
        result = integrate_geo(main_geo_data, geo_data_arr)
        progress.value += 1

        self.result = result

        print("🎉 Geo integration is done!")

        result.display()




    def write_document_to_disk(self, filepath: str):
        with open(filepath, 'w') as file:
            json.dump(self.document, file)

    def read_document_from_disk(self, filepath: str, viewer=True):
        with open(filepath, 'r') as file:
            self.document = json.load(file)

def join_raster_to_df_points(raster_shape, df_shape):

    raster_crs = raster_shape.get_crs()

    raster_transform = raster_shape.get_geo_transform()

    df_shape = to_target_crs_and_transform(df_shape, target_crs=raster_crs, target_transform=raster_transform)

    converted_rows = df_shape.df[df_shape.y_att].values.astype(int)
    converted_cols = df_shape.df[df_shape.x_att].values.astype(int)

    raster_np_array = raster_shape.get_np_array()

    result = extract_values_by_indices(raster_np_array, converted_rows, converted_cols)
    

    return result






def process_query_to_dbt(query, input_tables):
    pattern = r"^\s*CREATE\s+(?:OR\s+REPLACE)?(?:TABLE|VIEW)\s+\S+\s+AS"

    query = re.sub(pattern, "", query, flags=re.IGNORECASE)


    for table in input_tables:
        query = re.sub(rf"\b{table}\b", r"{{ ref('" + table + r"') }}", query)

    return query


class ProductSteps(Node):
    default_name = 'Product Steps'
    default_description = 'This is typically the dicussion result from business analysts and engineers'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        document = ["Setup", "Source", "Stage", "Model"]


        dbt_project_dir = self.input_item["dbt_directory"]


        header_html = f'<div style="display: flex; align-items: center;">' \
            f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
            f'<div style="margin: 0; padding: 0;">' \
            f'<h1 style="margin: 0; padding: 0;">Data Product</h1>' \
            f'<p style="margin: 0; padding: 0;">Powered by cocoon</p>' \
            f'</div>' \
            f'</div><hr>'

        description_html = f"""
        <div>
            <h2>Welcome to Cocoon for your data product development! 🛠️</h2>
            <p>There are 4 steps:</p>
            <ol>
                <li><strong>Setup 📋</strong>
                    <ul>
                        <li>You provide:
                            <ul>
                                <li>Your dbt project ({dbt_project_dir}).</li>
                                <li>Your business workflow.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Source 🔍</strong>
                    <ul>
                        <li>We handle, you verify:
                            <ul>
                                <li>Integrating multiple partitions into a single table.</li>
                                <li>Aligning schemas from mismatched data sources.</li>
                                <li>Documenting data sources with semantic meanings.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Stage 🚧</strong>
                    <ul>
                        <li>We handle, you verify:
                            <ul>
                                <li>Cleaning, transforming and deduplicating tables.</li>
                                <li>Updating CRUD tables (with deletion and overwrite ops) to the latest snapshot.</li>
                                <li>Normalizing tables containing multiple entities.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
                <li><strong>Model 📊</strong>
                    <ul>
                        <li>We handle, you verify:
                            <ul>
                                <li>Linking data sources to your business workflow.</li>
                                <li>Creating dimension and fact tables with a join graph.</li>
                            </ul>
                        </li>
                    </ul>
                </li>
            </ol>
            <p>Congratulations! You've successfully created the data products, ready for dashboards, ad hoc queries, or LLM applications. 🌟</p>
        </div>
        """

        display(HTML(header_html + description_html))

        next_button = widgets.Button(description="Start", button_style='success')
        
        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)
        display(next_button)


class DataSource(Node):
    default_name = 'Data Source'
    default_description = 'This reads from the data source'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        con = self.input_item["con"]
        directory = self.input_item["dir"]

        create_progress_bar_with_numbers(0, self.global_document["Data Product"]['Product Steps'])

        idx = 1
        html_content = ""

        files = list_files_in(directory)

        print("🧐 Scanning the data source ...")

        num_files = len(files)
        increment = max(1, num_files // 10)
        progress_counter = 0

        self.progress = show_progress(10)

        for file in files:
            if file.endswith(".csv"):
                file_path = f"{directory}/{file}"
                table_name = file.split(".")[0]

                df = pd.read_csv(file_path)

                run_sql_return_df(con, f"CREATE TABLE {table_name} AS SELECT * FROM df")

                html_content += f"<b>{idx}. {table_name} (<a href=\"{file_path}\">{file_path}</a>)</b>"
                html_content += wrap_in_scrollable_div(truncate_html_td(df.head(2).to_html()))
                idx += 1

            progress_counter += 1
            if progress_counter % increment == 0 or progress_counter == num_files:
                self.progress.value += 1

        print(f"🤓 We found {idx-1} seed tables. Below are their {BOLD}samples{END}...")

        schema_df = run_sql_return_df(con, "describe;")

        tables = {}
        for _, row in schema_df.iterrows():
            table_name = row['name']
            column_name = row['column_names']
            tables[table_name] = column_name

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(tables)
                
        next_button.on_click(on_button_click)
        display(next_button)

        display(HTML(wrap_in_scrollable_div(html_content, height='800px')))


class BusinessWorkflow(Node):
    default_name = 'Business Workflow'
    default_description = 'This is typically the dicussion result from business analysts and engineers'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        create_progress_bar_with_numbers(0, self.global_document["Data Product"]['Product Steps'])

        print("🤓 The discussed business workflow is as follows...")

        document = { "title" : "Retail Brokerage",
                    "descriptions": [
                        ("Customer holds Account and engages Broker.", 
                        [["Customer", "holds", "Account"], 
                        ["Customer", "engages", "Broker"]]),
                        ("Broker monitors Security and advises on Trade for customers.", 
                        [["Broker", "monitors", "Security"],
                        ["Broker", "advises on", "Trade"]]),
                        ("Company issues Security.", 
                        [["Company", "issues", "Security"]]),
                        ("Account reflects CashBalances and Holdings, showing ownership and value.", 
                        [["Account", "reflects", "CashBalances"], 
                        ["Account", "reflects", "Holdings"]]),
                        ("Security has a MarketHistory, tracking its performance and trends.", 
                        [["Security", "has", "MarketHistory"]])
                    ]
        }

        display_pages(total_page=len(document["descriptions"]), 
                      create_html_content=partial(create_html_content_project, 
                                                  document["title"], document["descriptions"]))

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)
        display(next_button)

class GroupDataSource(Node):
    default_name = 'Group Data Source'
    default_description = 'This groups the data source'

    def extract(self, input_item):
        clear_output(wait=True)

        create_progress_bar_with_numbers(1, self.global_document["Data Product"]['Product Steps'])
        
        print("🧐 Grouping the data source ...")
        self.progress = show_progress(1)

        tables = self.global_document["Data Product"]["Data Source"]
        self.groups = group_tables_exclude_single(tables)
        table_list = "\n".join([f"{idx}. {names}: {tables[names[0]]}" for idx, names in enumerate(self.groups)])
        return table_list

    def run(self, extract_output, use_cache=True):
        table_list = extract_output
        template = f"""Below are a list of table groups. Each table group has the same attributes.
We list the [table names]: [attributes] for each group:
{table_list}

The goal is to pick new names for a dbt project. 
The new table name shall start with src_. It is preferably lowercase and retain original table names as much as possible.
Now, return in the following format:
```json
[
    "new name for group 0",
    ...
]
```"""

        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)

        assistant_message = response['choices'][0]['message']
        messages.append(assistant_message)
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        summary = json.loads(json_code)

        

        return summary


    def postprocess(self, run_output, callback, viewer=False, extract_output=None):

        self.progress.value += 1
        new_names = run_output

        grouped_tables = {}

        for i in range(len(new_names)):
            new_name = new_names[i]
            group = self.groups[i]
            grouped_tables[new_name] = group

        print("🤓 We have identified potentially partitioned tables... ")

        for table in grouped_tables:
            group = grouped_tables[table]
            if len(group) > 1:
                print(f"{BOLD}{table}{END}: The source is partitioned from {len(group)} tables:")
                print(f"    {group}")
                print(f"    {BOLD}Reason{END}: These partitions share the same attributes.")
                print()

        print("😎 Next we will write codes to load source ...")
                

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(grouped_tables)

        next_button.on_click(on_button_click)

        display(next_button)


class SourceCodeWriting(Node):
    default_name = 'Source Code Writing'
    default_description = 'This writes the SQL codes to load the source'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(1, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Writing the SQL code to load the src ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        group = self.global_document["Data Product"]["Group Data Source"][table_name]
        schema = self.global_document["Data Product"]["Data Source"][group[0]]

        self.group = group
        self.table_name = table_name

        return table_name, group, schema

    def run(self, extract_output, use_cache=True):
        new_table_name, group, schema = extract_output

        sql_query = ""

        if len(group) > 1:
            template = f"""Below are a list of table names, all with the same attriutes {schema}:
tables = {group}

Write a python function that takes the tables var and write the SQL query that 
(1) "CREATE TABLE {new_table_name} AS" that unions all the tables in the tables var. 
(2) extracts features from the table name, if there are any, and creates new columns for each

The function should return the SQL query as a string.
Now, first brainstorm the steps you would take. Conclude by filling in the function. Please don't change the function signature. Don't include example usage.
```python
def create_union_table(tables):
    # Write your code here
    return query
```"""
            messages = [{"role": "user", "content": template}]

            response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
            messages.append(response['choices'][0]['message'])
            self.messages.append(messages)
            
            python_code = extract_python_code(response['choices'][0]['message']['content'])
            exec(python_code, globals())

            sql_query = create_union_table(group)

        else:
            sql_query = f"CREATE TABLE {new_table_name} AS SELECT * FROM {group[0]}"
    
        return sql_query
            
        
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        sql_query = run_output
        con = self.input_item["con"]
        run_sql_return_df(con, robust_create_to_replace(sql_query))

        input_tables = self.group
        dbt_formatted_query = process_query_to_dbt(sql_query, input_tables)
        dbt_project_dir = self.input_item["dbt_directory"]
        dbt_file_path = f"{dbt_project_dir}/models/source/{self.table_name}.sql"

        with open(dbt_file_path, "w") as f:
            f.write(dbt_formatted_query)

        callback(sql_query)
    
    
class SourceCodeWritingForAll(MultipleNode):

    default_name = 'Source Code Writing For All'
    default_description = 'This writes the SQL codes to load the source'

    def construct_node(self, element_name, idx=0, total=0):
        node = SourceCodeWriting(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):
        self.elements = list(self.global_document["Data Product"]["Group Data Source"].keys())
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        clear_output(wait=True)

        tab_data = []

        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"

        border_style = """
        <style>
        .border-class {
            border: 1px solid black;
            padding: 10px;
            margin: 10px;
        }
        </style>
        """

        combined_css = css_style + border_style

        dbt_project_dir = self.item["dbt_directory"]
        

        for key in document["Source Code Writing"]:
            sql_query = document["Source Code Writing"][key]

            dbt_file_path = f"{dbt_project_dir}/models/source/{key}.sql"

            highlighted_sql = wrap_in_scrollable_div(highlight(sql_query, SqlLexer(), formatter))
            file_path_dbt = f"The dbt file is saved at <a href=\"{dbt_file_path}\">{dbt_file_path}</a>" 
            bordered_content = f'<div class="border-class">{file_path_dbt}{highlighted_sql}</div>'
            tab_data.append((key, combined_css + bordered_content))

        tabs = create_dropdown_with_content(tab_data)
        
        print("🎉 The SQL codes to load the src have been written ...")
        display(tabs)
        print("😎 Next we will document each src table ...")
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)

        display(next_button)

class SourceDocument(Node):
    default_name = 'Source Document'
    default_description = 'This step documents each source table'
    clean = True

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):

        con = self.input_item["con"]
        table_name = self.para["element_name"]




        df = run_sql_return_df(con, f"select * from {table_name}")
        doc_df = DocumentedData(df, table_name=table_name)
        doc_df.rename_table=False

        doc_df.start(viewer=True)


        dbt_project_dir = self.input_item["dbt_directory"]
        dbt_file_path = f"{dbt_project_dir}/models/source/{table_name}.yml"

        yaml_dict = doc_df.to_yml()

        
        group = self.global_document["Data Product"]["Group Data Source"][table_name]

        for item in yaml_dict["models"]:
            if item["name"] == table_name:
                if "cocoon_tags" not in item:
                    item["cocoon_tags"] = {}
                if len(group) > 1:
                    item["cocoon_tags"]["partition"] = group
                else:
                    item["cocoon_tags"]["select_all"] = group[0]

        with open(dbt_file_path, 'w') as file:
            yaml.dump(yaml_dict, file)
            
            
        if "table_document" not in self.input_item:
            self.input_item["table_document"] = {}
        self.input_item["table_document"][table_name] = doc_df
        callback(doc_df.document)


class SourceDocumentForAll(MultipleNode):
    
    default_name = 'Source Document For All'
    default_description = 'This writes the SQL codes to load the source'

    def construct_node(self, element_name, idx=0, total=0):
        node = SourceDocument(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):
        self.elements = list(self.global_document["Data Product"]["Group Data Source"].keys())
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item
    
    def display_after_finish_workflow(self, callback, document):

        dropdown = widgets.Dropdown(
            options=document["Source Document"].keys(),
        )

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)


        def on_change(change):
            if change['type'] == 'change' and change['name'] == 'value':
                clear_output(wait=True)

                print("🎉 All the source tables have been documented!")
                
                print("😎 Next we will stage the tables ...")
                display(next_button)
                
                print("🧐 You can explore the documents below ...")
                display(dropdown)
                self.item["table_document"][change['new']].display_document()

        dropdown.observe(on_change)

        clear_output(wait=True)

        print("🎉 All the source tables have been documented!")
        print("😎 Next we will stage the tables ...")
        display(next_button)
        
        print("🧐 You can explore the documents for the source tables below ...")
        display(dropdown)
        if len(document["Source Document"]) > 0:
            self.item["table_document"][dropdown.value].display_document()

class BasicStage(Node):
    default_name = 'Basic Stage'
    default_description = 'This stages the source tables'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(2, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Writing the SQL code to stage the table ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        stg_table_name = rename_for_stg(table_name)
        doc_df = item["table_document"][table_name]
        basic_description = doc_df.get_basic_description()

        return table_name, stg_table_name, basic_description

    def run(self, extract_output, use_cache=True):
        table_name, stg_table_name, basic_description = extract_output

        template = f"""Write the SQL query for the stg table of the following table:

{basic_description}

The stg table shall be a SELECT FROM {table_name}.
Most of the times, the columns will be the same as the src table.

However, there are simple transformation you want to perform:
- Change the data type of a column. E.g., some value shall be better represented as INT/DATETIME instead of string. 
- Trim the string columns if there are leading or trailing spaces.
- Are there any columns that are redundant? If so, remove them.
- Split a column into multiple columns. E.g., each cell contains multiple values.
First reason about the table. Go through each column and its values in the reasoning.
Finally provide a list of transformations and the SQL query with detailed comments (skip comments for unchanged columns). Please use duckdb syntax.

```json
{{
    "reason: "1. format ... 2. trim... 3. redundancy ... 4. split ..."
    "transformations": [
        "..."
    ],
    "sql": "CREATE TABLE {stg_table_name} AS
SELECT
    W as W,
    -- trim column X
    trim(X) as X,
    -- Y column is removed
    -- split column Z into Z1, Z2
    regexp_split_to_array(Z, ',')[1] AS Z1,
    regexp_split_to_array(Z, ',')[2] AS Z2
    ...
FROM {table_name}"
}}
```""" 
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary
            
        
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        sql_query = run_output['sql']
        con = self.input_item["con"]
        run_sql_return_df(con, robust_create_to_replace(sql_query))

        input_table = self.para["element_name"]
        output_table = self.para["element_name"].replace("src", "stg")
        dbt_formatted_query = process_query_to_dbt(sql_query, [input_table])
        dbt_project_dir = self.input_item["dbt_directory"]
        dbt_file_path = f"{dbt_project_dir}/models/stage/{output_table}.sql"

        with open(dbt_file_path, "w") as f:
            f.write(dbt_formatted_query)

        yml_file_path = f"{dbt_project_dir}/models/stage/{output_table}.yml"
        yml_dict = yml_from_name(output_table)

        for item in yml_dict["models"]:
            if item["name"] == output_table:
                if "cocoon_tags" not in item:
                    item["cocoon_tags"] = {}
                item["cocoon_tags"]["stage"] = None

        with open(yml_file_path, 'w') as file:
            yaml.dump(yml_dict, file)

        callback(sql_query)
    

class BasicStageForAll(MultipleNode):
    default_name = 'Basic Stage For All'
    default_description = 'This stages the source tables'

    def construct_node(self, element_name, idx=0, total=0):
        node = BasicStage(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):
        self.elements = list(self.global_document["Data Product"]["Group Data Source"].keys())
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        tab_data = []

        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"
        border_style = """
        <style>
        .border-class {
            border: 1px solid black;
            padding: 10px;
            margin: 10px;
        }
        </style>
        """

        combined_css = css_style + border_style

        dbt_project_dir = self.item["dbt_directory"]

        for key in document["Basic Stage"]:
            sql_query = document["Basic Stage"][key]
            
            dbt_file_path = f"{dbt_project_dir}/models/source/{key}.sql"

            highlighted_sql = wrap_in_scrollable_div(highlight(sql_query, SqlLexer(), formatter))
            file_path_dbt = f"The dbt file is saved at <a href=\"{dbt_file_path}\">{dbt_file_path}</a>" 
            bordered_content = f'<div class="border-class">{file_path_dbt}{highlighted_sql}</div>'
            tab_data.append((key, combined_css + bordered_content))

        tabs = create_dropdown_with_content(tab_data)
        
        print("🎉 The SQL codes to stage the tables have been written!")
        display(tabs)
        print("😎 Next we will analyze the CRUD tables...")
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)
        display(next_button)

class DecideCRUDTable(Node):
    default_name = 'Decide CRUD Table'
    default_description = 'This decide if the given table contains CRUD/overwriting'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(2, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Deciding the CRUD tables ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        doc_df = item["table_document"][table_name]
        basic_description = doc_df.get_basic_description()

        return basic_description

    def run(self, extract_output, use_cache=True):
        basic_description = extract_output

        template =  f"""{basic_description}

Does the table rows represent "Create Read Update Delete"/Overwriting Logics?
If so, what are the columns that represent the operation and date? Leave as null if date is not available.
Respond with the following format:
```json
{{
    "reasoning": "..."
    "crud": true/false,
    (if true)
    "op_cols": ["col1", "col2"...]
    "date_cols": ["col1", "col2"...]
}}
```"""
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
    
class DecideCRUDForAll(MultipleNode):
    default_name = 'Decide CRUD For All'
    default_description = 'This decide if the given table contains CRUD/overwriting'

    def construct_node(self, element_name, idx=0, total=0):
        node = DecideCRUDTable(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):
        self.elements = list(self.global_document["Data Product"]["Group Data Source"].keys())
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        tab_data = []

        for key in document["Decide CRUD Table"]:
            if document["Decide CRUD Table"][key]["crud"]:
                op_cols = document["Decide CRUD Table"][key]["op_cols"]
                date_cols = document["Decide CRUD Table"][key]["date_cols"]
                doc_df = self.item["table_document"][key]

                columns = list(doc_df.df.columns)
                op_col_indices = [columns.index(col) for col in op_cols if col in columns]
                date_col_indices = [columns.index(col) for col in date_cols if col in columns]
                df_sample =  color_columns(doc_df.df.head(), 'lightgreen', op_col_indices+date_col_indices)

                tab_data.append((key, f"These columns represent CRUD operations: <i>{op_cols + date_cols}</i>" +\
                                        wrap_in_scrollable_div(truncate_html_td(df_sample.to_html()))))

        if len(tab_data) > 0:
            print(f"🧐 We have identified {BOLD}{len(tab_data)}{END} tables with CRUD logics:")
            tabs = create_dropdown_with_content(tab_data)
            display(tabs)

            print("😔 CRUD tables are hard to model and query.")
            print("😎 Instead, we will perform CRUD ops to create its latest snapshot ...")
        else:
            print("🤓 No CRUD/overwriting tables have been identified.")

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)
        

        next_button.on_click(on_button_click)
        display(next_button)

class PerformCRUD(Node):
    default_name = 'Perform CRUD'
    default_description = 'This performs CRUD operations'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(2, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Performing the CRUD operations ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        op_cols = self.global_document["Data Product"]["Decide CRUD For All"]["Decide CRUD Table"][table_name]["op_cols"]
        date_cols = self.global_document["Data Product"]["Decide CRUD For All"]["Decide CRUD Table"][table_name]["date_cols"]

        doc_df = item["table_document"][table_name]
        
        con = item["con"]
        stg_table_name = rename_for_stg(table_name)
        df = run_sql_return_df(con, f"select * from {stg_table_name}")
        doc_df.table_name = stg_table_name

        return op_cols, date_cols, doc_df, df, con

    def run(self, extract_output, use_cache=True):
        summary = {}
        op_cols, date_cols, doc_df, df, con = extract_output

        template = f"""{doc_df.get_sample_text()}
{doc_df.document["table_summary"]["summary"]}

This table represents "Create Read Update Delete" operations. {op_cols + date_cols} represent the operation-related columns.
Suppose the operations has **been performed**. 
What are the primary key for the final table (should exclude any column for operation)? 
You should think about what the table is about. 
If it's about a single entity, the key is entity id.
If it's about relation, the key is all the entity keys.

```json
{{
    "reasoning": "..."
    "primary_key": ["col1", "col2"...]
}}
```"""
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)

        summary["primary_key"] = json_code["primary_key"]
        primary_key = json_code['primary_key']

        deduplicated_df = df.drop_duplicates(subset=op_cols, keep='first')
        op_col_desc = "\n".join([describe_column(doc_df.stats, col) for col in  op_cols])
        template = f"""{describe_df_in_natural_language(df=deduplicated_df, 
                                table_name=doc_df.table_name, 
                                num_rows_to_show=len(deduplicated_df))}
{op_cols} represent the operation-related columns:
{op_col_desc} 

Help me classify the values in operator columns into CUD.
You shall try to understand what each value mean, and what are they creating, updating, or deleting.
For operations like deactivate, classify them as delete.
```json
{{
    "reasoning": "The operator values are ... Each means..."
    "create": {{
        "values": ["val1", val2", ...]
        "explanation": "val1 creates ... val2 creates ..."
    }}
    "update": ...
    "delete": ..
}}
```"""

        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)

        summary["attributes"] = json_code

        create = json_code["create"]
        update = json_code["update"]
        delete = json_code["delete"]

        deleted_table_name = doc_df.table_name.replace("stg", "txn")

        if len(create['values']) > 0:

            deleted_table_name += "_del"
            
            template = f"""{describe_df_in_natural_language(df=deduplicated_df, 
                                    table_name=doc_df.table_name, 
                                    num_rows_to_show=len(deduplicated_df))}
{op_cols} represent the operation-related columns.
The following values mean deletion: {delete['values']}
{delete['explanation']}

Write a SQL query that returns a new table '{deleted_table_name}' that performs all deletions.

First, read the rows for deletion. For each type of deletion, what it deletes, and how it use to identify the entities.
The primary key after the ops shall be {primary_key}. The ids likely come from them.
Then, remove the identified rows for each type of deletion.
Return the following:
```json
{{
    "reasoning": "For operator X, it identifies deleted rows by ..."
    "sql": "create table {deleted_table_name} as 
SELECT *
FROM {doc_df.table_name} as y
WHERE 
NOT EXISTS (SELECT 1 FROM stg_customer_mgmt 
            WHERE operator = del_op1
            AND id = t.id AND...) 
AND 
NOT EXISTS ... (for each deletion ops)"
}}
```"""
            messages = [{"role": "user", "content": template}]

            response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
            messages.append(response['choices'][0]['message'])
            self.messages.append(messages)
            
            json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
            json_code = replace_newline(json_code)
            json_code = json.loads(json_code)

            summary["deleted_table"] = json_code
            run_sql_return_df(con, robust_create_to_replace(json_code['sql']))
            df = run_sql_return_df(con, f"select * from {deleted_table_name}")

            input_table = doc_df.table_name
            output_table = deleted_table_name
            dbt_formatted_query = process_query_to_dbt(json_code['sql'], [input_table])
            dbt_project_dir = self.input_item["dbt_directory"]
            dbt_file_path = f"{dbt_project_dir}/models/stage/{output_table}.sql"
            with open(dbt_file_path, "w") as f:
                f.write(dbt_formatted_query)

        
        stats = collect_df_statistics_(df, doc_df.document)
        op_col_desc = "\n".join([describe_column(stats, col) for col in  op_cols + date_cols])
        deduplicated_df = df.drop_duplicates(subset=op_cols, keep='first')
        new_name_del = deleted_table_name
        new_name_final =  new_name_del.replace("_del", "_update")
        template = f"""{describe_df_in_natural_language(df=deduplicated_df, 
                                table_name=new_name_del, 
                                num_rows_to_show=len(deduplicated_df))}
{op_cols + date_cols} represent the operation-related columns.

The goal is to write a SQL query that returns a new table '{new_name_final}', which performs all updates in duckdb.
The primary keys for the final table shall be {primary_key}.

Now, write the SQL for update. The update rows may contain NULL for non-updated columns.
Use LAST_VALUE(value IGNORE NULLS) to get the latest value for each column.
Partition the table by the primary key and order by the transaction time
For the final table, exclude the columns for operation and date.

Now, respond in the following format:
```json
{{
    "reasoning": "For operator X, it identifies deleted rows by ..."
    "sql": "create table {new_name_final} as 

WITH imputed AS (            
    SELECT 
        LAST_VALUE(value IGNORE NULLS) OVER w as col1,
        ...
        ROW_NUMBER() OVER w AS rn
    FROM {new_name_del}
    WINDOW w AS (PARTITION BY primary keys ORDER BY timestamp DESC ROWS BETWEEN CURRENT ROW AND UNBOUNDED FOLLOWING)
)

SELECT * EXCLUDE (rn)
FROM imputed
WHERE rn = 1
ORDER BY primary keys"}}
```"""
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)

        summary["final_table"] = json_code
        run_sql_return_df(con, robust_create_to_replace(json_code['sql']))

        input_table = new_name_del
        output_table = new_name_final
        dbt_formatted_query = process_query_to_dbt(json_code['sql'], [input_table])
        dbt_project_dir = self.input_item["dbt_directory"]
        dbt_file_path = f"{dbt_project_dir}/models/stage/{output_table}.sql"
        with open(dbt_file_path, "w") as f:
            f.write(dbt_formatted_query)

        
        
        yml_file_path = f"{dbt_project_dir}/models/stage/{output_table}.yml"
        yml_dict = yml_from_name(output_table)

        for item in yml_dict["models"]:
            if item["name"] == output_table:
                if "cocoon_tags" not in item:
                    item["cocoon_tags"] = {}
                item["cocoon_tags"]["crud"] = None

        with open(yml_file_path, 'w') as file:
            yaml.dump(yml_dict, file)

        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)


class PerformCRUDForAll(MultipleNode):
    default_name = 'Perform CRUD For All'
    default_description = 'This performs CRUD operations'

    def construct_node(self, element_name, idx=0, total=0):
        node = PerformCRUD(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):
        elements = []
        for key in self.global_document["Data Product"]["Decide CRUD For All"]["Decide CRUD Table"]:
            if self.global_document["Data Product"]["Decide CRUD For All"]["Decide CRUD Table"][key]["crud"]:
                elements.append(key)

        self.elements = elements
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        if len(self.elements) == 0:
            callback(document)
            return

        tab_data = []

        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"
        border_style = """
        <style>
        .border-class {
            border: 1px solid black;
            padding: 10px;
            margin: 10px;
        }
        </style>
        """

        combined_css = css_style + border_style
        dbt_project_dir = self.item["dbt_directory"]

        for key in document["Perform CRUD"]:
            sql_query = document["Perform CRUD"][key]["final_table"]["sql"]

            if "deleted_table" in document["Perform CRUD"][key]:
                deleted_sql_query = document["Perform CRUD"][key]["deleted_table"]["sql"]
                sql_query = f"{deleted_sql_query}\n\n{sql_query}"

            output_table_name = key.replace("stg", "txn") + "_update"

            dbt_file_path = f"{dbt_project_dir}/models/stage/{output_table_name}.sql"
            file_path_dbt = f"The dbt file is saved at <a href=\"{dbt_file_path}\">{dbt_file_path}</a>" 

            highlighted_sql = wrap_in_scrollable_div(highlight(sql_query, SqlLexer(), formatter))
            bordered_content = f'<div class="border-class">{file_path_dbt}{highlighted_sql}</div>'

            tab_data.append((key.replace("src_", "stg_"), combined_css + bordered_content))

        print("🎉 The CRUD operations have been performed!")
        if len(tab_data) > 0:
            tabs = create_dropdown_with_content(tab_data)
            display(tabs)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)
        display(next_button)

class DecideForeignKey(Node):
    default_name = 'Decide Foreign Key'
    default_description = 'This decides the foreign key'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(2, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Deciding the foreign key ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        source_table = self.para["source_table"]
        con = item["con"]
        df = run_sql_return_df(con, f"SELECT * FROM {table_name}")
        doc_df = item["table_document"][source_table]
        basic_summary = doc_df.document['main_entity']["summary"]

        return df, basic_summary, table_name

    def run(self, extract_output, use_cache=True):
        df, basic_summary, table_name = extract_output

        template = f"""{describe_df_in_natural_language(df=df, 
                        table_name=table_name, 
                        num_rows_to_show=2)}

The table is about {basic_summary}

Find the columns look like foreign key. For each foreign key, what dimension it refers to?

Respond in the following format:
```json
{{
    "reasoning": "This table bridges dimensions or not.  The foreign keys for X are ... They correspond to Y dim. ..",
    "foreign_keys": [
        {{"key": ["col1", "col2", ...],
          "dimension": "dim1"}},
          ...
    ]
}}
```"""  

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)
        
        for message in messages:
            write_log(message['content'])
            write_log("---------------------")

        return json_code


    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)

class DecideForeignKeyForAll(MultipleNode):
    default_name = 'Decide Foreign Key For All'
    default_description = 'This decides the foreign key'

    def construct_node(self, element_name, idx=0, total=0, source_table=None):
        node = DecideForeignKey(para={"element_name": element_name, "idx": idx, "total": total, "source_table": source_table})
        node.inherit(self)
        return node

    def extract(self, item):
        source_tables = self.global_document["Data Product"]["Group Data Source"].keys()
        self.elements = []
        self.nodes = {}

        for idx, source_table in enumerate(source_tables):
            if source_table in self.global_document["Data Product"]["Perform CRUD For All"]["Perform CRUD"]:
                table_name = source_table.replace("src", "txn") + "_update"
                self.elements.append(table_name)
                self.nodes[table_name] = self.construct_node(table_name, idx, len(source_tables), source_table = source_table)

            else:
                table_name = source_table.replace("src", "stg")
                self.elements.append(table_name)
                self.nodes[table_name] = self.construct_node(table_name, idx, len(source_tables), source_table = source_table)

        self.item = item

class PerformNormalization(Node):
    default_name = 'Perform Normalization'
    default_description = 'This performs normalization'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(2, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Performing the normalization ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        self.table_name = table_name
        source_table = self.para["source_table"]
        con = item["con"]
        df = run_sql_return_df(con, f"SELECT * FROM {table_name}")
        doc_df = item["table_document"][source_table]

        self.foreign_keys = self.global_document["Data Product"]["Decide Foreign Key For All"]["Decide Foreign Key"][table_name]["foreign_keys"]
        fk_desc = []
        for entry in self.foreign_keys:
            key = entry["key"]
            dimension = entry["dimension"]
            fk_desc.append(f"{dimension}: {key}")
        fk_desc = "\n".join([f"{idx}. {entry}" for idx, entry in enumerate(fk_desc)]) 

        basic_summary = doc_df.document['main_entity']["summary"]

        return df, basic_summary, table_name, fk_desc, len(self.foreign_keys)

    def run(self, extract_output, use_cache=True):
        df, basic_summary, table_name, fk_desc, num_foreign_keys = extract_output

        template = f"""{describe_df_in_natural_language(df=df, 
                                table_name=table_name, 
                                num_rows_to_show=2)}

The table is about {basic_summary}

The table contains {num_foreign_keys} entities with their keys:
{fk_desc}

For each attribute, is it obviously **uniquely decided** by only one entity's key to normalize?

E.g, Country is **uniquely decided** by City. User Name is **uniquely decided** by User Id.
Note the difference between **uniquely decided** and **relates**.
However, City relates to Country, but is NOT **uniquely decided** by Country.

For each entity, return the list of attributes obviously **uniquely decided** by only this entity (and its key) independent of other info in the row.
Most of the time, the list shall be empty.

Respond in the following format:
```json
{{
    "reasoning": "The table is about ... X is **uniquely decided** by entity Y because when we know Y, we know X for sure.",
    "attributes": 
        {{"dim1": ["col1", "col2", ...],
          ...
        }}
    
}}
```"""
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])

        template = """Are there attributes from the previous answers also depend on other entities?
If so, remove them from the list.
Respond in the following format:
```json
{
    "reasoning": "col1 in dim1 also depend on dim2, so it's not **uniquely decided** by dim1",
    "attributes": 
        {"dim1": ["col2", ...],
          ...
        }
    
}
```"""
        messages += [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)

        for message in messages:
            write_log(message['content'])
            write_log("---------------------")

        return json_code

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        def create_query(new_table_name1, new_table_name2, key_atts, non_key_atts, table):
            key_atts_str = ', '.join(key_atts)

            non_key_atts_str = ', '.join(f'MAX({att}) AS {att}' for att in non_key_atts)

            select_clause = f'{key_atts_str}, {non_key_atts_str}'

            sql_query1 = f"""CREATE TABLE {new_table_name1} AS
-- MAX is used to prioritize non-null values
SELECT {select_clause} 
FROM {table} 
GROUP BY {key_atts_str}
ORDER BY {key_atts_str}
"""

            sql_query2 = f"""CREATE TABLE {new_table_name2} AS    
SELECT * EXCLUDE ({", ".join(non_key_atts)})
FROM {table}"""
            
            return sql_query1, sql_query2

        con = self.input_item["con"]

        for entry in self.foreign_keys:
            key = entry["key"]
            dimension = entry["dimension"]

            if dimension in run_output["attributes"]:
                attributes = run_output["attributes"][dimension]
                non_key_attributes = list(set(attributes) - set(key))
                non_key_attributes = sorted(non_key_attributes)
                if non_key_attributes:
                    new_table_name1 =  f"stg_{dimension.lower()}"
                    new_table_name2 = self.table_name + "_normalized"
                    sql_query1, sql_query2 = create_query(new_table_name1,  new_table_name2, key, non_key_attributes, self.table_name)
                    if "normalize" not in run_output:
                        run_output["normalize"] = {}
                    run_output["normalize"][dimension] = {}
                    run_output["normalize"][dimension]["sql_query"] = [sql_query1, sql_query2]
                    run_output["normalize"][dimension]["new_table_name"] = [new_table_name1, new_table_name2]
                    run_sql_return_df(con, robust_create_to_replace(sql_query1))
                    run_sql_return_df(con, robust_create_to_replace(sql_query2))

                    input_table = self.table_name
                    output_table = new_table_name1
                    dbt_formatted_query = process_query_to_dbt(sql_query1, [input_table])
                    dbt_project_dir = self.input_item["dbt_directory"]
                    dbt_file_path = f"{dbt_project_dir}/models/stage/{output_table}.sql"
                    with open(dbt_file_path, "w") as f:
                        f.write(dbt_formatted_query)
                    

                    yml_file_path = f"{dbt_project_dir}/models/stage/{output_table}.yml"
                    yml_dict = yml_from_name(output_table)

                    for item in yml_dict["models"]:
                        if item["name"] == output_table:
                            if "cocoon_tags" not in item:
                                item["cocoon_tags"] = {}
                            item["cocoon_tags"]["normalization"] = None

                    with open(yml_file_path, 'w') as file:
                        yaml.dump(yml_dict, file)


                    input_table = self.table_name
                    output_table = new_table_name2
                    dbt_formatted_query = process_query_to_dbt(sql_query2, [input_table])
                    dbt_project_dir = self.input_item["dbt_directory"]
                    dbt_file_path = f"{dbt_project_dir}/models/stage/{output_table}.sql"
                    with open(dbt_file_path, "w") as f:
                        f.write(dbt_formatted_query)

                    
                    yml_file_path = f"{dbt_project_dir}/models/stage/{output_table}.yml"
                    yml_dict = yml_from_name(output_table)

                    for item in yml_dict["models"]:
                        if item["name"] == output_table:
                            if "cocoon_tags" not in item:
                                item["cocoon_tags"] = {}
                            item["cocoon_tags"]["normalization"] = None

                    with open(yml_file_path, 'w') as file:
                        yaml.dump(yml_dict, file)
    
        callback(run_output)


class PerformNormalizationForAll(MultipleNode):
    default_name = 'Perform Normalization For All'
    default_description = 'This performs normalization'

    def construct_node(self, element_name, idx=0, total=0, source_table=None):
        node = PerformNormalization(para={"element_name": element_name, "idx": idx, "total": total, "source_table": source_table})
        node.inherit(self)
        return node

    def extract(self, item):
        self.elements = []

        source_tables = self.global_document["Data Product"]["Group Data Source"].keys()
        self.elements = []
        table_to_source = {}

        for source_table in source_tables:
            if source_table in self.global_document["Data Product"]["Perform CRUD For All"]["Perform CRUD"]:
                table_name = source_table.replace("src", "txn") + "_update"
            else:
                table_name = source_table.replace("src", "stg")

            table_to_source[table_name] = source_table

            foreign_keys = self.global_document["Data Product"]["Decide Foreign Key For All"]["Decide Foreign Key"][table_name]["foreign_keys"]
            if len(foreign_keys) == 0:
                continue
            
            self.elements.append(table_name)

        self.nodes = {element: self.construct_node(element, idx, len(self.elements), source_table=table_to_source[element])
                        for idx, element in enumerate(self.elements)}
        
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        if len(self.elements) == 0:
            callback(document)
            return
        
        tabs = []

        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"

        for key in document["Perform Normalization"]:
            if "normalize" in document["Perform Normalization"][key]:
                for dimension in document["Perform Normalization"][key]["normalize"]:
                    sql_queries = document["Perform Normalization"][key]["normalize"][dimension]["sql_query"]
                    highlighted_sql = wrap_in_scrollable_div(highlight("\n\n".join(sql_queries), SqlLexer(), formatter))
                    tabs.append((f"{key} - {dimension}", css_style + highlighted_sql))

        print("🎉 We have normalized your tables!")
        
        if len(tabs) > 0:        
            tabs = create_dropdown_with_content(tabs)
            display(tabs)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)

        display(next_button)


class ClassifyDimFact(Node):
    default_name = 'Classify Dim Fact'
    default_description = 'This classifies the dimension and fact tables'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(3, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Classifying the dimension and fact tables ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        self.table_name = table_name
        source_table = self.para["source_table"]
        con = item["con"]
        df = run_sql_return_df(con, f"SELECT * FROM {table_name}")
        doc_df = item["table_document"][source_table]


        return df, table_name

    def run(self, extract_output, use_cache=True):
        df, table_name = extract_output

        table_name = table_name.replace("_update", "")
        table_name = table_name.replace("_normalized", "")
        table_name = table_name.replace("txn_", "stg_")

        template = f"""{describe_df_in_natural_language(df=df, 
                                table_name=table_name, 
                                num_rows_to_show=2)}

Now, classify the tables:
1. Is this table about events, or matching between entities? If so, it's a fact table.
2. Is this table about entity attributes? If so, it's a dimension table.

Respond with following format:
```json
{{
    "reasoning": "...",
    "type": "fact/dimension"
    "summary": "this is for facts/dimensions of ..."
}}
```"""
        
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        for message in messages:
            write_log(message['content'])
            write_log("---------------------")

        return summary
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)

class ClassifyDimFactForAll(MultipleNode):
    default_name = 'Classify Dim Fact For All'
    default_description = 'This classifies the dimension and fact tables'

    def construct_node(self, element_name, idx=0, total=0, source_table=None):
        node = ClassifyDimFact(para={"element_name": element_name, "idx": idx, "total": total, "source_table": source_table})
        node.inherit(self)
        return node

    def extract(self, item):
        self.elements = []

        source_tables = self.global_document["Data Product"]["Group Data Source"].keys()
        self.elements = []
        table_to_source = {}

        for source_table in source_tables:
            if source_table in self.global_document["Data Product"]["Perform CRUD For All"]["Perform CRUD"]:
                table_name = source_table.replace("src", "txn") + "_update"
            else:
                table_name = source_table.replace("src", "stg")

            table_to_source[table_name] = source_table

            if table_name in self.global_document["Data Product"]["Perform Normalization For All"]["Perform Normalization"]:
                if "normalize" in self.global_document["Data Product"]["Perform Normalization For All"]["Perform Normalization"][table_name]:
                    for dimension in self.global_document["Data Product"]["Perform Normalization For All"]["Perform Normalization"][table_name]["normalize"]:
                        tables = self.global_document["Data Product"]["Perform Normalization For All"]["Perform Normalization"][table_name]["normalize"][dimension]["new_table_name"]
                        for table_name in tables:
                            table_to_source[table_name] = source_table
                            self.elements.append(table_name)
                    continue
            
            self.elements.append(table_name)

        self.nodes = {element: self.construct_node(element, idx, len(self.elements), source_table=table_to_source[element])
                        for idx, element in enumerate(self.elements)}
        
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        data = []

        for key in document["Classify Dim Fact"]:
            if document["Classify Dim Fact"][key]["type"] == "fact":
                data.append([key, "✔", "", document['Classify Dim Fact'][key]['summary']])
            else:
                data.append((key, "", "✔", document['Classify Dim Fact'][key]['summary']))

                import pandas as pd

        df = pd.DataFrame(data, columns=["Table Name", "Fact", "Dimension", "Reason"])

        print("🎉 The dimension and fact tables have been classified!")

        display(df)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)

        display(next_button)


class CreateFactTable(Node):
    default_name = 'Create Fact Table'
    default_description = 'This creates the fact table'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(3, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Creating the fact table ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        descriptions =  self.global_document["Data Product"]["Business Workflow"]["descriptions"]
        entities = set()
        relation_all = []
        nl_descs = []

        for line in descriptions:
            nl_desc, relations = line
            for relation in relations:
                entity1, action, entity2 = relation
                entities.add(entity1)
                entities.add(entity2)
                relation_all.append(relation)
            nl_descs.append(nl_desc)

        nl_descs = "\n".join(nl_descs)
        entity_desc = "\n".join(f"{idx+1}. {entity}" for idx, entity in enumerate(entities))
        relation_desc = "\n".join(f"{idx+1}. {relation}" for idx, relation in enumerate(relation_all))

        table_name = self.para["element_name"]
        con = item["con"]
        df = run_sql_return_df(con, f"SELECT * FROM {table_name}")


        return df, table_name, relation_desc

    def run(self, extract_output, use_cache=True):
        df, table_name, relation_desc = extract_output

        template = f"""You have the relations in the business Workflow:
{relation_desc}

{describe_df_in_natural_language(df=df, 
                                table_name=table_name, 
                                num_rows_to_show=2)}

This table is a fact table. 
1. First identify the relations this table directly describe. Provide its index.
2. Choose a new name for this table. The new name should begin with "fact_", followed by a concise, lowercase nouns.
Try to reuse the terms from the business workflow/table name. Don't invent new term 

Respond with following format:
```json
{{
    "reasoning": "The table is about ...",
    "related_relations": [1],
    "name": "fact_...",
    "summary": "This table is for ..."
}}
```"""
            
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        con = self.input_item["con"]
        table_name = self.para["element_name"]
        new_table_name = run_output["name"]
        sql_query = f"CREATE TABLE {new_table_name} AS SELECT * FROM {table_name}"
        run_sql_return_df(con, robust_create_to_replace(sql_query))

        input_table = table_name
        output_table = new_table_name
        dbt_formatted_query = process_query_to_dbt(sql_query, [input_table])
        dbt_project_dir = self.input_item["dbt_directory"]
        dbt_file_path = f"{dbt_project_dir}/models/model/{output_table}.sql"
        with open(dbt_file_path, "w") as f:
            f.write(dbt_formatted_query)

        callback(run_output)

class CreateFactTableForAll(MultipleNode):
    default_name = 'Create Fact Table For All'
    default_description = 'This creates the fact table'

    def construct_node(self, element_name, idx=0, total=0):
        node = CreateFactTable(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):

        self.elements = []

        for table_name in self.global_document["Data Product"]["Classify Dim Fact For All"]["Classify Dim Fact"]:
            if self.global_document["Data Product"]["Classify Dim Fact For All"]["Classify Dim Fact"][table_name]["type"] == "fact":
                self.elements.append(table_name)

        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                        for idx, element in enumerate(self.elements)}
        
        self.item = item

    def display_after_finish_workflow(self, callback, document):
        callback(document)

class CreateDimensionTable(Node):
    default_name = 'Create Dimension Table'
    default_description = 'This creates the dimension table'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(3, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Creating the dimension table ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        descriptions =  self.global_document["Data Product"]["Business Workflow"]["descriptions"]
        entities = set()
        relation_all = []
        nl_descs = []

        for line in descriptions:
            nl_desc, relations = line
            for relation in relations:
                entity1, action, entity2 = relation
                entities.add(entity1)
                entities.add(entity2)
                relation_all.append(relation)
            nl_descs.append(nl_desc)

        nl_descs = "\n".join(nl_descs)
        entity_desc = "\n".join(f"{idx+1}. {entity}" for idx, entity in enumerate(entities))
        relation_desc = "\n".join(f"{idx+1}. {relation}" for idx, relation in enumerate(relation_all))

        table_name = self.para["element_name"]
        con = item["con"]
        df = run_sql_return_df(con, f"SELECT * FROM {table_name}")

        return df, table_name, nl_descs

    def run(self, extract_output, use_cache=True):
        df, table_name, nl_descs = extract_output

        template = template = f"""You have the business Workflow:
{nl_descs}


{describe_df_in_natural_language(df=df, 
                                table_name=table_name, 
                                num_rows_to_show=2)}

This table is a dimension table. 
First, identify the primary key for this table. 
Then, choose a new name for this table. The new name should begin with "dim_", followed by a concise, lowercase nouns.
Try to reuse the terms from the business workflow/table name. Don't invent new term 

Respond with following format:
```json
{{
    "reasoning": "The table is about ... XXX could be used to uniquely identify ...",
    "primary_key": ["col", ...],
    "name": "dim_...",
    "summary": "This table is for ..."
}}
```"""
                
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)        

        for message in messages:
            write_log(message['content'])
            write_log("---------------------")

        return summary
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        con = self.input_item["con"]
        table_name = self.para["element_name"]
        new_table_name = run_output["name"]
        sql_query = f"CREATE TABLE {new_table_name} AS SELECT * FROM {table_name}"
        run_sql_return_df(con, robust_create_to_replace(sql_query))

        input_table = table_name
        output_table = new_table_name
        dbt_formatted_query = process_query_to_dbt(sql_query, [input_table])
        dbt_project_dir = self.input_item["dbt_directory"]
        dbt_file_path = f"{dbt_project_dir}/models/model/{output_table}.sql"
        with open(dbt_file_path, "w") as f:
            f.write(dbt_formatted_query)

        callback(run_output)

class CreateDimensionTableForAll(MultipleNode):
    default_name = 'Create Dimension Table For All'
    default_description = 'This creates the dimension table'

    def construct_node(self, element_name, idx=0, total=0):
        node = CreateDimensionTable(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):

        self.elements = []

        for table_name in self.global_document["Data Product"]["Classify Dim Fact For All"]["Classify Dim Fact"]:
            if self.global_document["Data Product"]["Classify Dim Fact For All"]["Classify Dim Fact"][table_name]["type"] == "dimension":
                self.elements.append(table_name)

        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                        for idx, element in enumerate(self.elements)}
        
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        fact_document = self.global_document["Data Product"]["Create Fact Table For All"]
        num_fact  = len(fact_document["Create Fact Table"])
        print(f"🎉 The following {num_fact} fact tables have been created!")

        for idx, key in enumerate(fact_document["Create Fact Table"]):
            print(f"{idx+1}. {BOLD}{fact_document['Create Fact Table'][key]['name']}{END} (from {BOLD}{key}{END})")
            print(f"   {ITALIC}{fact_document['Create Fact Table'][key]['summary']}{END}")

        print("")
        num_dim  = len(document["Create Dimension Table"])
        print(f"🎉 The following {num_dim} dimension tables have been created!")

        for idx, key in enumerate(document["Create Dimension Table"]):
            print(f"{idx+1}. {BOLD}{document['Create Dimension Table'][key]['name']}{END} (from {BOLD}{key}{END})")
            print(f"   {ITALIC}{document['Create Dimension Table'][key]['summary']}{END}")
        
        
        title = self.global_document["Data Product"]["Business Workflow"]["title"]
        descriptions = self.global_document["Data Product"]["Business Workflow"]["descriptions"]

        descriptions_size = [len(desc[1]) for desc in descriptions]
        descriptions_size_array = np.array(descriptions_size)
        descriptions_size_max = descriptions_size_array.cumsum()
        descriptions_size_max

        table_to_summary_map = {}
        new_table_name = {}

        def find_index(value, max_sizes):
            for i, size in enumerate(max_sizes):
                if value <= size:
                    return i
            return len(max_sizes) - 1

        related_tables = [set() for _ in range(len(descriptions))]

        for table in fact_document["Create Fact Table"]:
            table_to_summary_map[table] = fact_document["Create Fact Table"][table]["summary"]
            new_table_name[table] = fact_document["Create Fact Table"][table]["name"]

            for idx in fact_document["Create Fact Table"][table]["related_relations"]:
                i = find_index(idx, descriptions_size_max)
                related_tables[i].add(table)

        new_descriptions = []

        for i, desc in enumerate(descriptions):
            text, list_r = desc 
            related_table_set = related_tables[i]
            
            text += "<ul>"
            for table in related_table_set:
                new_name = new_table_name[table]
                text += f"<li><i>{new_name}</i></li>"
            text += "</ul>"
            new_descriptions.append((text, list_r))

        print("")
        print("🧐 The fact tables are related to the business workflow as follows ...")

        display_pages(total_page=len(new_descriptions), 
                create_html_content=partial(create_html_content_project, 
                                            title, new_descriptions))

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(document)
        
        next_button.on_click(on_button_click)
        display(next_button)

class ProductDocument(Node):
    default_name = 'Product Document'
    default_description = 'This step documents each product table'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        con = self.input_item["con"]
        table_name = self.para["element_name"]
        df = run_sql_return_df(con, f"select * from {table_name}")
        doc_df = DocumentedData(df, table_name=table_name)
        doc_df.rename_table=False
        doc_df.start(viewer=True)
        if "product_document" not in self.input_item:
            self.input_item["product_document"] = {}
        self.input_item["product_document"][table_name] = doc_df
        callback(doc_df.document)


class ProductDocumentForAll(MultipleNode):
    
    default_name = 'Product Document For All'
    default_description = 'This step documents all product tables'

    def construct_node(self, element_name, idx=0, total=0):
        node = ProductDocument(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):
        fact_names = []
        for table_name in self.global_document["Data Product"]["Create Fact Table For All"]["Create Fact Table"]:
            fact_name = self.global_document["Data Product"]["Create Fact Table For All"]["Create Fact Table"][table_name]["name"]
            fact_names.append(fact_name)
        

        dimension_tables = []
        for table_name in self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"]:
            dim_name = self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"][table_name]["name"]
            dimension_tables.append(dim_name)
        
        self.elements = fact_names + dimension_tables

        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item
    
    def display_after_finish_workflow(self, callback, document):

        dropdown = widgets.Dropdown(
            options=document["Product Document"].keys(),
        )

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                callback(document)
        
        next_button.on_click(on_button_click)

        def on_change(change):
            with self.output_context():
                if change['type'] == 'change' and change['name'] == 'value':
                    clear_output(wait=True)

                    print("🎉 All the fact and dimension tables have been documented!")
                    
                    print("😎 Next we will build the join graph ...")
                    display(next_button)
                    
                    print("🧐 You can explore the documents below ...")
                    display(dropdown)
                    self.item["product_document"][change['new']].display_document()

        dropdown.observe(on_change)

        clear_output(wait=True)
        
        print("🎉 All the fact and dimension tables have been documented!")
        
        print("😎 Next we will build the join graph ...")
        display(next_button)
        
        print("🧐 You can explore the documents for the product tables below ...")
        display(dropdown)
        if len(document["Product Document"]) > 0:
            self.item["product_document"][dropdown.value].display_document()

class CreateFactMatchDim(Node):
    default_name = 'Create Fact Match Dim'
    default_description = 'This creates the fact match dim'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(3, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Creating the fact match dim ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        con = item["con"]

        fact_name = self.global_document["Data Product"]["Create Fact Table For All"]["Create Fact Table"][table_name]["name"]
        summary = self.global_document["Data Product"]["Create Fact Table For All"]["Create Fact Table"][table_name]["summary"]

        dimension_tables = []
        for table_name in self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"]:
            dim_name = self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"][table_name]["name"]
            table_key = self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"][table_name]["primary_key"]
            dimension_tables.append((dim_name, table_key))

        dim_desc = ""
        for idx, (dim_name, table_key) in enumerate(dimension_tables):
            dim_desc += f"{idx+1}. {dim_name}: {table_key}\n"

        df = run_sql_return_df(con, f"SELECT * FROM {fact_name}")

        return df, fact_name, dim_desc, summary

    def run(self, extract_output, use_cache=True):
        df, table_name, dim_desc, fact_summary = extract_output

        template = template = f"""You have the following fact table:
{describe_df_in_natural_language(df=df, 
                                table_name=table_name, 
                                num_rows_to_show=2)}
This table is a fact table. {fact_summary}

You have the following dimension tables with their keys
{dim_desc}

First go through each attribute, especially the foreign keys. If they are abbreviation, guess the full name.
Then, find the corresponding dimension table. 

Respond with following format:
```json
{{
    "reasoning": "The table is about ... X column means, which matches Y dimension ...",
    "dimensons": ["dim_table_name" ...]
}}
```"""
                    
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)

class CreateFactMatchDimForAll(MultipleNode):
    default_name = 'Create Fact Match Dim For All'
    default_description = 'This creates the fact match dim'

    def construct_node(self, element_name, idx=0, total=0):
        node = CreateFactMatchDim(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):

        self.elements = list(self.global_document["Data Product"]["Create Fact Table For All"]["Create Fact Table"].keys())

        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                        for idx, element in enumerate(self.elements)}
        
        self.item = item

    def display_after_finish_workflow(self, callback, document):
        callback(document)


        



class MatchDimensions(Node):
    default_name = 'Match Dimensions'
    default_description = 'This matches the dimensions'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(3, self.global_document["Data Product"]['Product Steps'])

        print("🤓 Matching the dimensions ...")
        self.progress = show_progress()

        dimension_tables = []
        for table_name in self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"]:
            dim_name = self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"][table_name]["name"]
            table_key = self.global_document["Data Product"]["Create Dimension Table For All"]["Create Dimension Table"][table_name]["primary_key"]
            dimension_tables.append((dim_name, table_key))

        dim_desc = ""
        for idx, (dim_name, table_key) in enumerate(dimension_tables):
            dim_desc += f"{idx+1}. {dim_name}: {table_key}\n"

        return  dim_desc

    def run(self, extract_output, use_cache=True):
        dim_desc = extract_output

        template = f"""You have the following dimension tables with their keys
{dim_desc}

First go through each table and guess what they mean.
Then, find the pair of dimension tables that are joinable, e.g., because they are hierarchical and have the keys joinable.
E.g., dim_state(state_id, city_id) and dim_city(city_id)
Sometimes the pair may be joinable. Classify the confidence into "definite", and "maybe".

Respond with following format:
```json
{{
    "reasoning": "The dimensions are  ... X and Y and joinable pair",
    "definite_pairs": [["dim1", "dim2"] ... ],
    "maybe_pairs": [...]
}}
```"""

        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)


class BuildDataMart(Node):
    default_name = 'Build Data Mart'
    default_description = 'This builds the data mart'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(3, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Building the data mart ...")

        idx = self.para["idx"]
        total = self.para["total"]
        self.progress = show_progress(max_value=total, value=idx)

        table_name = self.para["element_name"]
        con = item["con"]

        fact_name = self.global_document["Data Product"]["Create Fact Table For All"]["Create Fact Table"][table_name]["name"]
        self.fact_name = fact_name

        def get_table_description(con, table_name, row=2):
            df = run_sql_return_df(con, f"SELECT * FROM {table_name}")
            return describe_df_in_natural_language(df, table_name, row)

        fact_table_desc = get_table_description(con, fact_name , 2)

        dimensions = self.global_document["Data Product"]["Create Fact Match Dim For All"]["Create Fact Match Dim"][table_name]["dimensions"]
        
        self.dimensions = dimensions

        dim_table_desc = ""
        for dim_name in dimensions:
            dim_table_desc += get_table_description(con, dim_name, 2)

        join_conditions = "\nAND ".join([f"JOIN {dim_name} ON (...)" for dim_name in dimensions])

        return fact_table_desc, dim_table_desc, join_conditions, fact_name

    def run(self, extract_output, use_cache=True):
        fact_table_desc, dim_table_desc, join_conditions, fact_name = extract_output

        template = f"""You have the following fact table:
{fact_table_desc}

The fact table joins with the following dimension tables:
{dim_table_desc}

Now, write the sql query to join the tables.

Respond with following format:
```json
{{
    "reasoning": "The table is about ... To join with the dimension tables ...",
    "sql": "SELECT * FROM {fact_name} 
{join_conditions}
"
}}
```"""

        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        con = self.input_item["con"]
        fact_name = self.fact_name.replace("fact_", "dm_")
        sql_query = run_output["sql"]
        sql_query = f"CREATE TABLE {fact_name} AS\n {sql_query}"
        run_output["sql"] = sql_query

        input_table = [self.fact_name] + self.dimensions
        output_table = fact_name
        dbt_formatted_query = process_query_to_dbt(sql_query, input_table)
        dbt_project_dir = self.input_item["dbt_directory"]
        dbt_file_path = f"{dbt_project_dir}/models/mart/{output_table}.sql"
        with open(dbt_file_path, "w") as f:
            f.write(dbt_formatted_query)


        callback(run_output)


class SourceToBusiness(Node):
    default_name = 'Source Business'
    default_description = 'Analyze how each source table is related to the business workflow'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(1, self.global_document["Data Product"]['Product Steps'])
        print("🤓 Analyzing the high-level purpose of source tables ...")

        con = item["con"]
        descriptions = self.global_document["Data Product"]["Business Workflow"]["descriptions"]
    
        def get_nl_entity_rel_desc(descriptions):
            entities = set()
            relation_all = []
            nl_descs = []

            for line in descriptions:
                nl_desc, relations = line
                for relation in relations:
                    entity1, action, entity2 = relation
                    entities.add(entity1)
                    entities.add(entity2)
                    relation_all.append(relation)
                nl_descs.append(nl_desc)

            nl_descs = "\n".join(nl_descs)
            entity_desc = "\n".join(f"{idx+1}. {entity}" for idx, entity in enumerate(entities))
            relation_desc = "\n".join(f"{idx+1}. {relation}" for idx, relation in enumerate(relation_all))
            return nl_descs, entity_desc, relation_desc

        nl_descs, entity_desc, relation_desc = get_nl_entity_rel_desc(descriptions)

        num_tables = len(self.global_document["Data Product"]["Group Data Source"])
        table_desc = f"You have the following {num_tables} tables: \n\n"
        for table_name in self.global_document["Data Product"]["Group Data Source"]:
            df = run_sql_return_df(con, f"SELECT * FROM {table_name}")
            table_desc += describe_df_in_natural_language(df=df, 
                                        table_name=table_name, 
                                        num_rows_to_show=2)
            table_desc += "\n\n"

        return nl_descs, table_desc

    def run(self, extract_output, use_cache=True):
        nl_descs, table_desc = extract_output

        template = f"""You have the following business workflow: {nl_descs}.

{table_desc}

Summarize how each table is related to the business workflow.
Return the result in yml
```yml
- table_name: xxx
  description: short simple desc in < 10 words.
...
```"""

        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)

        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output

        df = pd.DataFrame(summary)

        print("🎉 Here are the understanding results")

        display(df)

        print("🧐 Is the understanding correct?")
        yes_button = widgets.Button(description="Yes", button_style='success')
        no_button = widgets.Button(description="No", button_style='danger')
        display(HBox([yes_button, no_button]))

        print("🤓 Next we will dive deep into each table ...")

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback(summary)

        next_button.on_click(on_button_click)

        display(next_button)
        

class BuildDataMartForAll(MultipleNode):
    default_name = 'Build Data Mart For All'
    default_description = 'This builds the data mart'

    def construct_node(self, element_name, idx=0, total=0):
        node = BuildDataMart(para={"element_name": element_name, "idx": idx, "total": total})
        node.inherit(self)
        return node

    def extract(self, item):

        self.elements = []

        for fact_name in self.global_document["Data Product"]["Create Fact Match Dim For All"]["Create Fact Match Dim"]:
            dimensions = self.global_document["Data Product"]["Create Fact Match Dim For All"]["Create Fact Match Dim"][fact_name]["dimensions"]
            if len(dimensions) > 0:
                self.elements.append(fact_name)

        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                        for idx, element in enumerate(self.elements)}
        
        self.item = item

    def display_after_finish_workflow(self, callback, document):
        
        match_document = self.global_document["Data Product"]["Match Dimensions"]
        edges = match_document["definite_pairs"]

        for table_name in self.global_document["Data Product"]["Create Fact Match Dim For All"]["Create Fact Match Dim"]:
            fact_name = self.global_document["Data Product"]["Create Fact Table For All"]["Create Fact Table"][table_name]["name"]
            dimensions = self.global_document["Data Product"]["Create Fact Match Dim For All"]["Create Fact Match Dim"][table_name]["dimensions"]
            edges += [[fact_name, dim] for dim in dimensions]

        nodes = list({node for edge in edges for node in edge})

        data = {
            "nodes": [{"id": node} for node in nodes],
            "links": [{"source": edge[0], "target": edge[1]} for edge in edges]
        }
        print("🎉 The join graph has been constructed!")
        html_content = generate_draggable_graph_html(data)
        display(HTML(wrap_in_iframe(html_content, width=1000)))



        tab_data = []
        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"

        for key in document["Build Data Mart"]:
            sql_query = document["Build Data Mart"][key]["sql"]
            highlighted_sql = wrap_in_scrollable_div(highlight(sql_query, SqlLexer(), formatter))
            tab_data.append((key.replace("stg", "dm"), css_style + highlighted_sql))

        tabs = create_dropdown_with_content(tab_data)  

        num_fact  = len(document["Build Data Mart"])
        print(f"🎉 The following are the denormalized data marts!")
        display(tabs)

        print(f"Congratulations! You've successfully created the data products, ready for dashboards, ad hoc queries, or LLM applications. 🌟")
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')\
    
        def on_button_click(b):
            with self.output_context():
                callback(document)

        next_button.on_click(on_button_click)
        display(next_button)



def create_data_product_workflow(dbt_directory, output=False):

    item = {
        "con": duckdb.connect(database=':memory:'),
        "dir": dbt_directory + "/seeds",
        "dbt_directory": dbt_directory,
    }
    if output:
        output = Output()
    else:
        output = None

    main_workflow = Workflow("Data Product", 
                            item = item, 
                            description="A workflow to transform raw data into data product",
                            output=output)

    main_workflow.add_to_leaf(ProductSteps(output=output))
    main_workflow.add_to_leaf(DataSource(output=output))
    main_workflow.add_to_leaf(BusinessWorkflow(output=output))
    main_workflow.add_to_leaf(GroupDataSource(output=output))
    main_workflow.add_to_leaf(SourceCodeWritingForAll(output=output))
    main_workflow.add_to_leaf(SourceToBusiness(output=output))
    main_workflow.add_to_leaf(SourceDocumentForAll(output=output))
    main_workflow.add_to_leaf(BasicStageForAll(output=output))
    main_workflow.add_to_leaf(DecideCRUDForAll(output=output))
    main_workflow.add_to_leaf(PerformCRUDForAll(output=output))
    main_workflow.add_to_leaf(DecideForeignKeyForAll(output=output))
    main_workflow.add_to_leaf(PerformNormalizationForAll(output=output))
    main_workflow.add_to_leaf(ClassifyDimFactForAll(output=output))
    main_workflow.add_to_leaf(CreateFactTableForAll(output=output))
    main_workflow.add_to_leaf(CreateDimensionTableForAll(output=output))
    main_workflow.add_to_leaf(ProductDocumentForAll(output=output))
    main_workflow.add_to_leaf(CreateFactMatchDimForAll(output=output))
    main_workflow.add_to_leaf(MatchDimensions(output=output))
    main_workflow.add_to_leaf(BuildDataMartForAll(output=output))
    return main_workflow


def get_dbt_model_info(sql_file_path):
    model_name = os.path.splitext(os.path.basename(sql_file_path))[0]
    referenced_tables = []
    referenced_sources = []
    used_variables = []
    used_macros = []

    try:
        with open(sql_file_path, 'r') as file:
            content = file.read()

            config_match = re.search(r"\{\{\s*config\([^)]*alias=['\"]([^'\"]+)['\"]", content)
            if config_match:
                model_name = config_match.group(1)

            blocks = re.findall(r"\{\{(.+?)\}\}", content, re.DOTALL)

            for block in blocks:
                referenced_tables.extend(re.findall(r"ref\(['\"]([^'\"]+)['\"]\)", block))

                referenced_sources.extend(re.sub(r"\s*,\s*", ",", match) for match in re.findall(r"source\(.*?\)", block))

                referenced_sources.extend(re.findall(r"unpack_jotform\(.*?\)", block))

                used_variables.extend(re.findall(r"var\(['\"]([^'\"]+)['\"]\)", block))

                used_macros.extend(re.findall(r"([a-zA-Z0-9_]+)\(", block))

    except IOError:
        print(f"Error: File {sql_file_path} not accessible.")
        return None, None, None, None, None

    used_macros = list(set(used_macros) - {'config', 'ref', 'source', 'var'})

    referenced_tables = list(set(referenced_tables))
    referenced_sources = list(set(referenced_sources))
    used_variables = list(set(used_variables))
    used_macros = list(set(used_macros))

    return model_name, referenced_tables, referenced_sources, used_variables, used_macros



class DbtTable:
    def __init__(self):
        self.name = None
        self.description = None
        self.columns = []
        self.tests = []
        self.meta = {}
        self.config = {}
        self.sql_query = None
        self.referenced_models = []
        self.referenced_sources = []
        self.used_variables = []
        self.used_macros = []
        self.source = None 
        self.tags = []
        self.tag_results = {}
        self.source_purpose = None
        self.cocoon_tags = {}
    
    def add_yaml_info(self, table_data):
        self.name = table_data.get('name')
        self.description = table_data.get('description')
        self.columns = table_data.get('columns', [])
        self.tests = table_data.get('tests', [])
        self.meta = table_data.get('meta', {})
        self.config = table_data.get('config', {})
        self.cocoon_tags = table_data.get('cocoon_tags', {})

    def add_tag_result(self, tag):
        self.tag_results = tag
        for key, value in tag.items():
            if value["Label"] == True:
                self.tags.append(key)    

    def add_source_purpose(self, purpose):
        self.source_purpose = purpose
                
    def add_sql_info(self, sql_file_path):

        with open(sql_file_path, 'r') as file:
            sql_content = file.read()
            self.sql_query = sql_content

        model_name, referenced_tables, referenced_sources, used_variables, used_macros = get_dbt_model_info(sql_file_path)
        self.name = model_name
        self.referenced_models = referenced_tables
        self.referenced_sources = referenced_sources
        self.used_variables = used_variables
        self.used_macros = used_macros

    def get_name(self):
        if self.source:
            return f"source('{self.source.name}','{self.name}')"
        else:
            return self.name

    def get_tagged_name(self):
        tags_formatted = ''
        for tag in self.tags:
            tags_formatted += f"\n{tag}"
        for tag in self.cocoon_tags:
            tags_formatted += f"\n{tag}"
            
        return f"{self.name}{tags_formatted}"

    def add_source(self, source):
        if not isinstance(source, DbtSource):
            raise ValueError("The source must be a DbtSource instance.")
        self.source = source

    def get_source(self):

        if "partition" in self.cocoon_tags:
            group = self.cocoon_tags["partition"]
            return [f"{self.name} {len(group)} partitions"]
        return self.referenced_models + self.referenced_sources

    def __repr__(self):
        source_info = f"Source: '{self.source.name}'" if self.source else "No source associated"

        repr_str = f"DbtTable '{self.name}':\n"
        repr_str += f"  Description: {self.description or 'Not provided'}.\n"
        
        if self.columns:
            column_names = [col.get('name', 'Unnamed') for col in self.columns]
            repr_str += f"  Columns: {', '.join(column_names)}.\n"
        else:
            repr_str += "  Columns: None.\n"

        repr_str += f"  Tests: {len(self.tests)} defined.\n"
        repr_str += f"  Metadata: {'Provided' if self.meta else 'Not available'}.\n"
        repr_str += f"  Configuration: {'Customized' if self.config else 'Default'}.\n"
        repr_str += f"  SQL Query: {'Included' if self.sql_query else 'Not available'}.\n"
        repr_str += f"  Referenced Models: {', '.join(self.referenced_models) or 'None'}.\n"
        repr_str += f"  Referenced Sources: {', '.join(self.referenced_sources) if self.referenced_sources else 'None'}.\n"
        repr_str += f"  Used Variables: {', '.join(self.used_variables) if self.used_variables else 'None'}.\n"
        repr_str += f"  Used Macros: {', '.join(self.used_macros) if self.used_macros else 'None'}.\n"
        repr_str += f"  {source_info}\n"

        return repr_str

    def to_html(self):
        css = HtmlFormatter().get_style_defs('.highlight')
        html_content = f"<style>{css}</style>"

        label_emoji_map = {
            'Filtering': '🔍 Filtering',
            'Cleaning': '🧼 Cleaning',
            'Featurization': '⚙️ Featurization',
            'Integration': '🤝 Integration',
            'Other': '❓ Other'
        }


        if self.tag_results:
            html_content += "<h3>Tag Results:</h3>"
            html_content += "<ul>"
            for key, value in self.tag_results.items():
                if value['Label'] == True:
                    html_content += f"<li><b>{label_emoji_map[key]}</b>: {value['Reasoning']}</p></li>"
            html_content += "</ul>"

        if self.source_purpose:
            html_content += f"<h3>Source Purpose:</h3>"
            html_content += f"<ul>"
            for key, value in self.source_purpose.items():
                html_content += f"<li><b>{key}</b>: {value}</p></li>"
            html_content += "</ul>"

        html_content += f"<h2>{self.name} YML description</h2>"
        html_content += f"<p><b>Description:</b> {self.description or 'Not provided'}.</p>"
        if self.columns:
            column_names = [col.get('name', 'Unnamed') for col in self.columns]
            html_content += f"<p><b>Columns:</b> {', '.join(column_names)}.</p>"
        else:
            html_content += "<p><b>Columns:</b> Not available.</p>"

        if self.sql_query:
            html_content += wrap_in_scrollable_div(highlight(self.sql_query, SqlLexer(), HtmlFormatter()))
        else:
            html_content += "<p>No SQL query available.</p>"
        
        return html_content


    def display(self):
        html_content = self.to_html()
        display(HTML(html_content))
    
    def merge_with(self, other_table):

        if not isinstance(other_table, DbtTable):
            raise ValueError("Can only merge with another DbtTable.")

        self.name = self.name or other_table.name
        self.description = self.description or other_table.description
        self.sql_query = self.sql_query or other_table.sql_query

        self.columns = self.columns or other_table.columns
        self.tests += [test for test in other_table.tests if test not in self.tests]
        self.referenced_models += [rm for rm in other_table.referenced_models if rm not in self.referenced_models]
        self.referenced_sources += [rs for rs in other_table.referenced_sources if rs not in self.referenced_sources]
        self.used_variables += [var for var in other_table.used_variables if var not in self.used_variables]
        self.used_macros += [macro for macro in other_table.used_macros if macro not in self.used_macros]

        self.meta = {**other_table.meta, **{k: v for k, v in self.meta.items() if v is not None}}
        self.config = {**other_table.config, **{k: v for k, v in self.config.items() if v is not None}}
        self.cocoon_tags = {**other_table.cocoon_tags, **{k: v for k, v in self.cocoon_tags.items()}}

class DbtMetric:
    def __init__(self, metric_data):
        self.name = metric_data.get('name')
        self.description = metric_data.get('description')
        self.model = metric_data.get('model')
        self.expression = metric_data.get('expression')

        self.additional_attributes = {key: value for key, value in metric_data.items() 
                                      if key not in ['name', 'description', 'model', 'expression']}

    def get_name(self):
        """Return the name of the metric."""
        return self.name

    def __repr__(self):
        return (f"DbtMetric '{self.name}' based on the model '{self.model}', "
                f"using the expression '{self.expression}'.")

class DbtSource:
    def __init__(self, source_data):
        self.name = source_data.get('name')
        self.description = source_data.get('description')
        self.tables = []

        for table_data in source_data.get('tables', []):
            table = DbtTable()
            table.add_yaml_info(table_data)
            table.add_source(self)
            self.tables.append(table)

    def get_name(self):
        """Return the name of the source."""
        return self.name

    def __repr__(self):
        return (f"DbtSource named '{self.name}' with "
                f"{len(self.tables)} associated tables: " +
                ", ".join(table.name for table in self.tables if table.name))

class DbtSchema:
    def __init__(self, file_path):
        self.file_path = file_path
        self.version = None
        self.tables = {}
        self.metrics = {}
        self.sources = {}

        self._parse_file()

    def _parse_file(self):
        with open(self.file_path, 'r') as file:
            data = yaml.safe_load(file)

        self.version = data.get('version')

        for table_data in data.get('models', []):
            table = DbtTable()
            table.add_yaml_info(table_data)
            self.tables[table.get_name()] = table

        for source_data in data.get('sources', []):
            source = DbtSource(source_data)
            self.sources[source.get_name()] = source
            for table_data in source_data.get('tables', []):
                table = DbtTable()
                table.add_yaml_info(table_data)
                table.add_source(source)
                self.tables[table.get_name()] = table

        for metric_data in data.get('metrics', []):
            metric = DbtMetric(metric_data)
            self.metrics[metric.get_name()] = metric

    def add_to_project(self, project):
        if not isinstance(project, DbtProject):
            raise ValueError("project must be an instance of DbtProject")

        for table in self.tables.values():
            project.add_table(table)

        for metric in self.metrics.values():
            if metric.get_name() in project.metrics:
                raise ValueError(f"Metric '{metric.get_name()}' already exists in the project.")
            project.add_metric(metric)

        for source in self.sources.values():
            if source.get_name() in project.sources:
                raise ValueError(f"Source '{source.get_name()}' already exists in the project.")
            project.add_source(source)

    def __repr__(self):
        return (f"DbtSchema at '{self.file_path}' with version {self.version}, "
                f"containing {len(self.tables)} tables, "
                f"{len(self.metrics)} metrics, and "
                f"{len(self.sources)} sources.")


class DbtProject:
    def __init__(self):
        self.tables = {}
        self.metrics = {}
        self.sources = {}
        self.output = None
    
    @contextmanager
    def output_context(self):
        if hasattr(self, 'output') and self.output is not None:
            with self.output:
                yield
        else:
            yield

    def add_table(self, table):
        if table.name in self.tables:
            self.tables[table.name].merge_with(table)
        else:
            self.tables[table.name] = table

    def add_metric(self, metric):
        if metric.name in self.metrics:
            raise ValueError(f"A metric with the name '{metric.name}' already exists.")
        self.metrics[metric.name] = metric

    def add_source(self, source):
        if source.name in self.sources:
            raise ValueError(f"A source with the name '{source.name}' already exists.")
        self.sources[source.name] = source

    def __repr__(self):
        tables_repr = ', '.join(self.tables.keys())
        metrics_repr = ', '.join(self.metrics.keys())
        sources_repr = ', '.join(self.sources.keys())

        return (f"DbtProject:\n"
                f"  Tables: {tables_repr or 'None'}\n"
                f"  Metrics: {metrics_repr or 'None'}\n"
                f"  Sources: {sources_repr or 'None'}")

    def process_dbt_files(self, file_path):
        for root, dirs, files in os.walk(file_path):
            for file in files:
                
                full_path = os.path.join(root, file)


                if file.endswith('.yml') or file.endswith('.yaml'):
                    with open(full_path, 'r') as yml_file:
                        schema_data = yaml.safe_load(yml_file)
                        schema = DbtSchema(full_path)
                        schema.add_to_project(self)

                elif file.endswith('.sql'):
                    table = DbtTable()
                    table.add_sql_info(full_path)
                    self.add_table(table)

        nodes, edges = self.build_graph_from_project()
        self.nodes = nodes
        self.edges = edges

        tagged_names = []

        instance_names= []
        instances = []

        idx = 1

        for node in self.nodes:
            if node in self.tables:
                tagged_name = self.tables[node].get_tagged_name()
                tagged_names.append(str(idx) + ". " + tagged_name)
                instance_names.append(str(idx) + ". " + tagged_name)
                instances.append(self.tables[node])
                idx += 1
                
            else:
                tagged_name = node
                tagged_names.append(tagged_name)
  

        self.tagged_names = tagged_names
        self.instances = instances
        self.instance_names = instance_names

    def update_tagged_names(self):
        tagged_names = []

        idx = 1

        for node in self.nodes:
            if node in self.tables:
                tagged_name = self.tables[node].get_tagged_name()
                tagged_names.append(str(idx) + ". " + tagged_name)
                idx += 1
            else:
                tagged_name = node
                tagged_names.append(tagged_name)

        self.tagged_names = tagged_names
        
    def display_workflow(self):
        display_workflow(self.tagged_names, self.edges, height=400)
    
    def display(self, call_back=None):

        if call_back is None:
            call_back = self.display

        def create_widget(nodes, instances):
            
            dropdown = widgets.Dropdown(
                options=nodes,
                disabled=False,
            )

            button1 = widgets.Button(description="View")

            button3 = widgets.Button(description="Return")

            def on_button_clicked3(b):
                with self.output_context():
                    clear_output(wait=True)
                    call_back()

            def on_button_clicked(b):
                with self.output_context():




                    idx = nodes.index(dropdown.value)

                    selected_instance = instances[idx]
                    html_content = selected_instance.to_html()
                    html_widget.value = html_content

            button1.on_click(on_button_clicked)

            self.display_workflow()

            buttons = widgets.HBox([button1])

            html_widget = widgets.HTML(
                value="",
                placeholder='',
                description='',
            )

            display(dropdown, buttons, html_widget)


        create_widget(self.instance_names, self.instances)
        

    def build_graph_from_project(self):
        nodes_set = set()

        for table in self.tables.values():
            nodes_set.add(table.get_name())

            for source in table.get_source():
                nodes_set.add(source)

        nodes = list(nodes_set)

        edges = {}
        for idx, node in enumerate(nodes):
            table = self.tables.get(node)
            if table:


                for ref_source in table.get_source():
                    if ref_source in nodes_set:
                        ref_idx = nodes.index(ref_source)
                        if ref_idx not in edges:
                            edges[ref_idx] = []
                        edges[ref_idx].append(idx)
                    else:
                        raise ValueError(f"Source '{ref_source}' not found in the project.")

        return nodes, edges






def find_nodes_with_no_parents(nodes, edges):
    destination_indices = set()

    for destinations in edges.values():
        destination_indices.update(destinations)

    no_parent_nodes = [node for idx, node in enumerate(nodes) if idx not in destination_indices]

    return no_parent_nodes






class SQLStep(TransformationStep):
    
    def __init__(self, table_name, con, sql_code= "", database=None, schema=None, materialized = False, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.explanation = ""
        self.name = table_name
        self.sql_code = sql_code
        self.schema = None
        self.distinct_count = None
        self.con = con
        self.database = database
        self.schema = schema
        self.materialized = materialized
    
    def set_materialized(self, materialized=True):
        self.materialized = materialized
        
    def __repr__(self, full=True):
        if not full:
            return self.name
        
        con = self.con
        if self.database is not None and self.schema is not None:
            return f'{enclose_table_name(self.database, con=con)}.{enclose_table_name(self.schema, con=con)}.{enclose_table_name(self.name, con=con)}'
        elif self.schema is not None:
            return f'{enclose_table_name(self.schema, con=con)}.{enclose_table_name(self.name, con=con)}'
        else:
            return f'{enclose_table_name(self.name, con=con)}'

    def generate_codes(self):
        pass

    def get_schema(self, con=None):
        if con is None:
            con = self.con

        return get_table_schema(con, table_name=self.name, schema=self.schema, database=self.database)

    def get_distinct_count(self):
        if self.distinct_count is None:
            distinct_count = {}
            schema = self.get_schema()
            for col in schema:
                distinct_count[col] = count_total_distinct(self.con, self.name, col)
            self.distinct_count = distinct_count
        return self.distinct_count

    def remove_from_database(self, con=None):
        if con is None:
            con = self.con
            
        views = list_views(con, schema=self.schema, database=self.database)
        if self.name in views:
            remove_view(con, view_name=self.name, schema_name=self.schema, database_name=self.database)
            
        tables = list_tables(con, schema=self.schema, database=self.database)
        if self.name in tables:
            remove_table(con, table_name=self.name, schema_name=self.schema, database_name=self.database)
            
    def run_codes(self, mode="VIEW"):
        sql_code = self.get_codes(mode=mode)
        run_sql_return_df(self.con, sql_code)

    def display(self):
        print(self.name)
        print(self.sql_code)

    def edit_widget(self, callbackfunc=None):
        self.display()
        print("\033[91mEdit Source File is under development!\033[0m")

        return_button = widgets.Button(description="Return")

        def on_return_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                callbackfunc(self)
        
        return_button.on_click(on_return_clicked)

        display(return_button)
    
        
    def get_sql_code(self, source_steps=None, full=None):

        if isinstance(self.sql_code, str):
            return self.sql_code
        
        if callable(self.sql_code):
            con = self.con
            if source_steps is None:
                source_steps = []
            
            formatted_steps = []
            for step in source_steps:
                if full is not None:
                    formatted_step = str(step.__repr__(full=full))
                elif hasattr(step, 'materialized') and step.materialized:
                    formatted_step = str(step.__repr__(full=True))
                else:
                    formatted_step = str(step.__repr__(full=False))
                formatted_steps.append(enclose_table_name(formatted_step, con=con))
            
            return self.sql_code(*formatted_steps)
        
        raise TypeError("sql_code must be either a string or a callable")
    
    def get_codes(self, target_id=0, source_ids=None, source_steps=None, mode="", full=None):
        con = self.con
        
        if not self.sql_code:
            return ""
        
        if source_ids is None:
            source_ids = []
            
        modes = ["", "TABLE", "VIEW", "AS"]

        if mode not in modes:
            raise ValueError("Mode not supported")

        if mode == "":
            return self.get_sql_code(source_steps, full=full)
        
        table_name = enclose_table_name(self.name, con=con)
        
        if mode == "TABLE":
            return f"CREATE OR REPLACE TABLE {table_name} AS\n{indent_paragraph(self.get_sql_code(source_steps, full=full))}"

        if mode == "VIEW":
            return f"CREATE OR REPLACE VIEW {table_name} AS\n{indent_paragraph(self.get_sql_code(source_steps, full=full))}"
        
        if mode == "AS":
            return f"{table_name} AS (\n{indent_paragraph(self.get_sql_code(source_steps, full=full))}\n)"

class TransformationSQLPipeline(TransformationPipeline):
    
    def get_codes_for_step(self, step_idx, mode="", full=None):
        source_step_indicies = self.get_incoming_steps(step_idx)
        source_steps = [self.steps[source_idx] for source_idx in source_step_indicies]
        return self.steps[step_idx].get_codes(mode=mode, source_steps=source_steps, full=full)

    def is_materialized(self):
        final_step = self.get_final_step()
        return final_step is None or getattr(final_step, 'materialized', False)

    def get_codes(self, mode="dbt", con=None, para=None):
        if para is None:
            para = {}
            
        sorted_step_idx = topological_sort(self.steps, self.edges)
        
        if mode == "dbt":
            with_clauses = []
            
            for step_idx  in sorted_step_idx[:-1]:
                
                
                
                incoming_steps = self.get_incoming_steps(step_idx)
                full = False
                if len(incoming_steps) == 1:
                    if len(self.get_incoming_steps(incoming_steps[0])) == 0:
                        full = True
                codes = self.get_codes_for_step(step_idx=step_idx, mode="AS", full=full)
                    
                if codes and codes.strip():
                    with_clauses.append(codes)

            codes = cocoon_comment_start
            utc_time = datetime.datetime.now(datetime.timezone.utc)
            codes += f"-- Generated at {utc_time}\n"
            if len(with_clauses) > 0:
                codes += "WITH \n" + ",\n\n".join(with_clauses) + "\n\n"
            codes += cocoon_comment_end

            codes += self.get_codes_for_step(step_idx=sorted_step_idx[-1], mode="", full= False if len(with_clauses) > 0 else True)

            return codes
        
        if mode == "WITH":
            with_clauses = []
            visited = set()
            
            def dfs(step_idx):
                if step_idx in visited:
                    return
                visited.add(step_idx)
                
                step = self.steps[step_idx]
                
                for parent_idx in self.get_incoming_steps(step_idx):
                    dfs(parent_idx)
                
                if not step.materialized:
                    codes = self.get_codes_for_step(step_idx=step_idx, mode="AS")
                    if codes and codes.strip():
                        with_clauses.append(codes)
            
            final_step_idx = self.find_final_node()
            final_step = self.steps[final_step_idx]
            if final_step.materialized:
                return ""
            
            dfs(final_step_idx)

            codes = ""
            if with_clauses:
                codes = "WITH \n" + ",\n\n".join(with_clauses)
            
            return codes

        if mode == "WITH_TABLE":
            select_all_codes = self.get_codes(mode="WITH_SELECT_ALL", con=con)
            last_step = self.steps[sorted_step_idx[-1]]
            final_codes = f"CREATE OR REPLACE TABLE {last_step} AS\n{indent_paragraph(select_all_codes)}"
            
            return final_codes

        if mode == "WITH_VIEW":
            select_all_codes = self.get_codes(mode="WITH_SELECT_ALL", con=con)
            last_step = self.steps[sorted_step_idx[-1]]
            final_codes = f"CREATE OR REPLACE VIEW {last_step} AS\n{indent_paragraph(select_all_codes)}"
            return final_codes
        
        if mode == "WITH_SELECT_ALL":
            with_clauses = self.get_codes(mode="WITH", con=con)
            full= False if len(with_clauses) > 0 else True
            last_step = self.steps[sorted_step_idx[-1]]
            con = last_step.con
            last_step_name = enclose_table_name(last_step.__repr__(full=full), con=con)
            select_all_codes = with_clauses + "\n" + f"SELECT * FROM {last_step_name}"
            
            if con is not None:
                database_name = get_database_name(con)
                if database_name == "SQL Server":
                    try:
                        select_all_codes = sqlglot.transpile(select_all_codes, read="tsql", write="tsql", comments=False)[0]
                    except:
                        pass
            return select_all_codes
        
        if mode == "TABLE":
            
            table_clauses = []
            
            for step_idx  in sorted_step_idx[1:]:
                codes = self.get_codes_for_step(step_idx=step_idx, mode="TABLE")
                table_clauses.append(codes)
                
            return "\n\n".join(table_clauses)
    
    def get_incoming_steps(self, step_idx):
        incoming = [from_idx for from_idx, to_indices in self.edges.items() if step_idx in to_indices]
        return sorted(incoming)

    def get_samples(self, con, columns=None, sample_size=None):
        if columns is not None and len(columns) == 0:
            if cocoon_main_setting['DEBUG_MODE']:
                raise ValueError("Columns list cannot be empty")
            return pd.DataFrame()

        query = self.get_samples_query(con, columns, sample_size)
        result = run_sql_return_df(con, query)
        return result
    
    def get_samples_query(self, con, columns=None, sample_size=None):
        if columns is None:
            selection_clause = "*"
        else:
            selection_clause = ", ".join(enclose_table_name(col, con=con) for col in columns)
        
        with_context_clause = self.get_codes(mode="WITH")
        
        if with_context_clause:
            query = f'SELECT {selection_clause}\nFROM "{self.__repr__(full=False)}"'
        else:
            query = f'SELECT {selection_clause}\nFROM {self}'
        
        if sample_size is not None:
            query = sample_query(con, query, sample_size)

        query = f"{with_context_clause}\n{query}"
        
        return query
    
    def get_schema(self, con):
        query = self.get_samples_query(con, sample_size=0)
        return get_query_schema(con, query)
    
    def get_schema_dict(self, con):
        final_step = self.get_final_step()
        return final_step.get_schema(con)

    def run_codes(self, con, mode="dbt", database=None, schema=None):
        sql_code = self.get_codes(mode=mode, con=con)
        run_sql_return_df(con, sql_code, database=database, schema=schema)
    
    def materialize(self, con, mode="WITH_TABLE", database=None, schema=None):
        if mode not in ["WITH_TABLE", "WITH_VIEW"]:
            raise ValueError("Mode must be either 'WITH_TABLE' or 'WITH_VIEW'.")
        
        final_step = self.get_final_step()
        if database is not None:
            final_step.database = database
        if schema is not None:
            final_step.schema = schema

        final_step.remove_from_database(con)
        self.run_codes(con=con, mode=mode, database=database, schema=schema)
        final_step.set_materialized()
    
    def __repr__(self, full=True):
        final_step = self.get_final_step()
        return final_step.__repr__(full=full)



def serialize_and_order_tables(foreign_key):
    graph = defaultdict(set)
    all_tables = set()
    
    for (pk_table, _), fk_list in foreign_key.items():
        all_tables.add(pk_table)
        for fk_table, _ in fk_list:
            if fk_table != pk_table:
                graph[pk_table].add(fk_table)
                all_tables.add(fk_table)
    
    outgoing_count = {table: len(graph[table]) for table in all_tables}
    
    ordered_tables = sorted(all_tables, key=lambda t: (-outgoing_count[t], t))
    
    numbered_tables = {}
    visited = set()
    current_number = len(all_tables)

    def dfs(table):
        nonlocal current_number
        if table in visited:
            return
        numbered_tables[table] = current_number
        current_number -= 1
        visited.add(table)

        all_neighbors = set(graph[table])
        for k, v in graph.items():
            if table in v:
                all_neighbors.add(k)

        sorted_neighbors = sorted(all_neighbors, key=lambda t: (outgoing_count[t], t))
        for neighbor in sorted_neighbors:
            dfs(neighbor)
        
    for start_table in ordered_tables:
        dfs(start_table)
    
    result = []
    number_to_table = {}
    for table in sorted(numbered_tables, key=numbered_tables.get):
        number = numbered_tables[table]
        number_to_table[number] = table
        foreign_keys = [numbered_tables[t] for t in graph[table]]
        foreign_keys_str = f" <- {', '.join(map(str, sorted(foreign_keys)))}" if foreign_keys else ""
        result.append(f"{number}. {table}{foreign_keys_str}")
    
    return "\n".join(result), number_to_table







 
class DataProject:
    
    
    def __init__(self, directory=None):
        self.tables = {}
                
        self.table_object = {}

        self.table_pipelines = {}    
        
        self.links = {}

        self.history_table = {}
        self.scd_columns = {}
        
        self.primary_key = {}
        
        self.foreign_key = {}
        
        self.referential_integrity = pd.DataFrame(columns=["pk_table_name", "pk", "fk_table_name", "fk", "Orphan", 'Explanation'])
        
        self.key = {}
        
        self.fks = {}
        
        self.entities = {}
        
        self.relations = {}
        
        self.groups = pd.DataFrame(columns=["Group Name", "Group Summary", "Tables", "Join Info"])
        
        self.story = []
        
        self.partition_mapping = {}
        
        self.name = ""

        self.directory = directory

    def get_directory(self):
        return self.directory

    def get_name(self):
        return self.name

    def get_key_info(self, tables=None, include_ri=True):
        if not tables:
            tables = self.key.keys()
        
        key_info = {}
        for table in self.key:
            if table in tables:
                table_info = copy.deepcopy(self.key[table])
                
                if not include_ri:
                    fks = table_info.get('foreign_keys', [])
                    if fks:
                        for fk in fks:
                            fk.pop('referential_integrity', None)
                
                key_info[table] = table_info
                
            if table in self.partition_mapping:
                partitions = self.partition_mapping[table]
                related_tables = set(partitions) & set(tables)
                if related_tables:
                    if len(related_tables) == 1:
                        related_table = next(iter(related_tables))
                        key_info[related_table] = self.key[table]
                    else:
                        related_tables_key = ", ".join(sorted(related_tables))
                        key_info[f"[{related_tables_key}]"] = self.key[table]
            
            if table in self.history_table:
                history_table = self.history_table[table]
                if history_table in tables:
                    key_info[history_table] = self.key[table]
        
        return key_info
                    
    
    def get_referential_integrity_warning(self, table_attributes):

        warnings = []
        tables = set(table_attributes.keys())
        mask = self.referential_integrity['pk_table_name'].isin(tables) & self.referential_integrity['fk_table_name'].isin(tables)
        relevant_integrity = self.referential_integrity[mask]
        
        for _, row in relevant_integrity.iterrows():
            pk_table = row['pk_table_name']
            fk_table = row['fk_table_name']
            pk = row['pk']
            fk = row['fk']
            orphan_percentage = row['Orphan']
            
            if pk in table_attributes[pk_table] and fk in table_attributes[fk_table]:
                alert_name = f"⚠️ Referential Integrity Violation from {fk_table} to {pk_table}"
                explanation = f"{fk_table}[{fk}] contains {orphan_percentage} orphaned records, not found {pk_table}[{pk}]."
                warnings.append((alert_name, explanation))
        
        return warnings
    
    def generate_text_summary(self, idx=None):
        name = self.get_name
        intro = f"# This is a database\n"
        intro += "# Below is join graph (table with foreign keys <- tables with primary key):\n"
        return intro + self.generate_text_join()
        
    def generate_text_join(self):
        if not hasattr(self, '_str_join') or not hasattr(self, 'number_to_table'):
            str_join, number_to_table = serialize_and_order_tables(self.foreign_key)
            self._str_join = str_join
            self.number_to_table = number_to_table
        return self._str_join
    
    def generate_item_text_summary(self, item_id):
        return self.describe_table_in_text(table_input=item_id)
    
    def create_data_selectbox(self, model_ids=None):
        all_tables = list(self.number_to_table.items())
        
        if model_ids is not None:
            filtered_tables = [(table_id, table_name) for table_id, table_name in all_tables if table_id in model_ids]
        else:
            filtered_tables = all_tables

        options = ["Database Schema"] + [f"{table_id}. {table_name}" for table_id, table_name in filtered_tables]
        
        if model_ids:
            selectbox_key = f"table_select_{'_'.join(map(str, sorted(model_ids)))}"
        else:
            selectbox_key = "table_select_all"

        selected_option = st.selectbox("Select a table or view database schema", options=options, key=selectbox_key)
        
        if selected_option == "Database Schema":
            if model_ids is not None:
                schema_html = self.generate_html_summary(item_ids=model_ids)
            else:
                schema_html = self.generate_html_summary()
            st.components.v1.html(schema_html, height=400, scrolling=True)
            
            join_info = self.get_key_info(tables=model_ids)
            if join_info:
                st.markdown("*Join Relationship*")
                join_yaml = yaml.dump(join_info, default_flow_style=False)
                st.markdown(
                    f"""
                    <div style="max-height: 500px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;">
                    <pre><code class="language-yaml">{join_yaml}</code></pre>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
        elif selected_option != "Select an option":
            item_id = int(selected_option.split('.')[0])
            table_name = self.get_item_name_by_id(item_id)
            
            if table_name in self.table_object:
                table_obj = self.table_object[table_name]
                yml_content = table_obj.create_dbt_schema_yml()
                st.markdown("*Table and Column catalog prepared by Cocoon*")
                st.markdown(
                    f"""
                    <div style="max-height: 500px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;">
                    <pre><code class="language-yaml">{yml_content}</code></pre>
                    </div>
                    """,
                    unsafe_allow_html=True
                )
            else:
                st.error(f"No table object found for '{table_name}'")
                
    def get_item_name_by_id(self, item_id):
        return self.number_to_table.get(item_id)

    def describe_table_in_text(self, table_input, show_table_summary=True, show_category=True, 
                            show_unique=True, show_pattern=True, show_type=True,
                            show_keys=True):
        if isinstance(table_input, int):
            if hasattr(self, 'number_to_table'):
                table_name = self.number_to_table.get(table_input)
                if table_name is None:
                    return f"No table found for id {table_input}"
                model_id = table_input
            else:
                return "Table numbering not available"
        else:
            table_name = table_input
            model_id = None

        if table_name not in self.table_object:
            return f"No table object found for '{table_name}'"

        table_desc = self.table_object[table_name].print_column_desc(
            show_table_summary=show_table_summary,
            show_category=show_category,
            show_unique=show_unique,
            show_pattern=show_pattern,
            show_type=show_type
        )

        if show_keys:
            key_info = self.get_key_info(tables=[table_name])
            if table_name in key_info:
                table_keys = key_info[table_name]
                
                if 'primary_key' in table_keys:
                    table_desc += f"\n\nPrimary Key: '{table_keys['primary_key']}'"
                
                if 'foreign_keys' in table_keys:
                    table_desc += "\n\nForeign Keys:"
                    for fk in table_keys['foreign_keys']:
                        table_desc += f"\n- {fk['column']} references {fk['reference']['table_name']}['{fk['reference']['column']}']"

        return table_desc

    def generate_items_summary(self, item_ids=None, full=True):
        if item_ids is None:
            return self.generate_multiple_table_summaries(model_indices=item_ids,include_join_graph=True, full=full)
        else:
            return self.generate_multiple_table_summaries(model_indices=item_ids, full=full)

    def generate_multiple_table_summaries(self, model_indices=None, full=True, include_join_graph=False):
        
        self.generate_text_join()

        if model_indices is None:
            section_name = "model_summary_all"
            model_indices = list(self.number_to_table.keys())
        else:
            section_name = f"model_summary_{'_'.join(map(str, model_indices))}"
        
        options = OrderedDict()

        if include_join_graph:
            join_graph_html = self.generate_html_graph_static()
            options["Join Graph"] = join_graph_html

        for idx in model_indices:
            if idx in self.number_to_table:
                table_name = self.number_to_table[idx]
                if table_name in self.table_object:
                    table_obj = self.table_object[table_name]
                    yml_content = table_obj.create_dbt_schema_yml()
                    highlighted_yml_content = highlight_yml_only(yml_content)
                    highlighted_yml_content = f'<b>{idx}. {table_name}</b><br><small class="text-muted mb-3">Table catalog prepared by Cocoon</small>' + highlighted_yml_content
                    options[f"{idx}. {table_name}"] = highlighted_yml_content

        selected = next(iter(options.keys()), None)

        combined_html = generate_dropdown_html(section_name, options, default_selection="Explore Table", full=False, selected=selected)

        if not full:
            return combined_html
        
        formatter = HtmlFormatter(style='monokai')
        css_style = formatter.get_style_defs('.highlight')
        html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <style>
        body {{
            font-size: 0.75rem;
        }}
        pre {{ line-height: 125%; }}
        .highlight {{
            background: #272822;
            color: #f8f8f2;
            border-radius: 4px;
            padding: 1em;
            margin: 1em 0;
            overflow-x: auto;
            font-size: 12px;
        }}
        {css_style}
        {tag_css}
        {pandas_css}
    </style>
</head>
<body>
    {combined_html}
</body>
</html>
"""
        return html
    








    
    def create_table_ddl(self, table_name):
        if table_name not in self.tables:
            raise ValueError(f"Table '{table_name}' does not exist in the project.")
        
        columns = self.tables[table_name]
        column_definitions = []
        
        for column_name, column_type in columns.items():
            column_definitions.append(f"    {column_name} {column_type}")
        
        column_string = ",\n".join(column_definitions)
        
        ddl = f"CREATE TABLE '{table_name}' (\n{column_string}\n);"
        
        return ddl
    
    def create_all_tables_ddl(self):
        ddl_statements = []
        
        for table_name in self.tables:
            ddl_statements.append(self.create_table_ddl(table_name))
        
        return "\n\n".join(ddl_statements)
    
    def build_er_story_from_yml(self, er_yml):
        if isinstance(er_yml, str):
            er_yml = yaml.safe_load(er_yml)
        
        for entity in er_yml.get('entities', []):
            table_name = entity.get('table_name')
            if table_name:
                self.entities[table_name] = {
                    'entity_name': entity.get('entity_name'),
                    'entity_desc': entity.get('entity_description')
                }
        
        for relation in er_yml.get('relations', []):
            table_name = relation.get('table_name')
            if table_name:
                self.relations[table_name] = {
                    'relation_name': relation.get('relation_name'),
                    'entities': relation.get('entities', []),
                    'relation_desc': relation.get('relation_description')
                }
        
        groups_data = []
        for group in er_yml.get('groups', []):
            groups_data.append({
                'Group Name': group.get('group_name'),
                'Group Summary': group.get('group_summary'),
                'Tables': group.get('tables'),
                'Join Info': group.get('join_info', '')
            })
        self.groups = pd.DataFrame(groups_data)
        
        self.story = er_yml.get('story', [])
        
    def add_table_project(self, table_project):
        table_name = table_project.table_name
        self.table_object[table_name] = table_project
        

    def change_table_name(self, old_table_name, new_table_name):
        if old_table_name not in self.tables:
            raise KeyError(f"Table '{old_table_name}' does not exist.")
        
        if new_table_name in self.tables:
            raise ValueError(f"Table '{new_table_name}' already exists.")
        
        self.tables[new_table_name] = self.tables.pop(old_table_name)
        
        if old_table_name in self.table_object:
            self.table_object[new_table_name] = self.table_object.pop(old_table_name)
        
         
        if old_table_name in self.links:
            self.links[new_table_name] = self.links.pop(old_table_name)
        for table in self.links:
            if old_table_name in self.links[table]:
                self.links[table][new_table_name] = self.links[table].pop(old_table_name)
        
        if old_table_name in self.primary_key:
            self.primary_key[new_table_name] = self.primary_key.pop(old_table_name)
        
        new_foreign_key = {}
        for (pk_table, pk), fk_list in self.foreign_key.items():
            if pk_table == old_table_name:
                new_key = (new_table_name, pk)
            else:
                new_key = (pk_table, pk)
            
            new_value = [(new_table_name if t == old_table_name else t, fk) for t, fk in fk_list]
            new_foreign_key[new_key] = new_value
        self.foreign_key = new_foreign_key
        
        if old_table_name in self.fks:
            self.fks[new_table_name] = self.fks.pop(old_table_name)
        
        if old_table_name in self.entities:
            self.entities[new_table_name] = self.entities.pop(old_table_name)
    

    def join_graph_dict(self, add_time_keys=True):
        models = OrderedDict()
        
        integrity_lookup = {}
        if hasattr(self, 'referential_integrity'):
            for _, row in self.referential_integrity.iterrows():
                key = (row['pk_table_name'], row['pk'], row['fk_table_name'], row['fk'])
                integrity_lookup[key] = {
                    'Orphan': row['Orphan'],
                    'explanation': row['Explanation']
                }
                
        for (pk_table_name, pk), fk_list in self.foreign_key.items():
            if pk_table_name not in models:
                models[pk_table_name] = OrderedDict({
                    'table_name': pk_table_name,
                    'primary_key': pk,
                    'foreign_keys': []
                })
            else:
                if 'primary_key' not in models[pk_table_name]:
                    models[pk_table_name]['primary_key'] = pk

            for fk_table_name, fk in fk_list:
                fk_entry = OrderedDict({
                    'column': fk,
                    'reference': OrderedDict({
                        'table_name': pk_table_name,
                        'column': pk
                    })
                })
                
                integrity_key = (pk_table_name, pk, fk_table_name, fk)
                if integrity_key in integrity_lookup:
                    fk_entry['referential_integrity'] = OrderedDict({
                        'Orphan': integrity_lookup[integrity_key]['Orphan'],
                        'explanation': integrity_lookup[integrity_key]['explanation']
                    })


                if fk_table_name not in models:
                    models[fk_table_name] = OrderedDict({
                        'table_name': fk_table_name,
                        'foreign_keys': [fk_entry]
                    })
                else:
                    if 'foreign_keys' not in models[fk_table_name]:
                        models[fk_table_name]['foreign_keys'] = []
                    models[fk_table_name]['foreign_keys'].append(fk_entry)
        
        
        for table_name, attributes in self.tables.items():
            if table_name not in models:
                models[table_name] = OrderedDict({
                    'table_name': table_name
                })
            if add_time_keys:
                if isinstance(attributes, dict):
                    time_keys = [attr for attr, type_ in attributes.items() if is_type_time(type_)]
                    
                    if time_keys:
                        models[table_name]['time_keys'] = time_keys

        return OrderedDict({'join_graph': list(models.values())})
        
    def join_graph_yaml(self, add_time_keys=True):
        join_graph_dict = self.join_graph_dict(add_time_keys=add_time_keys)
        yml_content = yaml.dump(join_graph_dict, default_flow_style=False)
        return yml_content
    
    def build_foreign_key_from_join_graph_yml(self, join_graph_yml):
        if isinstance(join_graph_yml, str):
            join_graph_yml = yaml.safe_load(join_graph_yml)
            
        foreign_key = reverse_join_graph(join_graph_yml)
        self.foreign_key = foreign_key
        
        self.referential_integrity = pd.DataFrame(columns=["pk_table_name", "pk", "fk_table_name", "fk", "Orphan", 'Explanation'])

        for table in join_graph_yml['join_graph']:
            if 'foreign_keys' in table:
                for fk in table['foreign_keys']:
                    fk_table_name = table['table_name']
                    fk_column = fk['column']
                    pk_table_name = fk['reference']['table_name']
                    pk_column = fk['reference']['column']

                    orphan_percent = fk.get('referential_integrity', {}).get('Orphan', 0)
                    explanation = fk.get('referential_integrity', {}).get('Explanation', 0)
                    if orphan_percent == 0:
                        continue
                    
                    new_row = pd.DataFrame({
                        "pk_table_name": [pk_table_name],
                        "pk": [pk_column],
                        "fk_table_name": [fk_table_name],
                        "fk": [fk_column],
                        "Orphan": [orphan_percent],
                        "Explanation": [explanation]
                    })

                    self.referential_integrity = pd.concat([self.referential_integrity, new_row], ignore_index=True)
                    
    def add_foreign_key_to_primary_key(self, fk_table_name, fk, pk_table_name, pk):
        if (pk_table_name, pk) not in self.foreign_key:
            self.foreign_key[(pk_table_name, pk)] = []
            
        self.foreign_key[(pk_table_name, pk)].append((fk_table_name, fk))
        
    def add_table_primay_key(self, table_name, primary_key):
        self.primary_key[table_name] = primary_key
    
    def construct_links_from_pk_fk(self):
        for (pk_table_name, pk), fk_list in self.foreign_key.items():
            for fk_table_name, fk in fk_list:
                if pk_table_name not in self.links:
                    self.links[pk_table_name] = {}
                
                self.links[pk_table_name][fk_table_name] = [[pk], [fk]]  

    def add_links(self, join_infos):
        for join_info in join_infos:
            table1, table2 = join_info['tables'][:2]
            keys1, keys2 = join_info['join_keys'][:2]

            if table1 not in self.links:
                self.links[table1] = {}

            if table2 not in self.links:
                self.links[table2] = {}

            self.links[table1][table2] = [keys1, keys2]
            self.links[table2][table1] = [keys2, keys1]
    
    def get_story(self):
        return self.story

    def set_story(self, story):
        self.story = story

    def add_table(self, table_name, attributes):
        if table_name in self.tables:
            pass
        else:
            self.tables[table_name] = attributes

    def add_tables_with_con(self, table_names, con):
        for table_name in table_names:
            self.add_table(table_name, get_table_schema(con, table_name))
                
    def get_columns(self, table_name):
        if table_name in self.tables:
            return self.tables[table_name]
        else:
            return []

    def remove_table(self, table_name):
        if table_name in self.tables:
            del self.tables[table_name]
        else:
            pass
        
        if table_name in self.links:
            del self.links[table_name]

        for table_name_source in list(self.links.keys()):
            if table_name in self.links[table_name_source]:
                del self.links[table_name_source][table_name]

    def list_tables(self):
        return list(self.tables.keys())

    def describe_project(self, tables=None, limit=10):
        description = ""
        idx = 0
        for table, attributes in self.tables.items():
            if tables is not None:
                if table not in tables:
                    continue
            if isinstance(attributes, dict):
                attributes = list(attributes.keys())
            
            if len(attributes) < limit:
                description += f"{idx+1}. '{table}': {attributes}\n"
            else:
                description += f"{idx+1}. '{table}': {attributes[:limit] + ['...']}\n"
            
            idx += 1
            
            if table in self.history_table:
                history_table = self.history_table[table]
                
                history_table_attributes = self.scd_columns.get(history_table, [])
                
                if isinstance(history_table_attributes, dict):
                    history_table_attributes = list(history_table_attributes.keys())
                
                if len(history_table_attributes) < limit:
                    description += f"|- History -> '{history_table}': {history_table_attributes}\n"
                else:
                    description += f"|- History -> '{history_table}': {history_table_attributes[:limit] + ['...']}\n"
            
        return description

    def get_key_columns(self, table):
        keys_to_exclude = set()
        
        table_keys = self.key.get(table, {})
                
        if 'primary_key' in table_keys:
            keys_to_exclude.add(table_keys['primary_key'])
        
        if 'foreign_keys' in table_keys:
            if table_keys['foreign_keys']:
                for fk in table_keys['foreign_keys']:
                    keys_to_exclude.add(fk['column'])
                
        return keys_to_exclude
    
    def describe_project_yml(self, tables=None, limit=10, exclude_keys=False):
        result = []
        for table, attributes in self.tables.items():
            if tables is not None and table not in tables:
                continue
            
            table_info = OrderedDict()
            
            if table in self.partition_mapping:
                partitons = self.partition_mapping[table]
                table_info['partitons'] = f'The data for {table} is partitioned into {len(partitons)} tables' 
                table_info['table_names'] = partitons
                
            else:
                table_info['table_name'] = table
                
            if hasattr(self, 'table_object') and table in self.table_object:
                table_desc = getattr(self.table_object[table], 'table_summary', None)
                if table_desc:
                    table_info['table_desc'] = table_desc
            
            if isinstance(attributes, dict):
                attributes = list(attributes.keys())
            
            if exclude_keys:

                keys_to_exclude = self.get_key_columns(table)
                
                attributes = [attr for attr in attributes if attr not in keys_to_exclude]
            
            attribute_list = attributes[:limit]
            if len(attributes) > limit:
                attribute_list.append('...')
            
            if exclude_keys:
                table_info['attribute_subset'] = attribute_list
            else:
                table_info['attributes'] = attribute_list
            
            if table in self.history_table:
                history_table = self.history_table[table]
                history_table_attributes = self.scd_columns.get(history_table, [])
                
                if isinstance(history_table_attributes, dict):
                    history_table_attributes = list(history_table_attributes.keys())
                
                history_attribute_list = history_table_attributes[:limit]
                if len(history_table_attributes) > limit:
                    history_attribute_list.append('...')
                
                table_info['history_table'] = {
                    'table_name': history_table,
                    'table_desc': "This is the full history table of the snapshot " + table,
                    'version_attributes': history_attribute_list
                }
            
            result.append(table_info)
        
        return result
    
    def display_graph(self):
        tables = self.list_tables()

        graph_data = {}
        graph_data["nodes"] = [{"id": table} for table in tables]
        links = []

        source_target_pair = {}

        for table1 in self.links:
            for table2 in self.links[table1]:
                key = min(table1, table2)
                value = max(table1, table2)

                if key not in source_target_pair:
                    source_target_pair[key] = set()

                source_target_pair[key].add(value)

        for table1 in source_target_pair:
            for table2 in source_target_pair[table1]:
                links.append({"source": table1, "target": table2})

        graph_data["links"] = links

        display_draggable_graph_html(graph_data)

    def generate_nodes_edges(self, selected_nodes=None):
        if selected_nodes and all(isinstance(node, int) for node in selected_nodes):
            selected_nodes = [self.number_to_table.get(node) for node in selected_nodes if node in self.number_to_table]

        original_nodes, edges = generate_nodes_edges(self.foreign_key)
        
        for table in self.tables.keys():
            if table not in original_nodes:
                original_nodes.append(table)
        
        if selected_nodes:
            selected_set = set(selected_nodes)
            
            filtered_nodes = [node for node in original_nodes if node in selected_set]
            
            node_to_index = {node: index for index, node in enumerate(filtered_nodes)}
            
            filtered_edges = []
            for from_idx, to_idx in edges:
                from_node = original_nodes[from_idx]
                to_node = original_nodes[to_idx]
                if from_node in selected_set and to_node in selected_set:
                    new_from_idx = node_to_index[from_node]
                    new_to_idx = node_to_index[to_node]
                    filtered_edges.append((new_from_idx, new_to_idx))
            
            return filtered_nodes, filtered_edges
        
        return original_nodes, edges

    def generate_story_html(self):
        html_items = []
        for i, item in enumerate(self.story):
            html_item = f"<li>'{item['Name']}': {item['Description']}</li>"
            html_items.append(html_item)
        
        html_content = "<ol>\n" + "\n".join(html_items) + "\n</ol>"
        return html_content
    
    def generate_html_summary(self, item_ids=None):
        return self.generate_html_graph_static(selected_nodes=item_ids)

    def display_model_overview_st(self, item_ids=None):
        graph_html = self.generate_html_summary(item_ids=item_ids)
    
        st.markdown(graph_html, unsafe_allow_html=True)

    def generate_html_graph_static(self, selected_nodes=None, highlight_nodes=None):
        
        nodes, edges = self.generate_nodes_edges(selected_nodes=selected_nodes)
        edge_labels = [''] * len(edges)
        
        highlight_nodes_indices = set()
        if highlight_nodes is not None:
            highlight_nodes_indices = {nodes.index(node) for node in highlight_nodes if node in nodes}
        else:
            highlight_nodes = []
        
        highlight_edges_indices = set()
        
        
        for partition, table_list in self.partition_mapping.items():
            if not selected_nodes or partition in selected_nodes:
                if partition not in nodes:
                    nodes.append(partition)
                partition_index = nodes.index(partition)
                
                for table in table_list:
                    if table in highlight_nodes:
                        if table not in nodes:
                            nodes.append(table)
                        table_index = nodes.index(table)
                        
                        edges.append((partition_index, table_index))
                        edge_labels.append("partition")
                        
                        highlight_nodes_indices.add(table_index)
                        highlight_edges_indices.add(len(edges) - 1)
        
        
        for snapshot_table, history_table in self.history_table.items():
            if not selected_nodes or snapshot_table in selected_nodes:
                if snapshot_table not in nodes:
                    nodes.append(snapshot_table)
                snapshot_index = nodes.index(snapshot_table)
                
                if history_table in highlight_nodes:
                    if history_table not in nodes:
                        nodes.append(history_table)
                    history_index = nodes.index(history_table)
            
                    edges.append((snapshot_index, history_index))
                    edge_labels.append("history")
                    
                    highlight_nodes_indices.add(history_index)
                    highlight_edges_indices.add(len(edges) - 1)
        
        html_output = generate_workflow_html_multiple(
            nodes, edges, 
            edge_labels=edge_labels, 
            directional=True, 
            highlight_nodes_indices=list(highlight_nodes_indices),
            highlight_edges_indices=list(highlight_edges_indices)
        )
        
        return html_output

    def display_graph_static(self, selected_nodes=None, highlight_nodes=None):
        html_output = self.generate_html_graph_static(selected_nodes=selected_nodes, highlight_nodes=highlight_nodes)
        display(HTML(html_output))
            
    def display_story_multiple_page(self):
        relation_story = self.get_story_list()

        relation_to_entities = {}

        for relation_info in self.relations.values():
            relation_name = relation_info['relation_name']
            if relation_name is not None:
                relation_to_entities[relation_name] = relation_info['entities']
            
        display_pages(total_page=len(relation_story), 
            create_html_content=partial(create_html_content_er_story, relation_to_entities, relation_story, None))

    def get_story_list(self, exclude_categories=None):
        
        if exclude_categories is None:
            exclude_categories = []
            
        filtered_data = filter_categories(self.story, exclude_categories)
        collected_list = collect_list(filtered_data)
        
        return collected_list

    def display_tree(self, exclude_categories=None):
        filtered_data = self.story
        
        if exclude_categories is None:
            exclude_categories = []
        else:
            filtered_data = filter_categories(filtered_data, exclude_categories)
        
        html_output = generate_tree_html(filtered_data)
        
        html_content = f"""
    <style>
    {tree_hierarchy_css}
    </style>
    <body>
    {html_output}
    </body>
    """
        display(HTML(html_content))


    
    def get_lowest_categories(self, exclude_categories=None):
        
        filtered_data = self.story
        
        if exclude_categories is None:
            exclude_categories = []
        else:
            filtered_data = filter_categories(filtered_data, exclude_categories)
        
        return extract_lowest_categories(filtered_data)
    
    def extract_tables_from_stories(self, related_steps): 
        return extract_related_tables(self, related_steps)
            
def filter_categories(data, exclude_categories):
    if isinstance(data, list):
        return data
    
    if isinstance(data, dict):
        result = {}
        for key, value in data.items():
            if key not in exclude_categories:
                if isinstance(value, dict) and 'children' in value:
                    filtered_children = filter_categories(value['children'], exclude_categories)
                    if filtered_children:
                        result[key] = value.copy()
                        result[key]['children'] = filtered_children
                else:
                    result[key] = filter_categories(value, exclude_categories)
        return result
    
    return data

def collect_list(data):
    if isinstance(data, list):
        return data
    
    if isinstance(data, dict):
        result = []
        for value in data.values():
            if 'children' in value:
                result.extend(collect_list(value['children']))
            elif isinstance(value, list):
                result.extend(value)
            elif isinstance(value, dict):
                result.extend(collect_list(value))
        return result
    
    return []


def extract_lowest_categories(data):
    if isinstance(data, list):
        return {}
    
    result = {}

    def traverse(node, current_path):
        if isinstance(node, dict):
            if 'children' in node:
                if isinstance(node['children'], list):
                    category = current_path[-1]
                    result[category] = node.get('description', '')
                else:
                    for key, value in node['children'].items():
                        traverse(value, current_path + [key])
        elif isinstance(node, list):
            return

    for key, value in data.items():
        traverse(value, [key])

    return result

def generate_nodes_edges(foreign_key):
    table_names = set()
    for (pk_table, _), fk_list in foreign_key.items():
        table_names.add(pk_table)
        for fk_table, _ in fk_list:
            table_names.add(fk_table)
    
    nodes = list(table_names)
    
    table_to_index = {table: idx for idx, table in enumerate(nodes)}
    
    edges = []
    for (pk_table, _), fk_list in foreign_key.items():
        pk_idx = table_to_index[pk_table]
        for fk_table, _ in fk_list:
            fk_idx = table_to_index[fk_table]
            edges.append((fk_idx, pk_idx))
    
    return nodes, edges

def reverse_join_graph(join_graph_dict):
    foreign_key = OrderedDict()

    for table in join_graph_dict['join_graph']:
        table_name = table['table_name']
        primary_key = table.get('primary_key')

        if primary_key:
            key = (table_name, primary_key)
            if key not in foreign_key:
                foreign_key[key] = []

        if 'foreign_keys' in table:
            for fk in table['foreign_keys']:
                ref_table = fk['reference']['table_name']
                ref_column = fk['reference']['column']
                fk_column = fk['column']

                key = (ref_table, ref_column)
                if key not in foreign_key:
                    foreign_key[key] = []
                foreign_key[key].append((table_name, fk_column))

    return foreign_key

def generate_tree_html(filtered_data):
    tree_data = {"root": {
        'description': '',
        'children': filtered_data
    }}
    
    def process_hierarchy(data, func):
        if isinstance(data, dict):
            return {key: process_hierarchy(value, func) for key, value in data.items()}
        elif isinstance(data, list):
            return [func(item) for item in data]
        else:
            return data
        
    def extract_name(s):
        return s['name']

    processed_hierarchy = process_hierarchy(tree_data, extract_name)
    
    html_output = generate_html_tree(processed_hierarchy)
    return html_output






def display_duplicated_rows_html2(df):
    if 'cocoon_count' in df.columns:
        count_col = 'cocoon_count'
    elif 'COCOON_COUNT' in df.columns:
        count_col = 'COCOON_COUNT'
    else:
        raise ValueError("Expected column 'cocoon_count' or 'COCOON_COUNT' not found in DataFrame")

    html_output = f"<p>🤨 There are {len(df)} groups of duplicated rows.</p>"
    for i, row in df.iterrows():
        html_output += f"<p>Group {i+1} appear {row[count_col]} times:</p>"
        row_without_cocoon_count = row.drop(labels=[count_col])
        html_output += row_without_cocoon_count.to_frame().T.to_html(index=False)

    if len(df) > 5:
        html_output += "<p>...</p>"
    html_output += "<p>🧐 Do you want to remove the duplicated rows?</p>"
    
    display(HTML(html_output))

def create_explore_button(query_widget, table_name=None, query="", list_description="", with_context=None, show_with_context=True, logical_to_physical=None):
    error_msg = widgets.HTML(value='')
    
    if logical_to_physical is None:
        logical_to_physical = {}
        
    if isinstance(table_name, tuple):
        table_name = list(table_name)
        
    con = query_widget.con
    
    if isinstance(table_name, list):
        dropdown = widgets.Dropdown(
            description=list_description,
            options=[(name, name) for name in table_name],
            disabled=False,
        )
        
        explore_button = widgets.Button(
            description='Explore',
            disabled=False,
            button_style='info',
            tooltip='Click to explore',
            icon='search'
        )
        
        
        def on_button_clicked(b):
            selected_table = dropdown.value
            selected_table = logical_to_physical.get(selected_table, selected_table)
            
            selected_table = enclose_table_name(selected_table, con=con)
                
            if selected_table:
                query = f'SELECT * FROM {selected_table}'
                error_msg.value = f"<div style='color: green;'>😎 Query submitted for {selected_table}. Check out the data widget!</div>"
                query_widget.run_query(query)
            else:
                error_msg.value = f"<div style='color: red;'>Please select a table from the dropdown.</div>"
        
        explore_button.on_click(on_button_clicked)
        
        display(VBox([HBox([dropdown, explore_button]), error_msg]))
        return dropdown
        
    else:
        if table_name is not None:
            physical_table_name = logical_to_physical.get(table_name, table_name)
            physical_table_name = enclose_table_name(physical_table_name, con=con)
            query = f'SELECT * FROM {physical_table_name}'
            
        explore_button = widgets.Button(
            description='Explore',
            disabled=False,
            button_style='info',
            tooltip='Click to explore',
            icon='search'
        )

        def on_button_clicked(b):
            nonlocal with_context  
            error_msg.value = f"<div style='color: green;'>😎 Query submitted. Check out the data widget!</div>"
            if show_with_context:
                if with_context is None: 
                    if isinstance(table_name, TransformationSQLPipeline):
                        with_context = table_name.get_codes(mode="WITH")
                    else:
                        with_context = ""
                query_widget.update_context_value(with_context)
                
            query_widget.run_query(query)

        explore_button.on_click(on_button_clicked)
        display(VBox([explore_button, error_msg]))
        


class DescribeColumns(Node):
    default_name = 'Describe Columns'
    default_description = 'This node allows users to describe the columns of a table.'

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Checking columns ...")
        
        create_progress_bar_with_numbers(1, doc_steps)
        self.progress = show_progress(1)

        self.input_item = item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]

        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        sample_size = 5
        
        table_summary = self.get_sibling_document("Create Short Source Table Summary")

        sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        table_desc = sample_df.to_csv(index=False, quoting=1)
        
        return table_desc, table_summary, columns

    def run(self, extract_output, use_cache=True):
        table_desc, table_summary, column_names = extract_output

        template = f"""You have the following table:
{table_desc}

{table_summary}

Task: Describe the columns in the table.

Return in the following format:
```json
{{
    "{column_names[0]}": "Short description in < 10 words",
    ...
}}
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)

        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        default_response = {column: column for column in extract_output[2]}
        return default_response
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        _, _, columns = extract_output
        self.progress.value += 1

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)

        rows_list = []

        for col in columns:
            rows_list.append({
                "Column": col,
                "Summary": json_code[col],
            })

        df = pd.DataFrame(rows_list)
        
        editable_columns = [False, True]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        print("😎 We have described the columns:")
        display(grid)

        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  

        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                document = new_df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_clicked)

        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
        
class DecideProjection(Node):
    default_name = 'Decide Projection'
    default_description = 'This allows users to select a subset of columns.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        create_progress_bar_with_numbers(0, doc_steps)

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        schema = table_pipeline.get_schema(con)
        
        query_widget = self.item["query_widget"]
        create_explore_button(query_widget, table_pipeline)

        num_cols = len(schema)
        column_names = list(schema.keys())
        document = {"selected_columns": []}

        def remove_columns():
            df = grid_to_updated_dataframe(grid, reset=reset)
            
            kept_columns = df[df['Exclude?'] == False]['Column Name'].tolist()
             
            if len(kept_columns) == 0:
                print("🙁 Please keep at least one column.")
                return

            clear_output(wait=True)
            document["selected_columns"] = [enclose_table_name(kept_column, con) for kept_column in kept_columns]

            if len(kept_columns) < num_cols:
                old_table_name = table_pipeline.__repr__(full=False)
                new_table_name = old_table_name + "_projected"
                selection_clause = ',\n'.join(document['selected_columns'])
                sql_query = f'SELECT \n{indent_paragraph(selection_clause)}'
                comment = f"-- Projection: Selecting {len(document['selected_columns'])} out of {num_cols} columns\n"
                if num_cols - len(kept_columns) < 10:
                    not_selected_columns = [column_names[i] for i in range(num_cols) if i not in kept_columns]
                    comment += f"-- Columns projected out: {not_selected_columns}\n"
                    
                sql_query = lambda *args, comment=comment, sql_query=sql_query: f"{comment}{sql_query}\nFROM {args[0]}"
                     
                step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con)
                table_pipeline.add_step_to_final(step)
                
            
            return
            
        display(HTML(f"<b>🧐 Please select the columns to <u>exclude</u>.</b><div><em>For example, you may want to remove placeholder columns or those containing PII.</em></div>"))

        except_columns = ["_fivetran_synced", "fivetran_synced"]
        
        table_object = self.para["table_object"]
        
        data = {
            'Column Name': [],
            'Is PII': [],
            'Exclude?': [],
        }
        
        for column in column_names:
            is_pii = column in table_object.pii
            exclude = is_pii or column in except_columns
            
            data['Column Name'].append(column)
            data['Is PII'].append(is_pii)
            data['Exclude?'].append(exclude)

        df = pd.DataFrame(data)

        editable_columns = [False, False, True]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset)
        
        display(grid)
        
        submit_button = widgets.Button(description="Accept Projection", button_style='success', icon='check')
        
        def submit(b):
            with self.output_context():
                remove_columns()
                callback({})
                return 
        
        submit_button.on_click(submit)
        
        reject_button = widgets.Button(
            description="Reject Projection", 
            button_style='danger', 
            icon='times'
        )

        def reject(b):
            with self.output_context():
                callback({})
                return 

        reject_button.on_click(reject)

        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            if (hasattr(self, 'para') and 
                isinstance(self.para, dict) and 
                isinstance(self.para.get('cocoon_stage_options'), dict) and 
                self.para['cocoon_stage_options'].get('projection') is False):
                reject(reject_button)
            else:
                submit(submit_button)
            return
        
        display(HBox([reject_button, submit_button]))

class CreateColumnGrouping(Node):
    default_name = 'Create Column Grouping'
    default_description = 'This node allows users to group columns based on their meanings.'
    retry_times = 3

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Building column hierarchy ...")
        create_progress_bar_with_numbers(1, doc_steps)
        
        self.progress = show_progress(1)

        self.input_item = item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        sample_size = 5

        table_summary = self.get_sibling_document("Create Table Summary")
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())

        sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        table_desc = sample_df.to_csv(index=False, quoting=1)

        schema = table_pipeline.get_schema(con)
        column_names = list(schema.keys())

        return table_desc, table_summary, column_names
    
    def run(self, extract_output, use_cache=True):
        table_desc, table_summary, column_names = extract_output

        template = f"""You have the following table:
{table_desc}

- Task: Recursively group the attributes based on inherent entity association, not conceptual similarity.
    E.g., for [student name, student grade, teacher grade], group by student and teacher, not by name.
Avoid groups with too many/few subgroups. 

Conclude with the final result as a multi-level JSON. 

```json
{{
    "reasoning": "There are {len(column_names)} columns. The groups shall be ...",
    "Main group": # group name shall not be attribute name
        {{
        "Sub group": {{ 
            "sub-sub group": ["attribute1", "attribute2", ...], # Make sure each attribute is included exactly one array
        }},
    }}
}}
```"""
        
            
                        
                        
                            

            


            




        
        messages =[ {"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        assistant_message = response['choices'][0]['message']
        messages.append(assistant_message)
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(assistant_message['content'])
        json_var = json.loads(json_code)
        json_var.pop("reasoning", None)
        
        
        def repair_json_attributes(json_var, reference_attributes):
            seen_attributes = set()
            reference_set = set(reference_attributes)
            other_attributes = []

            def traverse(element):
                if isinstance(element, dict):
                    for key, value in list(element.items()):
                        if isinstance(value, list):
                            new_list = []
                            for item in value:
                                if isinstance(item, str):
                                    if item not in seen_attributes and item in reference_set:
                                        new_list.append(item)
                                        seen_attributes.add(item)
                                else:
                                    traverse(item)
                            element[key] = new_list
                        else:
                            traverse(value)
                elif isinstance(element, list):
                    for i, item in enumerate(list(element)):
                        if isinstance(item, str):
                            if item not in seen_attributes and item in reference_set:
                                seen_attributes.add(item)
                            else:
                                element.pop(i)
                        else:
                            traverse(item)

            traverse(json_var)
            
            missing_attributes = reference_set - seen_attributes
            if missing_attributes:
                json_var['Other'] = list(missing_attributes)

            return json_var

        repaired_json = repair_json_attributes(json_var, column_names)

        
        return repaired_json
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        self.progress.value += 1
        
        json_code = run_output
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        
        table_name = table_pipeline.__repr__(full=False)
        json_code_new = {}
        json_code_new[table_name] = json_code
        json_code = json_code_new
    
        html_content_updated, _, height = get_tree_html(json_code)

        print(f"😎 We have built the Column Hierarchy:")

        display_html_iframe(html_content_updated, height=f"{height+50}px")

        print("🧐 Next we will delve into the columns")

        edit_button = widgets.Button(
            description='Edit',
            disabled=False,
            button_style='danger', 
            tooltip='Click to edit',
            icon='edit'
        )

        def on_edit_clicked(b):
            with self.output_context():
                print("Not implemented yet")

        edit_button.on_click(on_edit_clicked)

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                print("Submission received.")
                callback(json_code)

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)

        display(HBox([edit_button, submit_button]))
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(submit_button)
            return
    
class CreateTableSummary(Node):
    default_name = 'Create Table Summary'
    default_description = 'This node creates a summary of the table.'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item

        print("📝 Generating table summary ...")
        
        create_progress_bar_with_numbers(1, doc_steps)

        self.progress = show_progress(1)

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        sample_size = 5

        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        
        table_desc = sample_df.to_csv(index=False, quoting=1)

        return table_desc, columns
    
    def run(self, extract_output, use_cache=True):
        table_desc, table_columns = extract_output

        template = f"""You have the following table:
{table_desc}
        
- Task: Summarize the table.
-  Highlight: Include and highlight ALL attributes as **Attribute**. 
-  Structure: Start with the big picture, then explain how attributes are related
-  Requirement: ALL attributes must be mentioned and **highlighted**. The attribute name should be exactly the same (case sensitive and no extra space or _).
-  Style: Use a few short sentences with very simple words.

Example: The table is about ... at **Time**, in **Location**...
Now, your summary:
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)

        summary = response['choices'][0]['message']['content']
        assistant_message = response['choices'][0]['message']
        messages.append(assistant_message)
        self.messages.append(messages)




        def extract_words_in_asterisks(text):
            import re

            pattern = r'\*\*(.*?)\*\*'

            matches = re.findall(pattern, text)

            return matches

        def find_extra_columns(text_columns, table_columns):
            text_columns_set = set(text_columns)
            table_columns_set = set(table_columns)

            if text_columns_set.issuperset(table_columns_set):
                extra_columns = text_columns_set - table_columns_set
                return list(extra_columns)
            else:
                raise ValueError(f"text_columns is not a superset of table_columns. text_columns: {text_columns}, table_columns: {table_columns}")

        def update_summary(summary, extra_columns):
            for column in extra_columns:
                summary = summary.replace(f"**{column}**", column)

            return summary

        

        return summary


    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output 
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        self.progress.value += 1

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)

        display(HTML(f"<b>Table Summary</b>:\n{replace_asterisks_with_tags(summary)}"))

        edit_button = widgets.Button(
            description='Edit',
            disabled=False,
            button_style='danger', 
            tooltip='Click to edit',
            icon='edit'
        )

        def on_edit_clicked(b):
            with self.output_context():
                print("Not implemented yet")

        edit_button.on_click(on_edit_clicked)

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                print("Submission received.")
                callback(summary)

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)

        display(HBox([edit_button, submit_button]))
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(submit_button)
            return

def find_duplicate_rows(con, table_name, columns=None, sample_size=0, with_context = ""):
    if isinstance(table_name, TransformationSQLPipeline):
        with_context = table_name.get_codes(mode="WITH")
        return find_duplicate_rows_result(con, table_name, sample_size=sample_size, with_context=with_context, columns=columns)
    else:
        return find_duplicate_rows_result(con, table_name, sample_size=sample_size, columns=columns)
        
class DecideDuplicate(Node):
    default_name = 'Decide Duplicate'
    default_description = 'This allows users to decide how to handle duplicated rows.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        display(HTML("🔍 Checking duplicated rows ..."))
        
        create_progress_bar_with_numbers(0, doc_steps)
        self.progress = show_progress(1)

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())

        database_name = get_database_name(con)
        
        if database_name == "SQL Server":
            for column_name, column_type in schema.items():
                if column_type.lower() in ['text', 'ntext', 'image']:
                    print(f"⚠️ The column {column_name} has type {column_type} that cannot be deduplicated. Skipping this step.")
                    document = {"duplicate_count": 0, "duplicate_removed": False}
                    callback(document)
                    return
                
        for column_name, column_type in schema.items():
            data_type = get_reverse_type(column_type, database_name)
            if not is_type_comparable(data_type):
                print(f"⚠️ The column {column_name} has type {data_type} that cannot be deduplicated. Skipping this step.")
                document = {"duplicate_count": 0, "duplicate_removed": False}
                callback(document)
                return
                
        try:
            duplicate_count, sample_duplicate_rows = find_duplicate_rows(con, table_pipeline, columns=columns)
        except Exception as e:
            if cocoon_main_setting['DEBUG_MODE']:
                raise e
            else:
                write_log(f"Erorr in finding duplicate rows: {e}")
                duplicate_count = 0
                

        document = {"duplicate_count": duplicate_count, 
                    "duplicate_removed": False}
        
        self.progress.value += 1

        if duplicate_count > 0:
            display_duplicated_rows_html2(sample_duplicate_rows)

            def on_button_clicked(b):
                with self.output_context():
                    clear_output(wait=True)
                    if b.description == 'Yes':
                        old_table_name = table_pipeline.__repr__(full=False)
                        new_table_name = old_table_name + "_dedup"
                        
                        sql_query = f'SELECT DISTINCT *'
                        sql_query = f"-- Deduplication: Removed {duplicate_count} duplicated rows\n" + sql_query
                        sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"      
                        step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=self.item["con"])
                        self.para["table_pipeline"].add_step_to_final(step)
                        database = self.para.get("database", None)
                        schema = self.para.get("schema", None)
                        
                        
                        try:
                            self.para["table_pipeline"].materialize(con, database=database, schema=schema)
                            document["duplicate_removed"] = True
                        except Exception as e:
                            if cocoon_main_setting['DEBUG_MODE']:
                                raise e
                            write_log(f"""
The node is {self.name}
Error in materialization!!
The error is: {e}
""")  
                            self.para["table_pipeline"].remove_final_node()
                    
                    callback(document)

            yes_button = widgets.Button(
                description='Yes',
                disabled=False,
                button_style='success',
                tooltip='Click to submit Yes',
                icon='check'
            )

            no_button = widgets.Button(
                description='No',
                disabled=False,
                button_style='danger',
                tooltip='Click to submit No',
                icon='times'
            )

            yes_button.on_click(on_button_clicked)
            no_button.on_click(on_button_clicked)

            display(HBox([no_button, yes_button]))

            if self.viewer or ("viewer" in self.para and self.para["viewer"]):
                
                if (hasattr(self, 'para') and 
                    isinstance(self.para, dict) and 
                    isinstance(self.para.get('cocoon_stage_options'), dict) and 
                    self.para['cocoon_stage_options'].get('deduplication') is False):
                    on_button_clicked(no_button)
                    return

                on_button_clicked(yes_button)  
                return       

        else:
            callback(document)   




def get_missing_percentage(con, table_name, column_name, with_context=""):
    table_name_str = enclose_table_name(table_name, con=con)
    query = f"""
    SELECT COUNT(*) as total_count, COUNT("{column_name}") as non_missing_count
    FROM {table_name_str}
    """
    
    query = with_context + "\n" + query
    
    result = run_sql_return_df(con, query)
    total_count = result.iloc[0, 0]
    non_missing_count = result.iloc[0, 1]
    
    return (total_count - non_missing_count) / total_count



class DecideRegex(Node):
    default_name = 'Decide Regex'
    default_description = 'This node allows users to decide the regex pattern for a string column.'
    max_iterations = 10
    default_sample_size = 100

    def extract(self, item):
        clear_output(wait=True)

        self.input_item = item

        idx = self.para["column_idx"]
        total = self.para["total_columns"]
        
        
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        
        column_name = self.para["column_name"]
        with_context = table_pipeline.get_codes(mode="WITH")
        
        display(HTML(f"{running_spinner_html} Reading regex pattern for <i>{table_name}[{column_name}]</i>..."))
        create_progress_bar_with_numbers(2, doc_steps)
        show_progress(max_value=total, value=idx)

        return column_name, table_pipeline, with_context
    
    def run(self, extract_output, use_cache=True):
        column_name, table_pipeline, with_context = extract_output
        
        max_iterations = self.class_para.get("max_iterations", self.max_iterations)

        regex_list = []
        results = []
        con = self.item["con"]
        sample_size = self.class_para.get("sample_size", self.default_sample_size)
        
        last_regex_list_size = -1
        
        i = 0
        for i in range(max_iterations):
            sample_values = create_sample_distinct_query_regex(con, table_pipeline, column_name, 
                                                sample_size, regex_except_list=regex_list, with_context=with_context)
            
            if len(sample_values) == 0:
                break
            
            if len(regex_list) == last_regex_list_size:
                use_cache = False
            else:
                use_cache = True
                
            last_regex_list_size = len(regex_list)
            
            value_list = sample_values[column_name].tolist()
            truncated_list = [truncate_value(val) for val in value_list]
            sample_values_list_str = f"[{', '.join(truncated_list)}]"
            
            
            
            template = f"""{column_name}' has the following distinct values: {sample_values_list_str}

Are there are common regular expression pattern, or just free texts (.*)
E.g., Date string "(1'2'1994)" has regex: \(\d{{1,2}}'\d{{1,2}}'\d{{4}}\)
Note to escape for special characters, e.g., \. for dot, \( for (, etc.
For unicode characters, include them directly, not as \\u. e.g.: [À-ÖØ-öø-ÿ]+
Some columns may have multiple patterns. Please provide all the categories, from strict to loose.

Now, return in the following format:
```yml
reasoning: >
    This column contains free texts/obvious regex patterns
patterns: # leave empty if free texts
    -   summary: >
            Date values in the format of (1'2'1994)
        regex: >-
            \(\d{{1,2}}'\d{{1,2}}'\d{{4}}\)
    - ...
```"""

            messages = [{"role": "user", "content": template}]
            response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
            messages.append(response['choices'][0]['message'])
            self.messages.append(messages)

            yml_code = extract_yml_code(response['choices'][0]['message']["content"])
            summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
            
            if "patterns" not in summary or summary["patterns"] is None or len(summary["patterns"]) == 0:
                summary["patterns"] = []
                break
            
            def clean_regex(regex):
                regex = escape_value_single_quotes(regex, con)
                return regex
            
            
            def has_non_capturing_groups(regex):
                pattern = r'\(\?:'
                return bool(re.search(pattern, regex))




            summary["patterns"] = [{"summary": clean_summary(pattern["summary"]), "regex": clean_regex(pattern["regex"])} for pattern in summary["patterns"]]
            database_name = get_database_name(con)
            for pattern in summary.get("patterns", []):
                if pattern["regex"] == ".*":
                    continue
                
                if database_name == "Snowflake" and has_non_capturing_groups(pattern["regex"]):
                    continue
                
                query = f'SELECT COUNT(*) FROM {table_pipeline} WHERE '
                query +=  create_regex_match_clause(con, column_name, pattern['regex']) + "\n"
                for regex in regex_list:
                    regex_match_clause = create_regex_match_clause(con, column_name, regex)
                    query += f"AND NOT {regex_match_clause} \n"
                
                query = with_context + "\n" + query
                try:
                    result_df = run_sql_return_df(con,query)
                except:
                    continue
                
                if result_df.iloc[0,0] == 0:
                    continue
                
                regex_list.append(pattern['regex'])
                results.append(pattern)
        
        if i == (max_iterations -1):
            return self.run_but_fail(extract_output)
        
        return results
    
    def run_but_fail(self, extract_output, use_cache=True):
        return []
    

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        column_name, table_name, with_context = extract_output
        table_object = self.para["table_object"]
        table_object.patterns[column_name] = run_output
        
        callback(run_output)

class DecideRegexForAll(MultipleNode):
    default_name = 'Decide Regex For All'
    default_description = 'This node allows users to decide the regex pattern for all string columns.'
    default_distinct_threshold = 100

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        node = DecideRegex(para=para, id_para ="column_name")
        node.inherit(self)
        return node

    def extract(self, item):
        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        database_name = get_database_name(con)
        
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        
        self.elements = []
        self.nodes = {}

        if database_name not in ["Snowflake", "DuckDB"]:
            return
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('detect_regex_patterns') is False):
            return
        
        with_context = table_pipeline.get_codes(mode="WITH")
        
        sql_query = generate_count_distinct_query(table_pipeline, columns, con=con, ratio=False)
        sql_query = with_context + "\n" + sql_query
        distinct_count_df = run_sql_return_df(con, sql_query)
        
        distinct_threshold = self.class_para.get("distinct_threshold", self.default_distinct_threshold)

        for col in columns:
            col_distinct_count = distinct_count_df[col].iloc[0]
            
            if is_type_string(get_reverse_type(schema[col], database_name)) and col_distinct_count > distinct_threshold:
                self.elements.append(col)

        self.nodes = {col: self.construct_node(col, idx, len(self.elements)) for idx, col in enumerate(self.elements)}

    def display_after_finish_workflow(self, callback, document):
        callback(document)
        
        
        
class DecideUnusual(Node):
    default_name = 'Decide Unusual'
    default_description = 'This node allows users to decide how to handle unusual values.'

    def extract(self, input_item):
        clear_output(wait=True)

        idx = self.para["column_idx"]
        total = self.para["total_columns"]
        
        self.input_item = input_item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        column_name = self.para["column_name"]
        
        display(HTML(f"{running_spinner_html} Understanding unusual values for <i>{table_pipeline}[{column_name}]</i>"))
        create_progress_bar_with_numbers(3, doc_steps)
        show_progress(max_value=total, value=idx)
        
        sample_size = 20

        query = create_sample_distinct_query(con, table_pipeline, column_name, sample_size)
        with_context = table_pipeline.get_codes(mode="WITH")
        query = with_context + "\n" + query
        sample_values = run_sql_return_df(con,query)
        sample_values = sample_values.applymap(truncate_cell)
        
        return column_name, sample_values

    def run(self, extract_output, use_cache=True):
        column_name, sample_values = extract_output

        template = f"""{column_name} has the following distinct values:
{sample_values.to_csv(index=False, header=False, quoting=1, quotechar="'")}

Review if there are any unusual values. Look out for:
1. Values too large/small that are inconsistent with the context.
E.g., age 999 or -5.
Outlier is fine as long as it falls in a reasonable range, e.g., person age 120 is fine.
2. Patterns that don't align with the nature of the data.
E.g., age 10.22
3. Special characters that don't fit within the real-world domain.
E.g., age X21b 

Now, respond in Json:
```json
{{
    "Reasoning": "The valuses are ... They are unusual/acceptable ...",
    "Unusualness": true/false,
    "Explanation": "xxx values are unusual because ..." # if unusual, short in 10 words
}}
```"""
        
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)

        return json_code
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)


class DecideUnusualForAll(MultipleNode):
    default_name = 'Decide Unusual For All'
    default_description = 'This node allows users to decide how to handle unusual values for all columns.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        node = DecideUnusual(para=para, id_para ="column_name")
        node.inherit(self)
        return node

    def extract(self, item):
        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        
        schema = table_pipeline.get_schema(con)
        columns = [col for col in schema if is_type_string(schema[col])]
        self.elements = []
        self.nodes = {}

        idx = 0
        for col in columns:
            self.elements.append(col)
            self.nodes[col] = self.construct_node(col, idx, len(columns))
            idx += 1

    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        create_progress_bar_with_numbers(3, doc_steps)
        
        data = []
        if "Decide Unusual" in document:
            for key in document["Decide Unusual"]:
                if document["Decide Unusual"][key]["Unusualness"]:
                    data.append([key, document["Decide Unusual"][key]["Explanation"], True])
        

        
        df = pd.DataFrame(data, columns=["Column", "Explanation", "Endorse"])
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        
        print("The following columns have unusual values: ❓")
        display(grid)
      
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                document = new_df.to_json(orient="split")
                callback(document)
        
        next_button.on_click(on_button_clicked)

        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
class DecideLongitudeLatitude(Node):
    default_name = 'Decide Longitude Latitude'
    default_description = 'This node allows users to decide the longitude and latitude columns.'

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Deciding longitude and latitude columns ...")
        create_progress_bar_with_numbers(2, doc_steps)

        self.input_item = item

        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        
        sample_size = 5
        con = self.item["con"]
        all_columns = [enclose_table_name(col, con=con) for col in schema if schema[col] in data_types['INT'] or schema[col] in data_types['DECIMAL']]
        query = f'SELECT {", ".join(all_columns)} FROM {table_pipeline}'
        sample_sql_query = sample_query(con, query, sample_size)
        sample_df = run_sql_return_df(con, sample_sql_query)
        sample_df = sample_df.applymap(truncate_cell)
        table_desc = sample_df.to_csv(index=False, quoting=1)
        
        return table_desc
    
    def run(self, extract_output, use_cache=True):
        table_desc = extract_output

        template = f"""You have the following table:
{table_desc}

Task: Identify the pairs of longitude/latitude attribute names (case sensitive), if any.
Respond in JSON format:
```json
[["longitude", "latitude"],...]
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)

        return json_code
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if not isinstance(run_output, list):
            run_output = []

        cleaned_output = [pair for pair in run_output if isinstance(pair, list) and len(pair) == 2]

        callback(cleaned_output)


class DecideColumnRange(Node):
    default_name = 'Decide Column Range'
    default_description = 'This node allows users to decide the range of numerical columns.'

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Deciding column range ...")
        create_progress_bar_with_numbers(2, doc_steps)

        self.input_item = item

        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        sample_size = 5
        
        
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        table_desc = sample_df.to_csv(index=False, quoting=1)
        database_name = get_database_name(con)
        
        numerical_columns = [col for col in schema if get_reverse_type(schema[col], database_name) in ['INT', 'DECIMAL']]
        
        min_max = {}
        for col in numerical_columns:
            min_max_tuple = run_sql_return_df(con, f"SELECT MIN({col}) as MIN, MAX({col}) as MAX FROM {table_pipeline}").iloc[0]
            min_max[col] = [min_max_tuple["MIN"], min_max_tuple["MAX"]]
        
        return table_desc, numerical_columns, min_max
    
    def run(self, extract_output, use_cache=True):
        table_desc, numerical_columns, min_max = extract_output
        
        if len(numerical_columns) == 0:
            return {}

        template = f"""You have the following table:
{table_desc}

The numerical columns in the table are: {numerical_columns}
Task: based on the understanding of the data, decide the normal range for numerical columns.
E.g., for a column "age", the normal range could be 0-150.
Skip the column if the range is unclear.

Now, return in the following format:
```json
{{
    "{numerical_columns[0]}": {{
        "explanation": "short in < 10 words",
        "min": 0,
        "max": 150
    }}, ...
}}
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: all(isinstance(key, str) for key in jc.keys()), "Not all keys in the dictionary are strings."),
            (lambda jc: all(key in numerical_columns for key in jc.keys()), "The dictionary contains keys that are not in the numerical_columns list."),
            (lambda jc: all(isinstance(value, dict) for value in jc.values()), "Not all values in the dictionary are dictionaries."),
            (lambda jc: all("explanation" in value and "min" in value and "max" in value for value in jc.values()), "Not all inner dictionaries contain 'explanation', 'min', and 'max' keys."),
            (lambda jc: all(isinstance(value["explanation"], str) for value in jc.values()), "Not all 'explanation' values are strings."),
            (lambda jc: all(isinstance(value["min"], (int, float)) for value in jc.values()), "Not all 'min' values are numbers (int or float)."),
            (lambda jc: all(isinstance(value["max"], (int, float)) for value in jc.values()), "Not all 'max' values are numbers (int or float)."),
            (lambda jc: all(value["min"] <= value["max"] for value in jc.values()), "Not all 'min' values are less than or equal to their corresponding 'max' values."),
        ]
        
        for check, error_message in checks:
            if not check(json_code):
                raise ValueError(f"Validation failed: {error_message}")

        
        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        _, _, min_max = extract_output

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)

        rows_list = []

        for col in schema:
            if col in json_code:
                rows_list.append({
                    "Column": col,
                    "True Range": min_max[col],
                    "Expected Range": [json_code[col]["min"],json_code[col]["max"]],
                    "Explanation": json_code[col]["explanation"],
                    "Within Range?": "❌ No" if json_code[col]["min"] > min_max[col][0] or json_code[col]["max"] < min_max[col][1] else "✔️ Yes",
                })

        df = pd.DataFrame(rows_list)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, False, False, True, False]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        print("😎 We have decided the column range:")
        display(grid)

        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  

        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                document = new_df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_clicked)

        display(next_button)
        
        if viewer:
            on_button_clicked(next_button)

class DecideMissing(Node):
    default_name = 'Decide Missing Values'
    default_description = 'This node allows users to decide how to handle missing values.'

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Checking missing values ...")
        create_progress_bar_with_numbers(2, doc_steps)
        
        self.progress = show_progress(1)

        self.input_item = item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.get_final_step()

        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        sample_size = 5
        
        with_context = table_pipeline.get_codes(mode="WITH")

        missing_columns = {}

        for col in columns:
            missing_percentage = get_missing_percentage(con, table_name, col, with_context=with_context)
            if missing_percentage > 0:
                missing_columns[col] = missing_percentage

        sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        sample_df_str = sample_df.to_csv(index=False, quoting=1)    

        return missing_columns, sample_df_str, table_name

    def run(self, extract_output, use_cache=True):
        
        missing_columns, sample_df_str, table_name = extract_output
        
        if len(missing_columns) == 0:
            return {"reasoning": "No missing values found.", "columns_obvious_not_applicable": {}}

        missing_desc = "\n".join([f'{idx+1}. {col}: {missing_columns[col]}' for idx, (col, desc) in enumerate(missing_columns.items())])

        template = f"""You have the following table:
{sample_df_str}

The following columns have percentage of missing values:
{missing_desc}

For each column, decide whether there is an obvious reason for the missing values to be "not applicable"
E.g., if the column is 'Date of Death', it is "not applicable" for patients still alive.
Reasons like not not collected, sensitive info, encryted, data corruption, etc. are not considered 'not applicable'.

Return in the following format:
```json
{{
    "reasoning": "X column has an obvious not applicable reason... The rest don't.",
    "columns_obvious_not_applicable": {{
        "column_name": "Short specific reason why not applicable, in < 10 words.",
        ...
    }}
}}
"""
            
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)

        if not isinstance(json_code, dict) or "reasoning" not in json_code or "columns_obvious_not_applicable" not in json_code:
            raise ValueError("The returned JSON code does not adhere to the required format.")
        
        
        json_code["columns_obvious_not_applicable"] = [
            col_name for col_name in json_code["columns_obvious_not_applicable"]
            if col_name in missing_columns
        ]

        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        missing_columns, sample_df_str, table_name = extract_output
        
        if len(missing_columns) == 0:
            return {"reasoning": "No missing values found. ", "columns_obvious_not_applicable": {}}
        else:
            return {
                "reasoning": "Failed to analyze the data due to an error.",
                "columns_obvious_not_applicable": {}
            }

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        self.progress.value += 1
        json_code = run_output
        missing_columns, sample_df_str, table_name = extract_output
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)

        rows_list = []
        for col in missing_columns:
            missing_percentage = missing_columns[col]
            reason = json_code["columns_obvious_not_applicable"].get(col, "")
            rows_list.append({
                "Column": col,
                "NULL (%)": f"{missing_percentage*100:.2f}",
                "Is NULL Acceptable?": True if reason != "" else False,
                "Explanation": reason
            })
            
        df = pd.DataFrame(rows_list)
        
        if len(rows_list) == 0:
            df = pd.DataFrame(columns=["Column", "NULL (%)", "Is NULL Acceptable?", "Explanation"])
        else:
            df = pd.DataFrame(rows_list)

        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, False, True, True]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        
        print("😎 We have identified missing values, and potential causes:")
        display(grid)
        
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                document = new_df.to_json(orient="split")

                callback(document)
        
        next_button.on_click(on_button_clicked)

        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
class DecideDMV(Node):
    default_name = 'Decide Disguised Missing Values'
    default_description = 'This node allows users to decide how to handle disguised missing values.'

    def extract(self, input_item):
        clear_output(wait=True)

        idx = self.para["column_idx"]
        total = self.para["total_columns"]

        self.input_item = input_item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        column_name = self.para["column_name"]
        
        display(HTML(f"{running_spinner_html} Understanding disguised missing values <i>{table_pipeline}[{column_name}]</i>"))
        create_progress_bar_with_numbers(3, doc_steps)
        show_progress(max_value=total, value=idx)
        
        sample_size = 20

        query = create_sample_distinct_query(con, table_pipeline, column_name, sample_size)
        with_context = table_pipeline.get_codes(mode="WITH")
        query = with_context + "\n" + query
        sample_values = run_sql_return_df(con,query)
        
        return column_name, sample_values

    def run(self, extract_output, use_cache=True):
        column_name, sample_values = extract_output
        
        if len(sample_values) == 0:
            return {"reasoning": "Column is fully missing", "null_values": []}
        
        sample_values_list = sample_values[column_name].values.tolist()
        truncated_list = [truncate_value(val) for val in sample_values_list]
        sample_values_list_str = f"[{', '.join(truncated_list)}]"

        template = f"""Column {column_name} has {len(sample_values_list)} distinct values: {sample_values_list_str}

From these {len(sample_values_list)} values, is there any values that semantically mean "missing"?
E.g., Values like "missing", "empty", "not available" or empty string ("").
Note that some values are simply typos. Disregard them.

Return in json format:
```json
{{
    "reasoning": "xxx values are disguised missing values because ...",
    "null_values": ["N/A", "-1", ...] (empty if no values mean missing)
}}
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "reasoning" in jc and isinstance(jc["reasoning"], str), "The 'reasoning' key is missing or its value is not a string."),
            (lambda jc: "null_values" in jc and isinstance(jc["null_values"], list), "The 'DMV' key is missing or its value is not a list."),
            (lambda jc: all(dmv in sample_values_list for dmv in jc["null_values"]), f"Some DMV values are not present in the sample_values. sample_values: {sample_values_list}, DMV: {json_code['null_values']}"),    
        ]

        for check, error_message in checks:
            if not check(json_code):
                raise ValueError(f"Validation failed: {error_message}")
        
        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"reasoning": "No disguised missing values found.", "null_values": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)


class DecideDMVforAll(MultipleNode):
    default_name = 'Decide Disguised Missing Values For All'
    default_description = 'This node allows users to decide how to handle disguised missing values for all columns.'
    
    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        node = DecideDMV(para=para, id_para ="column_name")
        node.inherit(self)
        return node
    
    def extract(self, item):
        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        database_name = get_database_name(con)
        
        schema = table_pipeline.get_schema(con)
        
        columns = []
        for col, col_type in schema.items():
            reverse_type = get_reverse_type(col_type, database_name)
            if is_type_string(reverse_type):
                if database_name == "SQL Server":
                    if col_type.lower() not in ['text', 'ntext', 'image']:
                        columns.append(col)
                else:
                    columns.append(col)

        self.elements = []
        self.nodes = {}
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('detect_disguised_missing_values') is False):
            return
        
        idx = 0
        for col in columns:
            self.elements.append(col)
            self.nodes[col] = self.construct_node(col, idx, len(columns))
            idx += 1
            
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        
        data = []
        if "Decide Disguised Missing Values" in document:
            for key in document["Decide Disguised Missing Values"]:
                if document["Decide Disguised Missing Values"][key]["null_values"]:
                    data.append([key, document["Decide Disguised Missing Values"][key]["null_values"], 
                                True,
                                ",".join(document["Decide Disguised Missing Values"][key]["null_values"])])
        
        df = pd.DataFrame(data, columns=["Column", "Disguised Missing Values", "Impute to NULL?", "Values to NULL (Sep By ,)"])
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True, True]
        lists = ["Disguised Missing Values"]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset, lists=lists)
        
        next_button = widgets.Button(
            description='Accept to NULL',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        reject_button = widgets.Button(
            description='Reject',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
            
        def on_button_clicked2(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, reset=reset, lists=lists)
                new_df["Impute to NULL?"] = False
                document = new_df.to_json(orient="split")
                callback(document)
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, reset=reset, lists=lists)
                
                has_non_empty_values = new_df["Impute to NULL?"].any()
                
                if has_non_empty_values:
                    table_pipeline = self.para["table_pipeline"]
                    old_table_name = table_pipeline.__repr__(full=False)
                    new_table_name = old_table_name + "_null"
                    comment = "-- NULL Imputation: Impute Null to Disguised Missing Values\n"
                    selection_clauses = []

                    con = self.item["con"]
                    schema = table_pipeline.get_schema(con)
                    columns = set(schema.keys())

                    for index, row in new_df.iterrows():
                        if not row["Impute to NULL?"]:
                            continue

                        col = row["Column"]

                        columns.remove(col)

                        to_remove_values = row["Values to NULL (Sep By ,)"]

                        to_remove_values = to_remove_values.split(",")

                        selection_str = "CASE\n"
                        for to_remove_value in to_remove_values:
                            to_remove_value = escape_value_single_quotes(to_remove_value, con)
                            selection_str += f'    WHEN {enclose_table_name(col, con=con)} = \'{to_remove_value}\' THEN NULL\n'

                        comment += f"-- {col}: {to_remove_values}\n"
                        selection_str += f'    ELSE {enclose_table_name(col, con=con)}\n'
                        selection_str += f'END AS {enclose_table_name(col, con=con)}'
                        selection_clauses.append(selection_str)

                    for col in columns:
                        selection_clauses.append(enclose_table_name(col, con=con))

                    selection_clause = ',\n'.join(selection_clauses)
                    sql_query = f'SELECT \n{indent_paragraph(selection_clause)}'
                    sql_query = comment + sql_query
                    sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"
                    con = self.item["con"]
                    step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con)
                    table_pipeline.add_step_to_final(step)
                    database = self.para.get("database", None)
                    schema = self.para.get("schema", None)
                    try:
                        self.para["table_pipeline"].materialize(con, database=database, schema=schema)
                    except Exception as e:
                        if cocoon_main_setting['DEBUG_MODE']:
                            raise e
                        write_log(f"""
The node is {self.name}
Error in materialization!!
The error is: {e}
""")  
                        self.para["table_pipeline"].remove_final_node()
                        
                document = new_df.to_json(orient="split")
                callback(document)
        
        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return

        create_progress_bar_with_numbers(2, doc_steps)
        display(HTML(f"""<b>😎 We have identified disguised missing values (DMV). Please review!</b>
<div><em>⚠️ Empty DMV is because of empty string.</em></div>"""))
        display(grid)
        display(HBox([reject_button,next_button]))

    

def check_column_uniqueness(con, table_name, column_name, allow_null=False):
    distinct_count, total_count = compute_unique_ratio(con, table_name, column_name, allow_null=allow_null)
    return distinct_count == total_count


class DecideUnique(ListNode):
    default_name = 'Decide Unique Columns'
    default_description = 'This node allows users to decide which columns should be unique.'
    

    def extract(self, item):
        clear_output(wait=True)
        
        table_pipeline = self.para["table_pipeline"]
        table_object = self.para["table_object"]
        table_name = table_pipeline.__repr__(full=False)
        with_context = table_pipeline.get_codes(mode="WITH")


        display(HTML(f"{running_spinner_html} Checking uniqueness for <i>{table_name}</i>..."))
        create_progress_bar_with_numbers(3, doc_steps)
        self.progress = show_progress(1)

        con = self.item["con"]

        schema = table_pipeline.get_schema(con)
        
        columns = []
        database_name = get_database_name(con)
        for col, col_type in schema.items():
            if database_name == "SQL Server":
                if col_type.lower() not in ['text', 'ntext', 'image']:
                    columns.append(col)
            else:
                data_type = get_reverse_type(col_type, database_name)
                if is_type_comparable(data_type):
                    columns.append(col)
        
        sample_size = 5

        unique_columns = []
        highly_unique_columns = []
        column_descs = {}
        
        for col in columns:
            distinct_count, total_count = compute_unique_ratio(con, table_pipeline, col, allow_null=True, with_context=with_context)
            if total_count  == 0:
                continue
            if distinct_count/total_count > 0.9:
                highly_unique_columns.append(col)
                column_desc = table_object.column_desc.get(col, "")
                column_descs[col] = f"{col}: {column_desc}"
                
            if distinct_count == total_count:            
                unique_columns.append(col)
        
        table_desc = table_object.table_summary
        
        if len(columns) > 0:
            sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
            sample_df = sample_df.applymap(truncate_cell)
            sample_df_str = sample_df.to_csv(index=False, quoting=1)
        else:
            sample_df_str = ""

        outputs = []
        
        for i in range(0, len(highly_unique_columns), 10):
            outputs.append((unique_columns, column_descs, table_desc, sample_df_str, table_name, columns, highly_unique_columns[i:i+10]))
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('unique_test') is False):
            outputs = []
        
        if len(outputs) == 0:
            outputs.append((unique_columns, column_descs, table_desc, sample_df_str, table_name, columns, []))
        
        return outputs

    def run(self, extract_output, use_cache=True):
        
        unique_columns, column_descs, table_desc, sample_df_str, table_name, columns, highly_unique_columns = extract_output
        
        if len(highly_unique_columns) == 0:
            return {"reasoning": "No single column that can be candidate key.", 
                    "continuous_columns": [],
                    "not_continuous_columns": {}}
        
        column_descs_str = "\n".join([column_descs[col] for col in highly_unique_columns])


        template = f"""You have the following table: {table_desc}
        
Samples:
{sample_df_str}

The following columns may be candidate key: {highly_unique_columns}
{column_descs_str}

Identify the column that can actually be candidate key.
First, identify continuous columns can't be candidate key (e.g., temperature)
Then, walk through each remaining key.
E.g., for a table of customer, "customer_id" is can be candidate key.
But "first_name" is not always, as there can be multiple customers with the same first name.
"region_id" is not the candidate key for customer table, where each row is a customer, because multiple customers (rows) can be in the same region.
But "region_id" could be the candidate key for region table.

Return in the following format:
```yml
continuous_columns: ['col1', ...]
not_continuous_columns: 
    {highly_unique_columns[0]}: 
        reasoning: this columns means... For this table, each row is for ... {highly_unique_columns[0]} is (not) unique across rows.
        candidate_key: true/false
...
```"""
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "continuous_columns" in jc, "The 'continuous_columns' key is missing."),
            (lambda jc: "not_continuous_columns" in jc, "The 'not_continuous_columns' key is missing."),
            (lambda jc: isinstance(jc["continuous_columns"], list), "The value of 'continuous_columns' is not a list."),
            (lambda jc: isinstance(jc["not_continuous_columns"], dict) or not jc["not_continuous_columns"], 
     "The value of 'not_continuous_columns' is neither a dictionary nor empty."),
        ]

        for check, error_message in checks:
            if not check(summary):
                raise ValueError(f"Validation failed: {error_message}")

        return summary
    
    def merge_run_output(self, run_outputs):
        continuous_columns = []
        not_continuous_columns = {}
        for run_output in run_outputs:
            continuous_columns.extend(run_output["continuous_columns"])
            not_continuous_columns.update(run_output["not_continuous_columns"])
        return {"continuous_columns": continuous_columns, "not_continuous_columns": not_continuous_columns}
    
    def run_but_fail(self, extract_output, use_cache=True):
        unique_columns, column_descs, table_desc, sample_df_str, table_name, columns, highly_unique_columns = extract_output
        return {"continuous_columns": [],
                "not_continuous_columns": {col: 
                    {"reasoning": "Fail to run",
                      "candidate_key": False} for col in highly_unique_columns}}

    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        self.progress.value += 1
        json_code = run_output
        
        unique_columns, column_descs, table_desc, sample_df_str, table_name, columns, highly_unique_columns = extract_output[0]
        
        for eo in extract_output[1:]:
            _, _, _, _, _, _, huc = eo
            highly_unique_columns.extend(huc)
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        
        
        rows_list = []
        for col in highly_unique_columns:
            is_unique = (col in unique_columns)
            
            if col not in json_code["not_continuous_columns"]:
                reason = "This column contains continuous values"
                should_unique = False
            else:       
                reason = json_code["not_continuous_columns"][col].get("reasoning", "")
                should_unique = json_code["not_continuous_columns"][col].get("candidate_key", False)
            
            rows_list.append({
                "Column": col,
                "Is Unique?": is_unique,
                "Should Unique?": should_unique,
                "Explanation": reason
            })
        
        df = pd.DataFrame(rows_list)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, False, True, True]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        
        next_button = widgets.Button(
            description='Accept Unique',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        reject_button = widgets.Button(
            description='Reject Unique',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='times'
        )
        
        def on_button_clicked2(b):
            with self.output_context():
                clear_output(wait=True)
                new_df =  grid_to_updated_dataframe(grid)
                new_df["Should Unique?"] = False
                callback(new_df.to_json(orient="split"))
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                document = new_df.to_json(orient="split")
                
                table_object = self.para["table_object"]
                
                for index, row in new_df.iterrows():
                    table_object.uniqueness[row["Column"]] = {"current_unique": False}
                    if row["Is Unique?"]:
                        table_object.uniqueness[row["Column"]]["current_unique"] = True
                    if row["Should Unique?"]:
                        table_object.uniqueness[row["Column"]]["unique_reason"] = row["Explanation"]
                        
                callback(document)
        
        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
        display(HTML(f"😎 Please verify unique columns. We will add unique to column tests:"))
        display(grid)
        display(HBox([reject_button, next_button]))
        

def create_profile_workflow(table_name, con, viewer=True):
        
    query_widget = QueryWidget(con)

    item = {
        "con": con,
        "query_widget": query_widget
    }

    sql_step = SQLStep(table_name=table_name, con=con)
    pipeline = TransformationSQLPipeline(steps = [sql_step], edges=[])

    para = {"table_pipeline": pipeline, 
            "viewer": viewer,
            "table_object": Table()}

    main_workflow = Workflow("Data Profiling", 
                            item = item, 
                            description="A workflow to profile dataset",
                            viewer=viewer,
                            para = para)

    main_workflow.add_to_leaf(DecideProjection(viewer=True))
    main_workflow.add_to_leaf(DecideDuplicate(viewer=True))
    main_workflow.add_to_leaf(CreateTableSummary(viewer=True))
    main_workflow.add_to_leaf(DescribeColumns(viewer=True))
    main_workflow.add_to_leaf(CreateColumnGrouping(viewer=True))
    main_workflow.add_to_leaf(DecideDataType(viewer=True))
    main_workflow.add_to_leaf(DecideMissing(viewer=True))
    main_workflow.add_to_leaf(DecideUnique(viewer=True))
    main_workflow.add_to_leaf(DecideUnusualForAll(viewer=True))
    main_workflow.add_to_leaf(DecideColumnRange(viewer=True))
    main_workflow.add_to_leaf(DecideLongitudeLatitude(viewer=True))
    main_workflow.add_to_leaf(GenerateProfileReport(viewer=True))
    
    return query_widget, main_workflow



def truncate_text_vectorized(df, max_length=100, col_max_length=10):
    def truncate_series(series):
        mask = series.str.len() > max_length
        truncated = series[mask].str.slice(0, max_length//2) + '...' + series[mask].str.slice(-max_length//2)
        series[mask] = truncated
        return series

    for column in df.select_dtypes(include=[object]).columns:
        df[column] = truncate_series(df[column])

    new_columns = []
    for column in df.columns:
        if len(column) > col_max_length:
            half_len = col_max_length // 2
            new_column = column[:half_len] + '_' + column[-half_len:]
        else:
            new_column = column
        new_columns.append(new_column)
    df.columns = new_columns
    
    return df



def create_select_html_from_list(list_of_values):
    options = ''.join(f"<option value='{item}'>{item}</option>" for item in list_of_values)
    dropdown_html = f"<select>{options}</select>"
    return dropdown_html

def replace_df_html_col_with_dropdown(df, df_html, col_name):
    soup = BeautifulSoup(df_html, 'html.parser')

    for idx, row in df.iterrows():
        dropdown_html = create_select_html_from_list(row[col_name])
        soup.find_all('td')[idx*len(df.columns) + df.columns.get_loc(col_name)].string.replace_with(BeautifulSoup(dropdown_html, 'html.parser'))

    modified_html = str(soup)
    return modified_html





def build_histogram_inputs(con, column, tablename):
    num_bins = 20
    query_min_max = f'SELECT MIN("{column}") AS min_val, MAX("{column}") AS max_val FROM {tablename} WHERE "{column}" IS NOT NULL;'
    
    if isinstance(tablename, TransformationSQLPipeline):
        with_context = tablename.get_codes(mode="WITH")
        query_min_max = with_context + "\n" + query_min_max
    
    result_df = run_sql_return_df(con, query_min_max)

    if result_df.empty or result_df.iloc[0].isnull().any():
        return [], 0, []

    min_val, max_val = result_df.iloc[0]

    if min_val == max_val:
        total_count_query = f'SELECT COUNT(*) FROM {tablename} WHERE "{column}" IS NOT NULL;'
        if isinstance(tablename, TransformationSQLPipeline):
            with_context = tablename.get_codes(mode="WITH")
            total_count_query = with_context + "\n" + total_count_query
        total_count = run_sql_return_df(con, total_count_query).iloc[0][0]
        return [total_count], min_val, [min_val]

    bin_width = (max_val - min_val) / num_bins
    min_val = float(min_val)
    max_val = float(max_val)
    bin_edges = np.linspace(min_val, max_val, num_bins + 1)

    case_statements = []
    for i in range(num_bins):
        bin_start = bin_edges[i]
        bin_end = bin_edges[i + 1]
        if i == num_bins - 1:
            case_statement = f'WHEN "{column}" >= {bin_start} AND "{column}" <= {bin_end} THEN {i+1}'
        else:
            case_statement = f'WHEN "{column}" >= {bin_start} AND "{column}" < {bin_end} THEN {i+1}'
        case_statements.append(case_statement)

    case_statement = ' '.join(case_statements)

    query = f'''
    BINS AS (
        SELECT CAST(ROW_NUMBER() OVER (ORDER BY NULL) AS INTEGER) AS BIN
        FROM {tablename}
        LIMIT {num_bins}
    )
    SELECT
        BINS.BIN,
        COALESCE(counts.count, 0) AS COUNT
    FROM BINS
    LEFT JOIN (
        SELECT
            CASE {case_statement} ELSE NULL END AS BIN,
            COUNT(*) AS COUNT
        FROM {tablename}
        WHERE "{column}" IS NOT NULL
        GROUP BY BIN
    ) COUNTS ON BINS.BIN = COUNTS.BIN
    ORDER BY BINS.BIN;
    '''
    
    if isinstance(tablename, TransformationSQLPipeline):
        with_context = tablename.get_codes(mode="WITH")
        if with_context.strip() == "":
            query = "WITH " + query
        else:
            query = with_context + ",\n" + query
    else:
        query = "WITH " + query
            
    result_df = run_sql_return_df(con, query)

    counts = result_df['COUNT'].tolist()
    bin_centers = [(bin_edges[i] + bin_edges[i+1]) / 2 for i in range(num_bins)]

    return counts, bin_width, bin_centers


def df_to_list(df):
    return [tuple(row) for row in df.itertuples(index=False, name=None)]


def build_barchat_input(con, column, tablename, limit=6):
    
    count_query = f'SELECT COUNT(DISTINCT "{column}") AS distinct_count FROM {tablename} WHERE "{column}" IS NOT NULL'
    
    if isinstance(tablename, TransformationSQLPipeline):
        with_context = tablename.get_codes(mode="WITH")
        count_query = with_context + "\n" + count_query
    
    distinct_count_result = run_sql_return_df(con, count_query).iloc[0][0]

    if distinct_count_result > limit:
        fetch_query = f'''
        CityCounts AS (
            SELECT CAST("{column}" AS VARCHAR) AS "{column}", COUNT(*) AS count
            FROM {tablename}
            WHERE "{column}" IS NOT NULL
            GROUP BY "{column}"
            ORDER BY count DESC
        ),
        TopCities AS (
            SELECT "{column}", count
            FROM CityCounts
            LIMIT {limit-1}
        ),
        OtherGroup AS (
            SELECT 'Other values' AS "{column}", SUM(count) AS count
            FROM CityCounts
            WHERE "{column}" NOT IN (SELECT "{column}" FROM TopCities) AND "{column}" IS NOT NULL
        )
        SELECT "{column}", count FROM TopCities
        UNION ALL
        SELECT * FROM OtherGroup
        '''
        
        if isinstance(tablename, TransformationSQLPipeline):
            with_context = tablename.get_codes(mode="WITH")
            if with_context.strip() == "":
                fetch_query = "WITH " + fetch_query
            else:
                fetch_query = with_context + ",\n" + fetch_query
        else:
            fetch_query = "WITH " + fetch_query
            
    else:
        fetch_query = f'''
        SELECT "{column}", COUNT(*) AS count
        FROM {tablename}
        WHERE "{column}" IS NOT NULL
        GROUP BY "{column}"
        ORDER BY count DESC
        '''

        if isinstance(tablename, TransformationSQLPipeline):
            with_context = tablename.get_codes(mode="WITH")
            fetch_query = with_context + "\n" + fetch_query
        
    result = df_to_list(run_sql_return_df(con, fetch_query))
    result_dict = {str(row[0]): row[1] for row in result}
    return result_dict


def plot_distribution_category(con, table_name, column_name):

    result_dict = build_barchat_input(con, column_name, table_name, limit=11)
    result_dict = modify_data_dict(result_dict, limit=30)
    
    plt.figure(figsize=(4, 2))
    
    categories = list(result_dict.keys())
    counts = list(result_dict.values())
    
    bar_plot = sns.barplot(y=categories, x=counts, color="#5DADE2", edgecolor="black")
    
    if 'Other values' in categories:
        other_index = categories.index('Other values')
        bar_plot.patches[other_index].set_facecolor('orange')
    
    for index, value in enumerate(counts):
        bar_plot.text(value, index, f'{int(value)}', color='black', va='center', fontsize=8)
    
    plt.xlabel(f"{table_name}[{column_name}]")
    
    plt.tight_layout()
    
    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)
    
    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')
    
    return image_base64

def modify_data_dict(data_dict, limit=30):
    modified_dict = {}
    for label, value in data_dict.items():
        new_label = (str(label)[:limit] + "...") if len(str(label)) > limit else str(label)
        
        if new_label in modified_dict:
            if isinstance(modified_dict[new_label], list):
                modified_dict[new_label].append(value)
            else:
                modified_dict[new_label] = [modified_dict[new_label], value]
        else:
            modified_dict[new_label] = value
    
    return modified_dict
  

def eng_formatter_with_precision(precision):
    def _formatter(x, pos):
        eng = EngFormatter(places=precision, sep="\N{THIN SPACE}")
        return eng.format_eng(x)
    return _formatter

def plot_distribution_numerical(con, table_name, column_name):
    """
    This function takes a database connection, table name, and a column name as input.
    It plots a vertical histogram for the numeric column and returns it as a base64-encoded PNG string.
    """
    counts, bin_width, bin_centers = build_histogram_inputs(con, column_name, table_name)

    sns.set_style("whitegrid")
    plt.figure(figsize=(4, 2))

    hist_plot = plt.bar(bin_centers, counts, width=bin_width, align='center', color="#5DADE2", edgecolor="black")

    plt.gca().xaxis.set_major_formatter(eng_formatter_with_precision(precision=1))
    plt.xticks(bin_centers[::6])

    for rect in hist_plot:
        height = rect.get_height()
        plt.text(rect.get_x() + rect.get_width() / 2, height, f'{int(height)}', ha='center', va='bottom', fontsize=8)

    plt.ylabel('Frequency')
    plt.xlabel(f"{table_name}[{column_name}]")

    plt.subplots_adjust(top=0.9)

    plt.tight_layout()

    buf = io.BytesIO()
    plt.savefig(buf, format='png', dpi=300)
    plt.close()
    buf.seek(0)

    image_base64 = base64.b64encode(buf.getvalue()).decode('utf-8')

    return image_base64

def filter_attributes(attributes, groups):
    retained_groups = []
    
    for group in groups:
        if all(item in attributes for item in group):
            retained_groups.append(group)
    
    all_retained = set([item for sublist in retained_groups for item in sublist])
    
    filtered_attributes = [attr for attr in attributes if attr not in all_retained]
    
    return filtered_attributes, retained_groups




def build_map_inputs(con, column, column2, tablename):
    result = run_sql_return_df(con, f'SELECT "{column}", "{column2}" FROM {tablename} WHERE "{column}" IS NOT NULL AND "{column2}" IS NOT NULL LIMIT 100')
    result = np.array(result)
    return result.tolist()



            
            
class GenerateProfileReport(Node):
    default_name = 'Generate Report'
    default_description = 'This node generates a report based on the profiling results.'

    def extract(self, item):
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        schema = table_pipeline.get_schema(con)
        df = run_sql_return_df(con, f'SELECT * FROM {table_pipeline} LIMIT  100')
        table_name = table_pipeline.__repr__(full=False)

        def replace_pattern(text):
            return re.sub(r'\*\*(.*?)\*\*', r'<u>\1</u>', text)

        table_summary = self.get_sibling_document('Create Table Summary')
        table_summary = replace_pattern(table_summary)
        table_summary

        group_json = self.get_sibling_document('Create Column Grouping')
        javascript_content, _, _ = get_tree_javascript(group_json)

        column_df = pd.read_json(self.get_sibling_document('Describe Columns'), orient="split")

        error_html = ""
        duplication_count = self.get_sibling_document('Decide Duplicate')['duplicate_count']

        if duplication_count > 0:
            error_html += f"<h2>Duplicate Rows</h2> There are <b>{duplication_count}</b> duplicated rows in the dataset.<br><br>"

        data_type_df = pd.read_json(self.get_sibling_document('Decide Data Type'), orient="split")
        error_html += f"<h2>Data Type</h2> {data_type_df.to_html()} <br>"

       


        missing_df = pd.read_json(self.get_sibling_document('Decide Missing Values'), orient="split")
        if len(missing_df) > 0:
            missing_df = missing_df.replace({"Is NULL Acceptable?": {True: "✔️ Yes", False: "❌ No"}})
            error_html += f"<h2>Missing Value</h2> {missing_df.to_html()} <br>"

        unique_df = pd.read_json(self.get_sibling_document('Decide Unique Columns'), orient="split")
        error_html += f"<h2>Column Uniqueness</h2> {unique_df.to_html()} <br>"

        numerical_df = pd.read_json(self.get_sibling_document('Decide Column Range'), orient="split")
        if len(numerical_df) > 0:
            error_html += f"<h2>Column Range</h2> {numerical_df.to_html()} <br>"
            
        unusual_text_df = pd.read_json(self.get_sibling_document('Decide Unusual For All'), orient="split")
        if len(unusual_text_df) > 0:
            unusual_text_df = unusual_text_df[unusual_text_df['Endorse']]
            unusual_text_df = unusual_text_df.drop(columns=['Endorse'])
        if len(unusual_text_df) > 0:
            error_html += f"<h2>Unusual Values</h2> {unusual_text_df.to_html()} <br>"
            
        lon_lat_groups = self.get_sibling_document('Decide Longitude Latitude')

        def build_column_details(column_name):
            html_content = ""
            errors = 0
            column_desc = column_df[column_df["Column"] == column_name]["Summary"].iloc[0]
            html_content += f'<b>{column_name}</b> {column_desc}<br>'
            
            current_datatype = data_type_df[data_type_df["Column"] == column_name]["Current Type"].iloc[0]
            target_datatype = data_type_df[data_type_df["Column"] == column_name]["Target Type"].iloc[0]
            
            if current_datatype != target_datatype:
                errors += 1
                html_content += f'<span class="circle">!</span> <b>{column_name} Data Type:</b>{current_datatype}. However, should be {target_datatype}<br>'
            else:
                html_content += f'<span class="circle2">✓</span> <b>{column_name} Data Type:</b> {current_datatype}<br>'
                
                
                    
                
            if len(missing_df) > 0 and column_name in missing_df["Column"].values:
                missing = missing_df[missing_df["Column"] == column_name]["NULL (%)"].iloc[0]
                
                if missing_df[missing_df["Column"] == column_name]["Is NULL Acceptable?"].iloc[0] == "✔️ Yes":
                    explanation = missing_df[missing_df["Column"] == column_name]["Explanation"].iloc[0]
                    html_content += f'<span class="circle2">✓</span> <b>{column_name} has {missing}% Missing Values</b>: {explanation}<br>'
                else:
                    errors += 1
                    html_content += f'<span class="circle">!</span> <b>{column_name} has {missing}% Missing Values</b><br>'
                    
            if len(unique_df) > 0 and column_name in unique_df["Column"].values:
                is_unique = unique_df[unique_df["Column"] == column_name]["Is Unique?"].iloc[0]
                should_unique = unique_df[unique_df["Column"] == column_name]["Should Unique?"].iloc[0]
                explanation = unique_df[unique_df["Column"] == column_name]["Explanation"].iloc[0]
                
                if is_unique and should_unique:
                    html_content += f'<span class="circle2">✓</span> <b>{column_name} is Unique</b><br>'
                elif not is_unique and should_unique:
                    errors += 1
                    html_content += f'<span class="circle">!</span> <b>{column_name} is not Unique</b> However, it should be unique: {explanation}<br>'
                
            if len(numerical_df) > 0 and column_name in numerical_df["Column"].values:
                within_range = numerical_df[numerical_df["Column"] == column_name]["Within Range?"].iloc[0]
                
                if within_range == "❌ No":
                    errors += 1
                    expected_range = numerical_df[numerical_df["Column"] == column_name]["Expected Range"].iloc[0]
                    true_range = numerical_df[numerical_df["Column"] == column_name]["True Range"].iloc[0]
                    explanation = numerical_df[numerical_df["Column"] == column_name]["Explanation"].iloc[0]
                    html_content += f'<span class="circle">!</span> <b>{column_name} has unexpected range:</b> It\'s expected be in: {expected_range}, because: {explanation}. However, its true range is: {true_range}<br>'
            
            if len(unusual_text_df) > 0 and column_name in unusual_text_df["Column"].values:
                explanation = unusual_text_df[unusual_text_df["Column"] == column_name]["Explanation"].iloc[0]
                errors += 1
                html_content += f'<span class="circle">!</span> <b>{column_name} is errorenous:</b> {explanation}<br>'
            
            return html_content, errors


        def build_column_group_html(d):
            html_output = ""
            javascript_output = ""
            total_errors = 0

            for key, value in d.items():
                column_html, column_javascript, errors = build_column_html(key, value)
                html_output += column_html
                javascript_output += column_javascript
                total_errors += errors

            return html_output, javascript_output, total_errors

        def build_column_html(column_name, value):

            html_output = ""
            javascript_output = ""
            
            errors = 0
            if isinstance(value, dict):
                child_html, child_javascript, child_errors = build_column_group_html(value)
                html_output += f'<div class="indent">\n{child_html}</div>\n'
                javascript_output += child_javascript
                errors += child_errors
            elif isinstance(value, list):
                html_output += '<div class="indent">\n'
                filtered_attributes, retained_groups = filter_attributes(value, lon_lat_groups)
                
                for item in retained_groups:
                    item_html, item_javascript, item_errors = handle_lon_lat(item)
                    html_output += item_html
                    javascript_output += item_javascript
                    errors += item_errors
                    
                for item in filtered_attributes:
                    item_html, item_javascript, item_errors = handle_list_item(item)
                    html_output += item_html
                    javascript_output += item_javascript
                    errors += item_errors
                
                html_output += '</div>\n'
                    
            final_html_output = f"""<div class="card-item">
            <span class="field-name">Group: {column_name}</span>
            <div class="card-controls">
                {'<span class="circle">' + str(errors) + '</span>' if errors > 0 else ""}
                <span class="toggle">▼</span>
            </div>
            </div>""" + html_output 
            
            return final_html_output, javascript_output, errors

        def handle_lon_lat(lon_lat_group):
            
            viz_html, viz_javascript = build_map_viz(lon_lat_group[0], lon_lat_group[1])
            detail_html1, errors1 = build_column_details(lon_lat_group[0])
            detail_html2, errors2 = build_column_details(lon_lat_group[1])
            errors = errors1+errors2
            
            html_output = "<div class=\"indent\">"
            html_output += viz_html
            html_output += detail_html1
            html_output += detail_html2
            html_output += "</div>"
            
            final_html_output = f"""<div class="card-item">
            <span class="field-name">Columns: {lon_lat_group[0], lon_lat_group[1]}</span>
            <div class="card-controls">
                {'<span class="circle">' + str(errors) + '</span>' if errors > 0 else ""}
                <span class="toggle">▼</span>
            </div>
            </div>""" + html_output 

            javascript_output = ""
            javascript_output += viz_javascript
            
            return final_html_output, javascript_output, errors

        def build_map_viz(lon_column, lat_column):
            html_output = ""
            javascript_output = ""
            
            html_output += f"<div id=\"map_viz_{lon_column}_{lat_column}\"></div>"
            data = build_map_inputs(con, lon_column, lat_column, table_pipeline)
            javascript_output += f"""data = {data};
            drawMap("map_viz_{lon_column}_{lat_column}", data);
        """
            
            return html_output, javascript_output

        def build_column_viz(column_name):
            html_output = ""
            javascript_output = ""
            data_type = schema[column_name]
            if data_type in data_types["INT"] or data_type in data_types["DECIMAL"]:
                html_output += f"<div id=\"hist_viz_{column_name}\"></div>"
                counts, bin_width, bin_centers = build_histogram_inputs(con, column_name, table_pipeline)
                javascript_output += f"""data = {[
                {"x": center, "y": count} for center, count in zip(bin_centers, counts)
            ]};
            binWidth = {bin_width};
            drawHistogram("hist_viz_{column_name}", data, binWidth);
        """
            
            else:
                html_output += f"<div id=\"bar_viz_{column_name}\"></div>"
                data_dict = build_barchat_input(con, column_name, table_pipeline)
                total_value = sum(data_dict.values())
                data = [{"label": (str(label)[:15] + "...") if len(str(label)) > 15 else str(label), "value": (value / total_value) * 100} for label, value in data_dict.items()]
                javascript_output += f"""data = {data};
            drawBarChart("bar_viz_{column_name}", data);
        """  
            
            return html_output, javascript_output
                

        def handle_list_item(column_name):

            
            viz_html, viz_javascript = build_column_viz(column_name)
            
            detail_html, errors = build_column_details(column_name)

            html_output = "<div class=\"indent\">"
            html_output += viz_html
            html_output += detail_html
            html_output += "</div>"
            
            final_html_output = f"""<div class="card-item">
            <span class="field-name">Column: {column_name}</span>
            <div class="card-controls">
                {'<span class="circle">' + str(errors) + '</span>' if errors > 0 else ""}
                <span class="toggle">▼</span>
            </div>
            </div>""" + html_output 

            javascript_output = ""
            javascript_output += viz_javascript
            
            return final_html_output, javascript_output, errors


        column_html, column_javascript, total_errors = build_column_group_html(group_json)


        html_content = f"""<!DOCTYPE html>
<html lang="en">
<head>
<title>Cocoon</title>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

<style>

html {{
    height: 100%; /* Ensure the html element covers the full height */
    background-color: #f0f0f0; /* Set your desired background color */
}}

body {{
    /* Scale the entire content to 70% */
    transform: scale(0.75);
    transform-origin: top left; /* Adjust as needed */
    width: 133.33%; /* Increase width to fit, since scaling down shrinks the viewport */
    height: 133.33%; /* Adjust height similarly if necessary */
    overflow: auto; /* Add scrollbars if content overflows */
}}

  body, h1, h2, p {{
    margin: 0;
    padding: 0;
    font-family: 'Arial', sans-serif;
  }}

  .container {{
      display: flex;
      justify-content: space-between;
      align-items: center; /* This ensures the items are aligned in the middle vertically */
  }}

  .map-container {{
    width: 300px;
    height: 200px;
    border: 1px solid black;
  }}
  
  .histogram-container {{
    width: 300px;
    height: 100px;
    border: 1px solid black;
  }}

  .chart-container {{ /* Style for the container */
      width: 200px;
      height: 80px;
      border: 1px solid black;
  }}
  
  .bar-chart-container {{
    width: 300px;
    height: 100px;
    border: 1px solid black;
  }}

  .dashboard {{
    display: grid;
    grid-template-areas:
      "main-panel right-panel"
      "bottom-panel right-panel";
    grid-template-rows: 1fr 1fr; /* Equal height for both rows */
    gap: 10px;
    height: 130vh;
    padding: 10px;
    background-color: #f0f0f0;
  }}

  .main-panel {{
      grid-area: main-panel;
      background-color: #ffffff;
      padding: 20px;
      position: relative; /* Set the main-panel to relative to position the chat box inside it */
      box-shadow: 0 0 10px rgba(0,0,0,0.1);
      overflow: hidden;
      display: flex;
      flex-direction: column; /* Stack the children vertically */
  }}

  .table-container {{
      overflow-x: auto; /* Allows horizontal scrolling for the table */
      overflow-y: auto; /* Allows vertical scrolling for the table */
      flex-grow: 1;
      max-width: 100%; /* Ensures that the container doesn't exceed the width of its parent */
  }}

  .right-panel {{
    grid-area: right-panel;
    background-color: #fff;
    padding: 15px;
    box-shadow: 0 0 10px rgba(0,0,0,0.1);
    overflow-y: auto; /* Enable scrolling for the panel */
    width: 360px;
  }}

  .indent {{
      padding-left: 5px; /* Increase as needed for deeper indents */
      font-size: small;
      display: none;
  }}

  .card-item {{
    display: flex;
    align-items: center;
    background-color: #626262; /* White background for the card */
    border: 1px solid #626262; /* Light grey border */
    color: white;
    border-radius: 4px; /* Slightly rounded corners */
    margin-bottom: 6px; /* Space between card items */
    padding: 3px;
    padding-left: 10px;
    width: 320px;
  }}

  .card-item-collapsed {{
      background-color: #d0d0d0; /* Lighter background for collapsed card */
      border: 1px solid #d0d0d0; /* Lighter border for collapsed card */
      color: black; /* Change text color for better contrast on light background */
  }}

  .icon {{
    /* Styles for the icon, you can replace it with an actual icon font or image */
    padding-right: 20px;
  }}

  .field-name {{
    /* Styles for the field name */
    flex-grow: 1;
    padding-right: 20px;
    font-size: 14px;

  }}

  .circle {{
      background-color: red;
      font-size: small;
      border-radius: 50%;
      color: white;
      padding: 0px;
      text-align: center;
      display: inline-block;
        width: 16px;         /* Fixed width */
        height: 16px;        /* Fixed height */
        line-height: 16px;   /* Center the number vertically */
  }}

  .circle2 {{
      background-color: green;
      font-size: small;
      border-radius: 50%;
      color: white;
      padding: 0px;
      text-align: center;
      display: inline-block;
      /* Changes for a better circle */
        width: 16px;         /* Fixed width */
        height: 16px;        /* Fixed height */
        line-height: 16px;   /* Center the number vertically */
  }}

  .card-controls {{
    display: flex;
  }}

  .drop-down-btn,
  .add-btn {{
    /* Shared styles for buttons */
    background-color: #4CAF50; /* Green background */
    color: white;
    border: none;
    border-radius: 2px; /* Slightly rounded corners for the buttons */
    cursor: pointer;
    padding: 2px 6px; /* Smaller padding for a compact look */
    margin-left: 4px; /* Spacing between buttons */
  }}

  .drop-down-btn:hover,
  .add-btn:hover {{
    background-color: #45a049; /* Darker green on hover */
  }}


  .bottom-panel {{
    grid-area: bottom-panel;
    background-color: #ffffff;
    padding: 20px;
    box-shadow: 0 0 10px rgba(0,0,0,0.1);
    overflow-x: auto;
  }}

  table {{
    width: 100%;
    border-collapse: collapse;
    margin-top: 20px;
    font-size: 0.9em;
    border-radius: 8px;
    overflow: hidden;

  }}

  th, td {{
    padding: 6px 8px;
    text-align: left;
    border-bottom: 1px solid #dddddd;
    border-right: 1px solid #dddddd;
  }}


  th:last-child, td:last-child {{
    border-right: none; /* Removes the border from the last cell of each row */
  }}

  thead tr {{
    background-color: #009879; /* Changed color for header */
    color: #ffffff; /* Changed text color for better contrast */
    text-align: left;
    font-weight: bold;
    font-size: 14px;
  }}

  th {{
    top: 0;
    background-color: #bebebe; /* Ensure the sticky header has the same background color */
  }}

  tbody tr {{
    background-color: #f9f9f9; /* Lighter color for content rows */
    font-size: 14px
  }}


  tbody tr:last-of-type {{
    border-bottom: 2px solid #009879;
  }}

  tbody tr.active-row {{
    font-weight: bold;
    color: #009879;
  }}

  .link {{
      fill: none;
      stroke: #555;
      stroke-opacity: 0.4;
      stroke-width: 1.5px;
  }}

  .node {{
      cursor: pointer;
  }}

  .node circle {{
      fill: #999;
      stroke: black;
      stroke-width: 1.5px;
  }}

  .node text {{
      font: 12px sans-serif;
      fill: #555;
  }}

  .icons {{
    /* Making the icon larger */
    font-size: 20px; /* You can adjust this value as needed */
    color: white; /* Icon color */
    background-color: black; /* Background color */
    padding: 6px 6px; /* Top/bottom padding and left/right padding */
    border-radius: 3px; /* Making the corners sharp for a rectangular look */
  }}

</style>
</head>
<body>

<div class="dashboard">

    <div class="main-panel">
        <div class="container">
            <a href="https://github.com/Cocoon-Data-Transformation/cocoon" target="_blank" style="display: flex; align-items: center; text-decoration: none; color: black;">
                <img src="https://raw.githubusercontent.com/Cocoon-Data-Transformation/cocoon/main/images/cocoon_icon.png" alt="cocoon icon" width=50 style="margin-right: 10px;">
                <div style="margin: 0; padding: 0;"><h2 style="margin: 0; padding: 0;">Table Profile</h2>
                    <p style="margin: 0; padding: 0;">Powered by cocoon</p>
                </div>
            </a>
            <div><h1>{table_name}</h1>(First 100 rows)</div>
            <div>
                <p style="text-align: right">Share Cocoon with the World 😊</p>
                <div>
                    <a href="https://twitter.com/intent/tweet?url=https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon&text=Can%27t+believe+how+amazing+the+table+profiles+are+created+by+Cocoon+using+AI.+It%27s+Open+sourced%21&related=ZacharyHuang12" target="_blank"  style="text-decoration: none;">
                        <i class="fa-brands fa-twitter icons"></i>
                    </a>
                    <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon" target="_blank"  style="text-decoration: none;">
                        <i class="fa-brands fa-facebook icons"></i>
                    </a>
                    <a href="https://www.linkedin.com/sharing/share-offsite/?url=https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon" target="_blank" style="text-decoration: none;">
                        <i class="fa-brands fa-linkedin icons"></i>
                    </a>
                    <a href="https://api.whatsapp.com/send?text=Can%27t%20believe%20how%20amazing%20the%20table%20profiles%20are%20created%20by%20Cocoon%20using%20AI.%20It%27s%20Open%20sourced%21%20https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon" target="_blank" style="text-decoration: none;">
                        <i class="fa-brands fa-whatsapp icons"></i>
                    </a>
                    <a href="https://www.reddit.com/submit?url=https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon&title=Can%27t%20believe%20how%20amazing%20the%20table%20profiles%20are%20created%20by%20Cocoon%20using%20AI.%20It%27s%20Open%20sourced%21" target="_blank" style="text-decoration: none;">
                        <i class="fa-brands fa-reddit icons"></i>
                    </a>
                    <a href="https://pinterest.com/pin/create/button/?url=https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon&media=&description=Can%27t%20believe%20how%20amazing%20the%20table%20profiles%20are%20created%20by%20Cocoon%20using%20AI.%20It%27s%20Open%20sourced%21" target="_blank" style="text-decoration: none;">
                        <i class="fa-brands fa-pinterest icons"></i>
                    </a>
                    <a href="https://www.tumblr.com/widgets/share/tool?posttype=link&title=Can%27t%20believe%20how%20amazing%20the%20table%20profiles%20are%20created%20by%20Cocoon%20using%20AI.%20It%27s%20Open%20sourced%21&caption=&content=https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon&canonicalUrl=https%3A%2F%2Fgithub.com%2FCocoon-Data-Transformation%2Fcocoon&shareSource=tumblr_share_button" target="_blank" style="text-decoration: none;">
                        <i class="fa-brands fa-tumblr icons"></i>
                    </a>

                </div>
            </div>
        </div>

        <div class="table-container">
            {df[:100].to_html()}
        </div>
    </div>
    <div class="right-panel">
{column_html}
    </div>

    <div class="bottom-panel">
        <div class="container">
            <h2>Table Summary</h2>
        </div>
        <p style="font-size: 15px;">{table_summary}</p>
        <br>
        <h2>Column Grouping</h2>
        <div id="tree"></div>
        
        <h2>Column Summary</h2>
        <div class="table-container">{column_df[:100].to_html()}</div>
        <br>
        {error_html}
    </div>
</div>

<script src="https://d3js.org/d3.v6.min.js"  charset="utf-8"></script>
<script src="https://d3js.org/topojson.v3.min.js"></script>
<script>
    document.querySelectorAll('.card-item').forEach(function(card) {{
        card.addEventListener('click', function() {{
            var indent = this.nextElementSibling;
            var toggle = this.querySelector('.toggle');
            if (indent.style.display === "block") {{
                indent.style.display = "none";
                toggle.textContent = '▼';
                this.classList.remove('card-item-collapsed');
            }} else {{
                indent.style.display = "block";
                toggle.textContent = '▲';
                this.classList.add('card-item-collapsed');
            }}
        }});
    }});
</script>
<script>
{javascript_content}
</script>
<script>
    function drawBarChart(divId, data) {{
      const margin = {{top: 10, right: 10, bottom: 25, left: 100}},
            width = 300 - margin.left - margin.right,
            height = 100 - margin.top - margin.bottom;

      const targetDiv = d3.select("#" + divId);
      targetDiv.classed("bar-chart-container", true);

      const svg = targetDiv
                    .append("svg")
                      .attr("width", width + margin.left + margin.right)
                      .attr("height", height + margin.top + margin.bottom)
                    .append("g")
                      .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

      const y = d3.scaleBand()
                  .range([0, height])
                  .domain(data.map(d => d.label))
                  .padding(0.1);

      const x = d3.scaleLinear()
                  .domain([0, d3.max(data, d => d.value)])  // Percent scale
                  .range([0, width]);

      svg.append("g")
         .call(d3.axisLeft(y));

      svg.selectAll(".bar")
         .data(data)
         .enter().append("rect")
           .attr("class", "bar")
           .attr("y", d => y(d.label))
           .attr("height", y.bandwidth())
           .attr("x", 0)
           .attr("width", d => x(d.value))
           .attr("fill", "steelblue");

      svg.append("g")
         .attr("transform", "translate(0," + height + ")")
         .call(d3.axisBottom(x).ticks(5).tickFormat(d => d + "%"));
    }}
    
    function drawHistogram(divId, data, binWidth) {{
      const margin = {{top: 10, right: 10, bottom: 20, left: 40}},
            width = 300 - margin.left - margin.right,
            height = 100 - margin.top - margin.bottom;
            
      const targetDiv = d3.select("#" + divId);
      targetDiv.classed("histogram-container", true);

      const svg = targetDiv
                    .append("svg")
                      .attr("width", width + margin.left + margin.right)
                      .attr("height", height + margin.top + margin.bottom)
                    .append("g")
                      .attr("transform", "translate(" + margin.left + "," + margin.top + ")");

      const x = d3.scaleLinear()
                  .domain([d3.min(data, d => d.x) - binWidth, d3.max(data, d => d.x) + binWidth])
                  .range([0, width]);

      const y = d3.scaleLinear()
                  .domain([0, d3.max(data, d => d.y) * 1.1]) // Increase the y-axis limit by 10% for margin
                  .range([height, 0]);

      const xAxis = d3.axisBottom(x).ticks(5);
      const yAxis = d3.axisLeft(y).ticks(5);

      svg.append("g")
         .attr("transform", "translate(0," + height + ")")
         .call(xAxis)

      svg.append("g")
         .call(yAxis)

      svg.selectAll("rect")
         .data(data)
         .enter().append("rect")
           .attr("x", d => x(d.x - binWidth / 2))
           .attr("y", d => y(d.y))
           .attr("width", x(binWidth) - x(0))
           .attr("height", d => height - y(d.y))
           .attr("fill", "steelblue");
    }}
    
    function drawMap(divId, coordinates) {{
      const targetDiv = d3.select("#" + divId);
      targetDiv.classed("map-container", true);

      const width = 300;
      const height = 200;
      const projection = d3.geoNaturalEarth1()
          .scale(width / 1.5 / Math.PI)
          .translate([width / 2, height / 2]);
      const path = d3.geoPath().projection(projection);

      const svg = targetDiv.append("svg")
          .attr("width", width)
          .attr("height", height);

      d3.json("https://raw.githubusercontent.com/holtzy/D3-graph-gallery/master/DATA/world.geojson").then(data => {{
        // Draw the map
        svg.append("g")
          .selectAll("path")
          .data(data.features)
          .join("path")
              .attr("fill", "#ccc")
              .attr("d", path)
              .style("stroke", "#fff");

        // Plotting the points
        coordinates.forEach(coords => {{
          svg.append("circle")
            .attr("cx", projection(coords)[0])
            .attr("cy", projection(coords)[1])
            .attr("r", 2)
            .attr("fill", "red");
        }});
      }});
    }}
    
    let data = [];
    let binWidth = 5;
    {column_javascript}

</script>
</body>
</html>
"""
        return html_content

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        html_content = extract_output
        display(HTML(wrap_in_iframe(html_content)))
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        ask_save_file(f"Cocoon_Profile_{table_name}.html", html_content)
        callback(html_content)

def ask_save_files(labels, file_names, contents):
    file_path_inputs = [Text(value=file_name, description=label) for label, file_name in zip(labels, file_names)]

    overwrite_checkbox = Checkbox(value=False, description='Allow Overwrite')

    error_msg = widgets.HTML(value='')

    def save_files_click(b):
        error_msg.value = ''
        for file_path_input, label, content in zip(file_path_inputs, labels, contents):
            file_path = file_path_input.value
            allow_overwrite = overwrite_checkbox.value

            if file_exists(file_path) and not allow_overwrite:
                error_msg.value += f"<div style='color: orange;'>⚠️ Warning: Failed to save {label}. File already exists.</div>"
            else:
                try:
                    write_to(file_path, content)
                    error_msg.value += f"<div style='color: green;'>🎉 File saved successfully as {file_path}</div>"
                except Exception as e:
                    error_msg.value += f"<div style='color: red;'>❌ Error saving {label}: {str(e)}</div>"

    save_button = Button(
        description="Save Files",
        button_style='primary',
        icon='save'
    )
    save_button.on_click(save_files_click)
    
    display(VBox(file_path_inputs + [overwrite_checkbox, save_button, error_msg]))
    
    return overwrite_checkbox, save_button, save_files_click

    

    
def ask_save_file(file_name, content):
    print(f"🤓 Do you want to save the file?")

    def save_file_click(b):
        updated_file_name = file_name_input.value
        allow_overwrite = overwrite_checkbox.value

        if file_exists(updated_file_name) and not allow_overwrite:
            print("\x1b[31m" + "Warning: Failed to save. File already exists." + "\x1b[0m")
        else:
            write_to(updated_file_name, content)
            print(f"🎉 File saved successfully as {updated_file_name}")

    file_name_input = Text(value=file_name, description='File Name:')

    save_button = Button(description="Save File")
    save_button.on_click(save_file_click)

    overwrite_checkbox = Checkbox(value=False, description='Allow Overwrite')

    display(HBox([file_name_input, overwrite_checkbox]), save_button)
    


class DecideTableHubRelationForAll(MultipleNode):
    default_name = 'Decide Table Hub Relation For All'
    default_description = 'This node allows users to decide the relation between tables and hubs.'

    def construct_node(self, element_name, idx=0, total=0, hubs=None):
        if hubs is None:
            hubs = []
            
        para = self.para.copy()
        para["table_name"] = element_name
        para["table_idx"] = idx
        para["total_tables"] = total
        para["hubs"] = hubs
        node = DecideTableHubRelation(para=para, id_para ="table_name")
        node.inherit(self)
        return node

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Deciding the relation between tables and hubs...")

        self.input_item = item

        data_project = self.para["data_project"]
        tables = data_project.list_tables()
        
        reference_tables_document = self.get_sibling_document('Decide Reference Tables')
        reference_tables_df = pd.read_json(reference_tables_document, orient="split")
        reference_tables = reference_tables_df[reference_tables_df["Is Reference Table?"] == True]["Table"].tolist()
        
        tables = [table for table in tables if table not in reference_tables]
        
        hubs = self.get_sibling_document('Decide Hubs')
        self.elements = []
        self.nodes = {}

        idx = 0
        for table in tables:
            self.elements.append(table)
            self.nodes[table] = self.construct_node(table, idx, len(tables), hubs)
            idx += 1
            
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        
        tables = self.para["data_project"].list_tables()
        query_widget = self.item["query_widget"]
        
        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        hub_document = document.get('Decide Table Hub Relation', {})
        
        data = {
            'Table': [],
            'Related Hubs': [],
            'Explanation': [],
        }

        for table_name, details in hub_document.items():
            data['Table'].append(table_name)
            data['Related Hubs'].append(list(details["contain_hubs"].keys()))
            data['Explanation'].append(details["description"])
            
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        hub_document = self.get_sibling_document('Decide Hubs')
        hub_df = pd.read_json(hub_document, orient="split")
        hubs = hub_df["Hub"].tolist()
        
        editable_columns = [False, True, True, True]
        reset = True
        long_text = []
        
        editable_list = {
            'Related Hubs': {
                'allowed_tags': hubs,
                'allow_duplicates': False
            }
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset, long_text=long_text, editable_list=editable_list)
        
        
        hub_desc = ", ".join(hubs)
        print(f"🤓 The hubs are: {hub_desc}")
        
        print("😎 We have decided the relation between tables and hubs. Please verify:")
        display(grid)
        
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                try:
                    new_df =  grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                except Exception as e:
                    print(f"{str(e)}")
                    return
                
                document = new_df.to_json(orient="split")
                callback(document)
            
        next_button.on_click(on_button_clicked)
        display(next_button)
        

class CleanUnusual(ListNode):
    default_name = 'Clean Unusual'
    default_description = 'This node allows users to clean the unusual values.'
    default_sample_size = 300
    batch_size = 50

    def extract(self, item):
        clear_output(wait=True)

        self.input_item = item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)

        idx = self.para["column_idx"]
        total = self.para["total_columns"]
        unusual_reason = self.para["unusual_reason"]

        column_name = self.para["column_name"]
        
        display(HTML(f"{running_spinner_html} Cleaning unusual values for <i>{table_name}[{column_name}]</i>..."))
        create_progress_bar_with_numbers(3, doc_steps)
        show_progress(max_value=total, value=idx)
        
        sample_size = self.class_para.get("sample_size", self.default_sample_size)
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            'cardinality_threshold' in self.para['cocoon_stage_options']):
            sample_size = self.para['cocoon_stage_options']['cardinality_threshold']

        query = create_sample_distinct_query(con, table_pipeline, column_name, sample_size)
        with_context = table_pipeline.get_codes(mode="WITH")
        query = with_context + "\n" + query
        
        sample_values = run_sql_return_df(con,query)
        sample_values = sample_values[sample_values[column_name].str.len() < 200]
        
        query =  construct_distinct_count_query(con, table_pipeline, column_name)
        query = with_context + "\n" + query
        total_distinct_count = run_sql_return_df(con,query).iloc[0, 0]
        

        outputs = []
        
        if total_distinct_count > sample_size:
            return [(column_name, None, unusual_reason, sample_size, total_distinct_count)]
        
        for i in range(0, len(sample_values), self.batch_size):
            sample_values_batch = sample_values[i:i+self.batch_size]
            outputs.append((column_name, sample_values_batch, unusual_reason, sample_size, total_distinct_count))
        
        return outputs

    def run(self, extract_output, use_cache=True):
        column_name, sample_values, unusual_reason, sample_size, total_distinct_count = extract_output

        if total_distinct_count > sample_size:
            return {"explanation": unusual_reason, "could_clean": False, "mapping": {}, "projection": False}
        
        value_list = sample_values[column_name].tolist()
        truncated_list = [truncate_value(val) for val in value_list]
        sample_values_list_str = f"[{', '.join(truncated_list)}]"
        
        template = f"""{column_name} column is unusual: {unusual_reason}
It has the following values, ordered by frequency: {sample_values_list_str}

Task: First, understand which values are unusual and why.
Then, maps those unusual values to the correct ones to fix the problems.
E.g., if the old values have inconsistent patters/typos, map to the most frequent case.
If old values are meaningless, map to empty string.

Return in the following format:
```yml
explanation: >
    The problem is ... The correct values are ...
mapping: # just the values need to be changed;
    {truncated_list[0]}: {truncated_list[0]}
    'I''m showing how to escape': "I'm showing how to escape"
    ...
```"""

        messages = [{"role": "user", "content": template}]
        self.messages.append(messages)
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages[-1] = messages
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        summary["projection"] = False 
        summary["could_clean"] = True
        
        def clean_mapping(mapping):
            if mapping is None:
                return {}
            return {k: v for k, v in mapping.items() if k != v}

        summary["mapping"] = clean_mapping(summary.get("mapping", {}))

        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"explanation": "Fail to run", "could_clean": False, "mapping": {}, "projection": False}
    
    def merge_run_output(self, run_outputs):
        
        explanation = run_outputs[0]["explanation"]
       
        merged_mapping = {}

        for run_output in run_outputs:
            mapping = run_output["mapping"]
            for key, value in mapping.items():
                if key not in merged_mapping:
                    merged_mapping[key] = value

        return {"explanation": explanation, 
                "mapping": merged_mapping, 
                "could_clean": True if len(merged_mapping) > 0 else False,
                "projection": False}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        json_code = run_output
        column_name, _, unusual_reason, _, _ = extract_output[0]
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        con = self.item["con"]
        
        html_content = f"⚠️ <b>We've found errors in</b>: {column_name} <br><i>{unusual_reason}</i>"
        display(HTML(html_content))
        
        table_pipeline = self.para["table_pipeline"]
        column_name = self.para["column_name"]
        query_widget = self.item["query_widget"]

        table_pipeline = self.para["table_pipeline"]
        with_context = table_pipeline.get_codes(mode="WITH")

        query = create_sample_distinct_query(con, table_name=table_pipeline, column_name=column_name)
        create_explore_button(query_widget=query_widget, query=query, with_context=with_context)

        json_code["unusual_reason"] = unusual_reason

        if not json_code["could_clean"]:
            print("☹️ Cocoon can't clean this for you, as it's too complex...")
            print("😎 We'll log this for future cleanings and analyses")
            
            submit_button = widgets.Button(
                description='Next',
                disabled=False,
                button_style='success',
                tooltip='Click to submit',
                icon='check'
            )

            def on_button_clicked(b):
                with self.output_context():
                    clear_output(wait=True)
                    callback(json_code)
            
            submit_button.on_click(on_button_clicked)

            display(submit_button)
            
            if self.viewer or ("viewer" in self.para and self.para["viewer"]):
                on_button_clicked(submit_button)
            return


        if json_code["projection"]:
            
            print("🧐 It is possible to use a simple projection to the clean values.")

            explanation = json_code["explanation"]

            print(f" {ITALIC}{explanation}{END}")
            text_area = create_text_area(json_code["projection_clause"])

            toggle_button = widgets.Button(
                description='Reject Clean',
                disabled=False,
                button_style='danger',
                tooltip='Click to reject',
                icon='check'
            )
            def on_reject_button_clicked(b):
                with self.output_context():
                    clear_output(wait=True)
                    json_code["could_clean"] = False
                    callback(json_code)

            toggle_button.on_click(on_reject_button_clicked)


            submit_button = widgets.Button(
                description='Accept Clean',
                disabled=False,
                button_style='success',
                tooltip='Click to endorse',
                icon='check'
            )
            def on_button_clicked(b):
                with self.output_context():
                    clear_output(wait=True)
                    json_code["could_clean"] = True
                    json_code["projection_clause"] = text_area.value    
                    display(HTML(""))              
                    callback(json_code)
            
            submit_button.on_click(on_button_clicked)

            display(text_area)
            display(HBox([toggle_button, submit_button]))

        else:
            html_content = "🧐 We recommend the following transformation of values:<br>"
            explanation = json_code["explanation"]
            mapping = json_code["mapping"]

            html_content += f"<i>{explanation}</i>"
            display(HTML(html_content))

            grid = create_dictionary_grid_remove(mapping, "Old Value", "New Value")

            submit_button = widgets.Button(
                description='Accept Clean',
                disabled=False,
                button_style='success',
                tooltip='Click to submit',
                icon='check'
            )

            def on_button_clicked(b):
                with self.output_context():
                    clear_output(wait=True)
                    old_values_to_remove, non_removed_values = process_grid_changes_remove(grid)
                    json_code["mapping"] = non_removed_values
                    json_code["old_values_to_remove"] = old_values_to_remove
                    display(HTML(""))
                    callback(json_code)

            submit_button.on_click(on_button_clicked)

            toggle_button = widgets.Button(
                description='Reject Clean',
                disabled=False,
                button_style='danger',
                tooltip='Click to reject',
                icon='close'
            )
            def on_reject_button_clicked(b):
                with self.output_context():
                    clear_output(wait=True)
                    json_code["could_clean"] = False
                    callback(json_code)

            toggle_button.on_click(on_reject_button_clicked)

            display(grid)
            display(HBox([toggle_button, submit_button]))
            
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(submit_button)
            return

class CleanUnusualForAll(MultipleNode):
    default_name = 'Clean Unusual For All'
    default_description = 'This node allows users to clean the unusual values for all columns.'
    default_sample_size = 100

    def construct_node(self, element_name, idx=0, total=0, unusual_reason=""):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        para["unusual_reason"] = unusual_reason
        node = CleanUnusual(para=para, 
                            id_para ="column_name",
                            class_para={"sample_size": self.class_para.get("sample_size", self.default_sample_size)})
        node.inherit(self)
        return node

    def extract(self, item):
        clear_output(wait=True)
        document = self.get_sibling_document("Decide Unusual String For All").get("Decide String Unusual", {})
        table_object = self.para["table_object"]

        columns = list(document.keys())
        self.elements = []
        self.nodes = {}

        idx = 0
        for col in columns:
            if col in document:
                if document[col]["Unusualness"]:
                    unusual_reason = document[col]["Examples"]
                    table_object.unusualness[col] = unusual_reason
                    self.elements.append(col)
                    self.nodes[col] = self.construct_node(col, idx, len(columns), unusual_reason)
                    idx += 1

    def display_after_finish_workflow(self, callback, document):
        
        clean_unusual = document.get("Clean Unusual", {})
        table_object = self.para["table_object"]

        all_no_clean = True

        for col in clean_unusual:
            if clean_unusual[col]["could_clean"]:
                all_no_clean = False
                del table_object.unusualness[col]

        if all_no_clean:
            callback(document)
            return

        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        
        old_table_name = table_pipeline.__repr__(full=False)
        new_table_name = old_table_name + "_cleaned"

        selections = []
        filters = []

        comment = "-- Clean unusual string values: \n"
        
        for col in columns:
            if col not in clean_unusual:
                selections.append(enclose_table_name(col, con=con))
                continue
                
            clean_unusual_col = clean_unusual[col]
            
            explanation_value = clean_unusual_col.get('explanation', '')
            comment += f"-- {col}: {remove_newline(explanation_value)}\n"

            if not clean_unusual_col["could_clean"]:
                selections.append(enclose_table_name(col, con=con))
            else:
                if clean_unusual_col["projection"]:
                    selections.append(clean_unusual_col["projection_clause"] + f' AS {enclose_table_name(col, con=con)}')
                        
                else:
                    mapping = clean_unusual_col["mapping"]
                    old_values_to_remove = clean_unusual_col.get("old_values_to_remove", [])
                    remove_list_str = "(" + ", ".join([f"'{val}'" for val in old_values_to_remove]) + ")"
                    if old_values_to_remove:
                        filters.append(f'{enclose_table_name(col, con=con)} NOT IN {remove_list_str}')

                    selection_str = "CASE\n"
                    for old_value, new_value in mapping.items():
                        old_value = escape_value_single_quotes(old_value, con)
                        new_value = escape_value_single_quotes(new_value, con)
                        if new_value != "":
                            new_value = f"'{new_value}'"
                        else:
                            new_value = "NULL"
                        selection_str += f'    WHEN {enclose_table_name(col, con=con)} = \'{old_value}\' THEN {new_value}\n'
                    selection_str += f'    ELSE {enclose_table_name(col, con=con)}\n'
                    selection_str += f'END AS {enclose_table_name(col, con=con)}'
                    selections.append(selection_str)

        selection_sql = indent_paragraph(",\n".join(selections))
        where_sql = ""

        if len(filters) > 0:
            where_sql = "\nWHERE\n" + indent_paragraph(" AND\n".join(filters) if filters else "")

        sql_query = f"""SELECT
{selection_sql}"""  
        
        sql_query = comment + sql_query
        sql_query = lambda *args, sql_query=sql_query, where_sql=where_sql: f"{sql_query}\nFROM {args[0]}{where_sql}"  
        step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=self.item["con"])
        table_pipeline.add_step_to_final(step)
        database = self.para.get("database", None)
        schema = self.para.get("schema", None)
        try:
            self.para["table_pipeline"].materialize(con, database=database, schema=schema)
        except Exception as e:
            if cocoon_main_setting['DEBUG_MODE']:
                raise e
            write_log(f"""
The node is {self.name}
Error in materialization!!
The error is: {e}
""")  
            self.para["table_pipeline"].remove_final_node()
            
        callback(document)
            
class HandleMissing(Node):
    default_name = 'Handle Missing Values'
    default_description = 'This node allows users to handle missing values.'
    
    def extract(self, item):
        clear_output(wait=True)
        create_progress_bar_with_numbers(2, doc_steps)

        document = self.get_sibling_document("Decide Missing Values")
        con = self.item["con"]
        schema = self.para["table_pipeline"].get_schema(con)
    
        return document, schema
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        document, schema = extract_output
        
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        
        df = pd.read_json(document, orient="split")
        
        df = df[~df["Is NULL Acceptable?"]]
        
        if len(df) == 0:
            df = pd.DataFrame(columns=["Column", "NULL (%)", "Strategy"])
            document = df.to_json(orient="split")
            callback(document)
            return
        
        df = df.drop(columns=["Is NULL Acceptable?"])
        df = df.drop(columns=["Explanation"]) 
        
        UNCHANGE = "🔄 Unchanged"
        DROP_COLUMN = "🗑️ Drop Column"
        REMOVE_ROWS = "🧹 Remove Rows"
        
        categories = [UNCHANGE, DROP_COLUMN, REMOVE_ROWS]
        
        df["Strategy"] = UNCHANGE
        
        editable_columns = [False, False, True]
        reset = True
        category = {
            'Strategy': categories
        }

        grid = create_dataframe_grid(df, editable_columns, reset, category)
        
        next_button = widgets.Button(
            description='Accept Handling',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        reject_button = widgets.Button(
            description='Reject Handling',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='times'
        )
        
        def reject_button_click(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                new_df["Strategy"] = UNCHANGE
                document = new_df.to_json(orient="split")
                callback(document)
            
        def create_missing_handling_sql(df, schema, table_name):
            drop_columns = df[df["Strategy"] == DROP_COLUMN]["Column"].tolist()
            remove_rows = df[df["Strategy"] == REMOVE_ROWS]["Column"].tolist()

            if len(drop_columns) == len(schema):
                raise ValueError("All columns are dropped. Please keep at least one column.")

            if len(remove_rows) == 0 and len(drop_columns) == 0:
                return ""

            con = self.item["con"]
            select_columns = [enclose_table_name(col, con=con) for col in schema if col not in drop_columns]
            selection_sql = indent_paragraph(',\n'.join(select_columns))
            sql_query = f"""SELECT
{selection_sql}
FROM {table_name}"""

            where_clause = " AND\n".join([f'{enclose_table_name(col, con=con)} IS NOT NULL' for col in remove_rows])

            if where_clause:
                sql_query += f"\nWHERE \n{indent_paragraph(where_clause)}"

            handling_comments = f"-- Handling missing values: There are {len(df)} columns with unacceptable missing values\n"

            for index, row in df.iterrows():
                handling_comments += f"-- {row['Column']} has {row['NULL (%)']} percent missing. Strategy: {row['Strategy']}\n"

            sql_query = handling_comments + sql_query

            return sql_query
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                table_pipeline = self.para["table_pipeline"]
                missing_handling_sql = create_missing_handling_sql(new_df, schema, table_pipeline)
                
                if missing_handling_sql != "":
                    old_table_name = table_pipeline.__repr__(full=False)
                    new_table_name = old_table_name + "_missing_handled"
                    sql_query = lambda *args: create_missing_handling_sql(new_df, schema, args[0])
                    step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=self.item["con"])
                    table_pipeline.add_step_to_final(step)
                
                document = new_df.to_json(orient="split")

                callback(document)
            
        next_button.on_click(on_button_clicked)
        reject_button.on_click(reject_button_click)
        
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
            
        display(HTML(f"🧹 The following columns have abnormal missing values. Please select a strategy:"))
        display(grid)
        display(HTML(f"😎 More missing value handling strategies coming soon..."))
        display(HBox([reject_button, next_button]))
        
class DecideDataType(Node):
    default_name = 'Decide Data Type'
    default_description = 'This node allows users to decide the data type for each column.'

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Checking data types ...")
        create_progress_bar_with_numbers(2, doc_steps)
        self.progress = show_progress(1)

        self.input_item = item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]

        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        sample_size = 5
        
        database_name = get_database_name(con)
        all_data_types = list(data_types_database[database_name].keys())

        sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        schema = table_pipeline.get_schema(con)

        return sample_df, all_data_types, database_name, schema

    def run(self, extract_output, use_cache=True):
        sample_df, all_data_types, database_name, schema = extract_output

        template = f"""You have the following table:
{sample_df.to_csv(index=False, quoting=1)}

For each column, classify what the column type should be.
The column type should be one of the following:
{all_data_types}

Return in the following format:
```json
{{
    "column_type": {{
        "column1": "INT",
        ...
    }}
}}
```"""
        
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)

        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "column_type" in jc, "The 'column_type' key is missing in the JSON code."),
            (lambda jc: all(col_type in all_data_types for col_type in jc["column_type"].values()), "The column types are not all strings."),
            (lambda jc: all(col_name in sample_df.columns for col_name in jc["column_type"]), "One or more column names specified in 'column_type' are not present in the sample DataFrame."),
        ]

        for check, error_message in checks:
            if not check(json_code):
                raise ValueError(f"Validation failed: {error_message}")
            
        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        _, _, database_name, schema = extract_output
        return {"column_type": {col: get_reverse_type(schema[col], database_name) for col in schema}}

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        _, all_data_types, database_name, schema = extract_output
        self.progress.value += 1
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        

        rows_list = []

        for col in schema:
            current_type = get_reverse_type(schema[col], database_name)
            target_type = json_code["column_type"][col]

            rows_list.append({
                "column_name": col,
                "current_type": current_type,
                "target_type": target_type,
            })

        df = pd.DataFrame(rows_list)
        
        grid = create_data_type_grid(df, all_data_types = all_data_types)
        print("😎 We have recommended the Data Types for Columns:")
        display(grid)

        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                df = extract_grid_data_type(grid)
                
                callback(df.to_json(orient="split"))

        next_button.on_click(on_button_clicked)

        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return


class TransformType(ListNode):
    default_name = 'Transform Type'
    default_description = 'This node allows users to transform the data type for a column.'

    def extract(self, item):
        clear_output(wait=True)
        
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        column_name = self.para["column_name"]
        current_type = self.para["current_type"]
        target_type = self.para["target_type"]
        idx = self.para["column_idx"]
        total = self.para["total_columns"]

        display(HTML(f"{running_spinner_html} Transforming type for <i>{table_name}[{column_name}]</i>..."))
        create_progress_bar_with_numbers(1, doc_steps)
        show_progress(max_value=total, value=idx)

        database_name = get_database_name(con)
        
        hint = ""
        if current_type in transform_hints:
            if target_type in transform_hints[current_type]:
                if database_name in transform_hints[current_type][target_type]:
                    hint = transform_hints[current_type][target_type][database_name]
                    
        sample_size = 5
        outputs = []
        
        table_object = self.para["table_object"]
        patterns = table_object.patterns.get(column_name, [])
        with_context = table_pipeline.get_codes(mode="WITH")
        
        
        if not is_type_comparable(current_type):
            return [(column_name, None, current_type, target_type, database_name, table_pipeline, con, hint, None, [], with_context)]
        
        if len(patterns) == 0:
            query = create_sample_distinct_query(con, table_pipeline, column_name, sample_size)
            
            query = with_context + "\n" + query
            sample_values = run_sql_return_df(con, query)
            return [(column_name, sample_values, current_type, target_type, database_name, table_pipeline, con, hint, None, [], with_context)]
        
        else:
            for i in range(len(patterns)):
                pattern = patterns[i]
                regex = pattern["regex"]
                previous_patterns = [p["regex"] for p in patterns[:i]]
                sample_values = create_sample_distinct_query_regex(con, table_pipeline, column_name, sample_size, 
                                                                   regex_except_list=previous_patterns, regex_list=[regex], with_context=with_context)
                outputs.append((column_name, sample_values, current_type, target_type, database_name, table_pipeline, con, hint, regex, previous_patterns, with_context))
            return outputs
        
    def run(self, extract_output, use_cache=True):
        column_name, sample_values, current_type, target_type, database_name, table_pipeline, con, hint, regex, except_regex, with_context= extract_output
        
        if not is_type_comparable(current_type):
            return self.run_but_fail(extract_output, use_cache)
        
        max_iterations = 5
        
        value_list = sample_values[column_name].tolist()
        truncated_list = [truncate_value(val) for val in value_list]
        sample_values_list_str = f"[{', '.join(truncated_list)}]"
        
        column_intro = f"'{column_name}' has the following distinct values: {sample_values_list_str}"
        if regex:
            column_intro = f"'{column_name}' has the regex pattern '{regex}', and the following distinct values: {sample_values_list_str}"

        template = f"""{column_intro}
Task: Transform the data type of the column from '{current_type}' to '{target_type}', in a simple cast clause.
Note that we use {database_name} syntax. {hint}

Return the result in yml
```yml
reasoning: >
    To transform, we need to ...

cast_clause: |
    CAST({column_name} AS {target_type})
```"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        reasoning = summary["reasoning"]
        
        def clean_clause(clause):

            if clause.lower().startswith("select"):
                clause = clause[6:].strip()

            
            if database_name == "DuckDB" and is_type_string(current_type):
                clause = unescape_regex(clause)
            
            if clause.lower().endswith(f"as {column_name}"):
                clause = clause[:-(len(column_name) + 3)].strip()
                
            clause = clause.replace(enclose_table_name(column_name, con=con), column_name).replace(column_name, enclose_table_name(column_name, con=con))
            return clause

        summary["cast_clause"] = clean_clause(summary["cast_clause"]) 
        
        for i in range(max_iterations):
            sql = f'SELECT {summary["cast_clause"]} AS {enclose_table_name(column_name, con=con)}'
            sql += f'\nFROM {table_pipeline}'
            if regex:
                sql += f'\nWHERE {create_regex_match_clause(con, column_name, regex)}'
            for each_except_regex in except_regex:
                sql += f'\nAND NOT {create_regex_match_clause(con, column_name, each_except_regex)}'
            sql = with_context + "\n" + sql
            
            if database_name == "SQL Server":
                try:
                    sqlglot.transpile(sql, read="tsql", write="tsql", comments=False)
                except:
                    return self.run_but_fail(extract_output, use_cache)


            try:
                df = run_sql_return_df(con, sql)
                break
            except Exception: 
                detailed_error_info = get_detailed_error_info()
                template = f"""{column_intro}
You have the following CAST clause:
{summary['cast_clause']}
For query: SELECT [cast_clause] AS {column_name} FROM table
It has an error: {detailed_error_info}

Please correct the CAST clause, but don't change the logic.
Note that we use {database_name} syntax. {hint}
Return the result in yml
```yml
reasoning: >
    The error is caused by ...

cast_clause: |
    CAST({column_name} AS {target_type})
```
"""

                messages = [{"role": "user", "content": template}]
                response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=(i == 0))
                messages.append(response['choices'][0]['message'])
                self.messages.append(messages)

                yml_code = extract_yml_code(response['choices'][0]['message']["content"])
                summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
                summary["cast_clause"] = clean_clause(summary["cast_clause"]) 
                
        if i == max_iterations - 1:
            return self.run_but_fail(extract_output, use_cache)
        
        summary["reasoning"] = reasoning
        summary["regex"] = regex if regex else ".*"
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        column_name, sample_values, current_type, target_type, database_name, table_pipeline, con, hint, regex, except_regex, with_context= extract_output
        return {"reasoning": "Fail to cast", "cast_clause": enclose_table_name(column_name, con=con)}
    
    def merge_run_output(self, run_outputs):
        if len(run_outputs) == 1:
            return run_outputs[0]
        
        for run_output in run_outputs:
            if run_output["reasoning"] == "Fail to cast":
                return run_output
        
        merged_explanation = ""
        merged_clause = "CASE\n"
        con = self.item["con"]
        column_name = self.para["column_name"]
        
        for run_output in run_outputs:
            merged_explanation += run_output["reasoning"] + "\n"
            pattern_match_clause = create_regex_match_clause(con, column_name, run_output['regex'])
            merged_clause += f"    WHEN {pattern_match_clause} THEN {run_output['cast_clause']}\n"
        
        merged_clause += f"END"
        
        if not cocoon_main_setting['DEBUG_MODE']:
            try:
                table_pipeline = self.para["table_pipeline"]
                with_context = table_pipeline.get_codes(mode="WITH")
            
                sql = f'SELECT {merged_clause} AS {enclose_table_name(column_name, con=con)}'
                sql += f'\nFROM {table_pipeline}'
                sql = with_context + "\n" + sql
                df = run_sql_return_df(con, sql)
            except Exception: 
                return {"reasoning": "Fail to cast", "cast_clause": enclose_table_name(column_name, con=con)}
            
        return {"reasoning": merged_explanation, "cast_clause": merged_clause}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if run_output["reasoning"] != "Fail to cast":
            self.para["table_object"].patterns[self.para["column_name"]] = []
        return callback(run_output)
    

class TransformTypeForAll(MultipleNode):
    default_name = 'Transform Type For All'
    default_description = 'This node allows users to transform the data type for all columns.'

    def construct_node(self, element_name, current_type="", target_type="", idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        para["current_type"] = current_type
        para["target_type"] = target_type
        node = TransformType(para=para, id_para ="column_name")
        node.inherit(self)
        return node

    def extract(self, item):

        document = self.get_sibling_document("Decide Data Type")
        df = pd.read_json(document, orient="split")
        
        self.elements = []
        self.nodes = {}
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('transform_data_type') is False):
            return
        
        for idx, row in df.iterrows():
            column_name = row['Column']
            current_type = row['Current Type']
            target_type = row['Target Type']
            if current_type != target_type:
                self.elements.append(column_name)

        idx = 0
        for idx, row in df.iterrows():
            column_name = row['Column']
            current_type = row['Current Type']
            target_type = row['Target Type']
            if current_type != target_type:
                self.nodes[column_name] = self.construct_node(column_name, 
                                                                current_type, target_type,
                                                              idx, len(self.elements))
                idx += 1

    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        
        data = {
            'Column Name': [],
            'Clause': [],
            'Reasoning': []
        }

        decided_column_type = self.get_sibling_document("Decide Data Type")
        decided_column_type_df = pd.read_json(decided_column_type, orient="split")
        
        if "Transform Type" in document:
            for column_name, details in document["Transform Type"].items():
                data['Column Name'].append(column_name)
                data['Clause'].append(details['cast_clause'])
                data['Reasoning'].append(details['reasoning'])

        df = pd.DataFrame(data)

        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return

        editable_columns = [False, True, False]
        long_text = ["Reasoning"]
        grid = create_dataframe_grid(df, editable_columns, reset=True, long_text=long_text)

        query_widget = self.item["query_widget"]

        test_button = widgets.Button(
            description='Test Cast',
            disabled=False,
            button_style='info',
            tooltip='Click to test',
            icon='play'
        )
        
        table_pipeline = self.para["table_pipeline"]
        with_context_clause = table_pipeline.get_codes(mode="WITH")
        con = self.item["con"]
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, long_text=long_text)
                query = "SELECT\n"
                for i, row in new_df.iterrows():
                    column_name = row["Column Name"]
                    clause = row["Clause"] + f' AS {enclose_table_name(column_name, con=con)},'
                    query += f'    {enclose_table_name(column_name, con=con)} AS {enclose_table_name(column_name, con=con)},\n'
                    query += indent_paragraph(clause) + "\n"
                query = query[:-2] + f'\nFROM {self.para["table_pipeline"]}'
                query_widget.update_context_value(with_context_clause)
                query_widget.run_query(query)
                print("😎 Query submitted. Check out the data widget!")
        
        test_button.on_click(on_button_clicked)
        
        next_button = widgets.Button(
            description='Endorse Cast',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        def on_button_clicked(b):
            with self.output_context():
                new_df = grid_to_updated_dataframe(grid, long_text=long_text)

                affected_columns = new_df["Column Name"].tolist()
                document = new_df.to_json(orient="split")
                table_pipeline = self.para["table_pipeline"]
                old_table_name = table_pipeline.__repr__(full=False)

                new_table_name = old_table_name + "_casted"
                
                schema = table_pipeline.get_schema(con)
                columns = list(schema.keys())
                non_affected_columns = [enclose_table_name(col, con=con) for col in columns if col not in affected_columns]

                sql_query = "SELECT\n"
                sql_query += indent_paragraph(",\n".join(non_affected_columns + [f"{row['Clause']} \nAS \"{row['Column Name']}\"" for i, row in new_df.iterrows()]))

                comment = "-- Column Type Casting: \n"

                for idx, row in decided_column_type_df.iterrows():
                    column_name = row['Column']
                    current_type = row['Current Type']
                    target_type = row['Target Type']
                    if current_type != target_type:
                        comment += f"-- {column_name}: from {current_type} to {target_type}\n"

                sql_query = comment + sql_query
                
                sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}" 
                step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con)
                table_pipeline.add_step_to_final(step)
                database = self.para.get("database", None)
                schema = self.para.get("schema", None)
                try:
                    self.para["table_pipeline"].materialize(con, database=database, schema=schema)
                    schema = table_pipeline.get_schema(con)
                    database_name = get_database_name(con)
                    
                    table_object = self.para["table_object"]
                    for idx, row in new_df.iterrows():
                        column = row['Column Name']
                        
                        if column in table_object.data_type:
                            current_type = get_reverse_type(schema[column], database_name)
                            table_object.data_type[column]["current_data_type"] = current_type
                            
                            if "expected_data_type" in table_object.data_type[column]:
                                if table_object.data_type[column]["expected_data_type"] == current_type:
                                    del table_object.data_type[column]["expected_data_type"]

                except Exception as e:
                    if cocoon_main_setting['DEBUG_MODE']:
                        raise e
                    write_log(f"""
The node is {self.name}
Error in materialization!!
The error is: {e}
""")  
                    self.para["table_pipeline"].remove_final_node()
                    
                
                callback(document)
        
        next_button.on_click(on_button_clicked)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
        create_progress_bar_with_numbers(1, doc_steps)
        display(HTML(f"😎 We have written the clause to cast the columns:"))
        display(grid)
        display(HTML(f"🧪 Please test the cast and ensure the result is as expected."))
        display(test_button)
        display(next_button)
        


class DecideTrim(Node):
    default_name = 'Decide Trim'
    default_description = 'This node allows users to decide how to handle leading and trailing spaces.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML("🔍 Deciding Trim for leading/trailing white spaces..."))
        create_progress_bar_with_numbers(2, doc_steps)

        con = self.item["con"]
        database_name = get_database_name(con)

        table_pipeline = self.para["table_pipeline"]
        schema = table_pipeline.get_schema(con)
        columns = []

        with_context = table_pipeline.get_codes(mode="WITH")
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('trim') is False):
            
            pass
        
        else:
            for column in schema:
                current_type = get_reverse_type(schema[column], database_name)

                if database_name == "SQL Server":
                    if schema[column].lower() in ['text', 'ntext', 'image']:
                        print(f"⚠️ The column {column} has type {schema[column]} that cannot be compared or sorted in SQL Server. Skipping this node.")
                        continue

                if is_type_string(current_type):
                    def where_clause_for_space(column_name, con):
                        return f'{enclose_table_name(column_name, con=con)} <> TRIM({enclose_table_name(column_name, con=con)})'

                    where_clause = where_clause_for_space(column, con)
                    count = run_sql_return_df(con, f'{with_context} \n SELECT COUNT(*) FROM {table_pipeline} WHERE {where_clause}').iloc[0, 0]
                    if count > 0:
                        columns.append(column)

        sample_size = 2
        sample_df = None
        if len(columns) > 0:
            sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
            sample_df = sample_df.applymap(truncate_cell)
            
        return columns, sample_df
    
    def run(self, extract_output, use_cache=True):
        columns, sample_df = extract_output

        if len(columns) == 0:
            return {}, columns

        template = f"""You have a table, and the following columns have leading or trailing spaces:
{sample_df.to_csv(index=False, quoting=1)}

Task: Decide whether to keep the leading and trailing spaces.
Most of the time, they are not meaningful.
Some special cases are: password, code snippet...

Return in the following format:
```json
{{
    "reasoning": "The columns mean ... The spaces are (not) meaningful...",
    "columns_to_keep_spaces": ["col1", ...]
}}
```"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "reasoning" in jc and "columns_to_keep_spaces" in jc, "The 'reasoning' or 'columns_to_keep_spaces' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["columns_to_keep_spaces"], list), "The 'columns_to_keep_spaces' value is not a list."),
            (lambda jc: all(col_name in columns for col_name in jc["columns_to_keep_spaces"]), "One or more column names specified in 'columns_to_keep_spaces' are not present in the columns list."),
        ]

        for check, error_message in checks:
            if not check(json_code):
                raise ValueError(f"Validation failed: {error_message}")

        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"reasoning": "failed", "columns_to_keep_spaces": []}, 


    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        json_code = run_output
        columns, _  = extract_output
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        df = pd.DataFrame(columns=["Column Name", "Trim?"])

        df["Column Name"] = columns

        df["Trim?"] = [not (col in json_code["columns_to_keep_spaces"]) for col in columns]

        grid = create_dataframe_grid(df, [False, True], reset=True)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return

        query_widget = self.item["query_widget"]
        table_pipeline = self.para["table_pipeline"]

        con = self.item["con"]
        columns = [enclose_table_name(column, con=con) for column in columns]
        columns_str = ", ".join(columns)

        explore_button = widgets.Button(
            description='Explore',
            disabled=False,
            button_style='info',
            tooltip='Click to explore',
            icon='search'
        )

        with_context_clause = table_pipeline.get_codes(mode="WITH")
                
        def on_button_clicked(b):
            with self.output_context():
                print("😎 Query submitted. Check out the data widget!")
                query_widget.update_context_value(with_context_clause)
                query_widget.run_query(f"SELECT DISTINCT {columns_str} FROM {table_pipeline}")

        explore_button.on_click(on_button_clicked)

        next_button = widgets.Button(
            description='Accept Trim',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        reject_button = widgets.Button(
            description='Reject Trim',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='times'
        )
        
        def on_button_clicked2(b):
            with self.output_context():
                clear_output(wait=True)
                new_df =  grid_to_updated_dataframe(grid)
                new_df["Trim?"] = False
                callback(new_df.to_json(orient="split"))
            

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                new_df = grid_to_updated_dataframe(grid)
                
                if new_df["Trim?"].any():
                    sql_pipeline = self.para["table_pipeline"]
                    old_table_name = table_pipeline.__repr__(full=False)
                    new_table_name = old_table_name + "_trimmed"

                    con = self.item["con"]
                    schema = self.para["table_pipeline"].get_schema(con)
                    all_columns = list(schema.keys())
                    non_affected_columns = [enclose_table_name(col, con=con) for col in all_columns if col not in new_df["Column Name"].tolist()]
                    sql_query = "SELECT\n"

                    columns_to_trim = []
                    columns_not_to_trim = []

                    for i, row in new_df.iterrows():
                        column_name = row["Column Name"]
                        trim = row["Trim?"]
                        if trim:
                            columns_to_trim.append(enclose_table_name(column_name, con=con))
                        else:
                            columns_not_to_trim.append(enclose_table_name(column_name, con=con))

                    sql_query += indent_paragraph(",\n".join(non_affected_columns + \
                                                            columns_not_to_trim +\
                                                            [f'TRIM({col}) AS {col}' for col in columns_to_trim]))

                    
                    sql_query = "-- Trim Leading and Trailing Spaces\n" + sql_query
                    sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"  

                    step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=self.item["con"])
                    sql_pipeline.add_step_to_final(step)
                    database = self.para.get("database", None)
                    schema = self.para.get("schema", None)
                    try:
                        self.para["table_pipeline"].materialize(con, database=database, schema=schema)
                    except Exception as e:
                        if cocoon_main_setting['DEBUG_MODE']:
                            raise e
                        write_log(f"""
The node is {self.name}
Error in materialization!!
The error is: {e}
""")  
                        self.para["table_pipeline"].remove_final_node()
                        
                callback(new_df.to_json(orient="split"))

        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        if "viewer" in self.para and self.para["viewer"]:
            on_button_clicked(next_button)
            return
        
        display(HTML(f"😎 We have found the following columns with leading or trailing spaces, and recommend to <b>trim</b> them:"))
        display(explore_button)
        display(grid)
        display(HBox([reject_button, next_button]))
        
class DecideStringUnusual(Node):
    default_name = 'Decide String Unusual'
    default_description = 'This node allows users to decide how to handle unusual values.'
    default_sample_size = 20

    def extract(self, input_item):
        clear_output(wait=True)
        
        idx = self.para["column_idx"]
        total = self.para["total_columns"]

        self.input_item = input_item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        column_name = self.para["column_name"]
        
        display(HTML(f"{running_spinner_html} Understanding unusual values for <i>{table_pipeline}[{column_name}]</i>"))
        create_progress_bar_with_numbers(3, doc_steps)
        show_progress(max_value=total, value=idx)
        
        sample_size = self.class_para.get("sample_size", self.default_sample_size)

        query = create_sample_distinct_query(con, table_pipeline, column_name, sample_size)
        with_context = table_pipeline.get_codes(mode="WITH")
        query = with_context + "\n" + query
        sample_values = run_sql_return_df(con,query)
        
        return column_name, sample_values

    def run(self, extract_output, use_cache=True):
        column_name, sample_values = extract_output
        
        if len(sample_values) == 0:
            return {"Reasoning": "Column is fully missing", "Unusualness": False}
        sample_values_list = sample_values[column_name].values.tolist()
        truncated_list = [truncate_value(val) for val in sample_values_list]
        sample_values_list_str = f"[{', '.join(truncated_list)}]"
        
        template = f"""{column_name} has the following distinct values: {sample_values_list_str}

Review if there are
1. Any weird characters/typo (e.g., "cofffee") Note that escape characters are expected.
2. Redundant/Inconsistent representations of the same coencept (e.g., "New York" and "NY")
If so, report them as unusual values.

Now, respond in Json:
```json
{{
    "Reasoning": "The valuses are ... They are unusual/acceptable ...",
    "Unusualness": true/false,
    "Examples": "xxx values are unusual because ..." (<20 words, empty if not unusual) 
}}
```"""
        
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "Reasoning" in jc, "The 'Reasoning' key is missing in the JSON code."),
            (lambda jc: "Unusualness" in jc, "The 'Unusualness' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["Unusualness"], bool), "The value of 'Unusualness' must be a boolean."),
            (lambda jc: "Examples" in jc, "The 'Examples' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["Examples"], str), "The value of 'Examples' must be a string."),
        ]

        for check, error_message in checks:
            if not check(json_code):
                raise ValueError(f"Validation failed: {error_message}")

        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"Reasoning": "Fail to run", "Unusualness": False}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
        
        
class DecideStringUnusualForAll(MultipleNode):
    default_name = 'Decide Unusual String For All'
    default_description = 'This node allows users to decide how to handle unusual values for all columns.'
    default_sample_size = 20
    
    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        node = DecideStringUnusual(para=para, 
                                   id_para ="column_name",
                                   class_para={"sample_size": self.class_para.get("sample_size", self.default_sample_size)})
        node.inherit(self)
        return node

    def extract(self, item):
        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        database_name = get_database_name(con)
        
        schema = table_pipeline.get_schema(con)

        columns = []
        for col, col_type in schema.items():
            reverse_type = get_reverse_type(col_type, database_name)
            if is_type_string(reverse_type):
                if database_name == "SQL Server":
                    if col_type.lower() not in ['text', 'ntext', 'image']:
                        columns.append(col)
                else:
                    columns.append(col)

        self.elements = []
        self.nodes = {}
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('standardize_values') is False):
            return

        idx = 0
        for col in columns:
            self.elements.append(col)
            self.nodes[col] = self.construct_node(col, idx, len(columns))
            idx += 1
        







            



            
            
            

            
            

                    
                        



    
    
def create_dbt_schema_yml(table_name, table_summary, columns, column_desc, 
                          miss_df=None, unique_df=None, categorical_df=None, unusual_df=None):
    yml_content = f"""version: 2
models:
  - name: {table_name}
    description: "{table_summary}"
    columns:"""
    for column in columns:
        description = column_desc.loc[column_desc["New Column Name"].str.lower() == column.lower(), 'Summary'].values[0]
        tests = []
        cocoon_meta = {}
        
        if miss_df is not None:
            is_null_acceptable = miss_df.loc[miss_df['Column'].str.lower() == column.lower(), 'Is NULL Acceptable?'].values
            if is_null_acceptable.size == 0 or not is_null_acceptable[0]:
                tests.append("not_null")
            else:
                explanation = miss_df.loc[miss_df['Column'].str.lower() == column.lower(), 'Explanation'].values[0]
                cocoon_meta["missing_reason"] = explanation
        else:
            tests.append("not_null")
                
        if unusual_df is not None:
            unusual_reason  = unusual_df.loc[unusual_df['Column'].str.lower() == column.lower(), 'Explanation'].values
            if unusual_reason.size != 0:
                cocoon_meta["unusual_values"] = unusual_reason[0]
                      
        if unique_df is not None:
            is_unique = unique_df.loc[unique_df['Column'].str.lower() == column.lower(), 'Is Unique?'].values
            if is_unique.size > 0 and is_unique[0]:
                tests.append("unique")
                
        if categorical_df is not None:
            is_categorical = categorical_df.loc[categorical_df['Column'].str.lower() == column.lower(), 'Is Categorical?'].values
            if is_categorical.size > 0 and is_categorical[0]:
                accepted_values = categorical_df.loc[categorical_df['Column'].str.lower() == column.lower(), 'Domain'].values[0]
                if not isinstance(accepted_values, list):
                    if accepted_values.startswith("[") and accepted_values.endswith("]"):
                        accepted_values = ast.literal_eval(accepted_values)
                    else:
                        accepted_values = accepted_values.split(",")
                tests.append(f"accepted_values:\n{indent_paragraph('values: [' + ', '.join(accepted_values) + ']', spaces=4)}")
        
        
        yml_content += f"""
      - name: {column}
        description: "{description}\""""

        if tests:
            yml_content += f"""
        tests:
{indent_paragraph(chr(10).join(f"- {test}" for test in tests), spaces=10)}"""

        if cocoon_meta:
            cocoon_meta_yaml = yaml.dump(cocoon_meta, default_flow_style=False).strip()
            yml_content += f"""
        cocoon_meta:
{indent_paragraph(cocoon_meta_yaml, spaces=10)}"""

    return yml_content





class WriteStageYMLCode(Node):
    default_name = 'Write Stage Code'
    default_description = 'This node allows users to write the code for the stage.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        border_style = """
<style>
.border-class {
    border: 1px solid black;
    padding: 10px;
    margin: 10px;
}
</style>
"""

        tab_data = []
        labels = []
        file_names = []
        contents = []
        table_pipeline = self.para["table_pipeline"]
        old_table_name = table_pipeline.get_source_step().name
        
        if len(table_pipeline.steps) > 1:
            new_table_name = rename_for_stg(old_table_name)
        else:
            new_table_name = old_table_name
        
        table_name = new_table_name
        con = self.item["con"]
        database_name = get_database_name(con)
        schema = table_pipeline.get_schema(con)
        
        columns = list(schema.keys())
        
        table_object = self.para["table_object"]
        table_object.columns = columns
        table_object.table_name = table_name
        
        yml_dict = table_object.create_dbt_schema_dict()
        yml_content = yaml.dump(yml_dict, default_flow_style=False)
        
        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"

        highlighted_yml = highlight(yml_content, YamlLexer(), formatter)

        bordered_content = f'<div class="border-class">{highlighted_yml}</div>'

        combined_html = css_style + border_style + bordered_content

        tab_data.append(("yml", combined_html))
        labels.append("YML")
        file_names.append(f"{table_name}.yml")
        contents.append(yml_content)
        
        if "yaml_only" not in self.class_para or not self.class_para["yaml_only"]:
            if len(table_pipeline.steps) > 1:
                sql_query = f'SELECT *'
                final_table_name = table_pipeline.__repr__(full=False)
                if final_table_name != new_table_name:
                    sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"   
                    step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=self.item["con"])
                    table_pipeline.add_step_to_final(step)
                    
                formatter = HtmlFormatter(style='default')
                css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"
                combined_css = css_style + border_style
                sql_query = table_pipeline.get_codes()
                highlighted_sql = wrap_in_scrollable_div(highlight(sql_query, SqlLexer(), formatter), height='600px')
                bordered_content = f'<div class="border-class">{highlighted_sql}</div>'
                combined_html = combined_css + bordered_content
                tab_data.append(("sql", combined_html))
                labels.append("SQL")
                file_names.append(f"{new_table_name}.sql")
                contents.append(sql_query)

        try:
            if (hasattr(self, 'para') and 
                isinstance(self.para, dict) and 
                isinstance(self.para.get('cocoon_stage_options'), dict) and 
                self.para['cocoon_stage_options'].get('generate_html_report') is True):
                table_pipeline = self.para["table_pipeline"]
                table_object = self.para["table_object"]
                before_table_name = table_pipeline.get_source_step()
                after_table_name = table_pipeline.get_final_step().name
            
                bottom_html = "<h1>Cocoon Cleaning Summary 🤗</h1><hr><br>"
                bottom_idx = 1

                table_summary = table_object.table_summary
                if table_summary != "":
                    bottom_html += f"""<h2>📃 {bottom_idx}. Table Summary</h2><br>{table_summary}<br><br><br>"""
                    bottom_idx += 1

                column_document = self.get_sibling_document('Describe Columns')
                if column_document:
                    column_df = pd.read_json(column_document, orient="split")
                    sample_size = 100
                    
                    if len(column_df) > 0:
                        renamed_columns = column_df[column_df['Column'] != column_df['New Column Name']]
                        bottom_html += f"""<h2>📊 {bottom_idx}. Column Rename</h2><br>
                        {"😎 <b>" + str(len(renamed_columns)) + "</b> columns have been renamed" if len(renamed_columns) > 0 else "🤓 No column is renamed"}<br>
                        {column_df.to_html()}<br><br><br>"""
                        bottom_idx += 1
                        
                        column_mapping = dict(zip(column_df['New Column Name'], column_df['Column']))
                        final_columns = table_object.columns
                        
                        after_columns = [col for col in final_columns]
                        after_df = table_pipeline.get_samples(con, columns=after_columns, sample_size=sample_size)
                        before_selection = ", ".join([enclose_table_name(column_mapping[col], con=con) for col in final_columns])
                        
                        before_query = f'SELECT {before_selection} FROM {before_table_name}'
                        before_sample_query = sample_query(con, before_query, sample_size)
                        before_df = run_sql_return_df(con, before_sample_query)
                    else:
                        before_query = f'SELECT * FROM "{before_table_name}"'
                        before_sample_query = sample_query(con, before_query, sample_size)
                        before_df = run_sql_return_df(con, before_sample_query)
                        after_df =table_pipeline.get_samples(con, sample_size=sample_size)

                    
                duplication_document = self.get_sibling_document('Decide Duplicate')
                if duplication_document:
                    duplication_count = duplication_document['duplicate_count']
                    if duplication_count > 0:
                        duplication_removed = duplication_document['duplicate_removed']
                        bottom_html += f"""<h2>👯‍♀️ {bottom_idx}. Duplicate</h2><br>
                        {"🔍 <b>" + str(duplication_count) + "</b> duplicates have been detected" }<br>
                        {"✔️ Duplicates have been removed" if duplication_removed else "❌ Duplicates have <b>not</b> been removed"}<br><br><br>"""
                        bottom_idx += 1 

                    trim_document = self.get_sibling_document('Decide Trim')
                    trim_df = pd.read_json(trim_document, orient="split")
                    if len(trim_df) > 0:
                        trimed_column_count = len(trim_df[trim_df['Trim?']])
                        bottom_html += f"""<h2>✂️ {bottom_idx}. Leading and Trailing Whitespace</h2><br>
                        {"🔍 <b>" + str(len(trim_df)) + "</b> columns have leading and trailing whitespace"}<br>
                        {"✔️ <b>" + str(trimed_column_count) + "</b> columns have been trimmed" if trimed_column_count > 0 else "❌ No column is trimmed"}<br><br><br>"""
                        bottom_idx += 1

                clean_unusual_document = self.get_sibling_document('Clean Unusual For All')
                if clean_unusual_document:
                    clean_unusual_document = clean_unusual_document['Clean Unusual']
                    bottom_html += f"""<h2>🚧 {bottom_idx}. Erroneous Values</h2><br>
                {"🔍 <b>" + str(len(clean_unusual_document)) + "</b> columns have erroneous values"}<br>"""
                    bottom_idx += 1
                    bottom_html += "<ol>"
                    for column in clean_unusual_document:
                        value = clean_unusual_document[column]
                        unusual_reason = value['unusual_reason']
                        bottom_html += f"<li><b>{column}</b>: {unusual_reason}<br>"
                        could_clean = value['could_clean']
                        bottom_html += f"{'✔️ The column has been cleaned:' if could_clean else '❌ The column has not been cleaned'}<br>"
                        if could_clean:
                            mapping = value['mapping']
                            mapping_df = pd.DataFrame(list(mapping.items()), columns=["Original Value", "Cleaned Value"])
                            bottom_html += mapping_df.to_html()
                        bottom_html += "<br></li>"
                    bottom_html += "</ol><br>"

                dmv_document = self.get_sibling_document('Decide Disguised Missing Values For All')
                if dmv_document:
                    dmv_df = pd.read_json(dmv_document, orient="split")
                    if len(dmv_df) > 0:
                        dmv_column_count = len(dmv_df[dmv_df["Impute to NULL?"]])
                        bottom_html += f"""<h2>🕵️‍♂️ {bottom_idx}. Disguised Missing Values</h2><br>
                        {"🔍 <b>" + str(len(dmv_df)) + "</b> columns have disguised missing values"}<br>
                        {"✔️ <b>" + str(dmv_column_count) + "</b> columns have been cleaned" if dmv_column_count > 0 else "❌ No column is cleaned"}<br>
                        {dmv_df.to_html()}<br><br><br>"""
                        bottom_idx += 1

                transform_document = self.get_sibling_document('Transform Type For All')
                if transform_document:
                    transform_df = pd.read_json(transform_document, orient="split")
                    if len(transform_df) > 0:
                        bottom_html += f"""<h2>🔧 {bottom_idx}. Data Type</h2><br>
                        {"✔️ <b>" + str(len(transform_df)) + "</b> columns have been casted"}<br>
                        {transform_df.to_html()}<br><br><br>"""
                        bottom_idx += 1
                
                fd_document = self.get_sibling_document('Decide If Functional Dependency For All')
                if fd_document and 'Decide If Functional Dependency' in fd_document:
                    fd_document = fd_document['Decide If Functional Dependency']
                    bottom_html += f"""<h2>🔗 {bottom_idx}. Functional Dependencies</h2><br>
                    {"🔍 <b>" + str(len(fd_document)) + "</b> functional dependencies found"}<br>"""
                    bottom_idx += 1
                    bottom_html += "<ol>"
                    for column_a in fd_document:
                        value = fd_document[column_a]
                        if value['repair_dfs']:
                            bottom_html += f"<li><b>{column_a}</b> determines:<br>"
                            for column_b in value['repair_dfs']:
                                bottom_html += f"<b>{column_b}</b>:<br>"
                                repair_df = pd.read_json(value['repair_dfs'][column_b], orient="split")
                                bottom_html += repair_df.to_html()
                                bottom_html += "<br>"
                            explanation = value['explanation']
                            bottom_html += f"<b>Explanation:</b> {explanation}<br>"
                            bottom_html += "</li>"
                    bottom_html += "</ol><br>"
        
                missing_document = self.get_sibling_document('Decide Missing Values')
                if missing_document:
                    missing_df = pd.read_json(missing_document, orient="split")
                    if len(missing_df) > 0:
                        acceptable_missing_count = len(missing_df[missing_df["Is NULL Acceptable?"]])
                        bottom_html += f"""<h2>❓ {bottom_idx}. Missing Values</h2><br>
                        {"🔍 <b>" + str(len(missing_df)) + "</b> columns have missing values"}<br>
                        {"✔️ <b>" + str(acceptable_missing_count) + "</b> of them are acceptable" if acceptable_missing_count > 0 else "❌ No missing value is acceptable"}<br>
                        {missing_df.to_html()}<br>"""
                        
                        handle_missing_document = self.get_sibling_document('Handle Missing Values')
                        if handle_missing_document:
                            handle_missing_df = pd.read_json(handle_missing_document, orient="split")
                            
                            if len(handle_missing_df) > 0:
                                bottom_html += f"""🧩 These missing values are handled as follows:<br>
                                {handle_missing_df.to_html()}<br>"""
                                
                        bottom_html += "<br><br>"
                        bottom_idx += 1
                    
                column_html = ""        
                column_javascript = ""
                schema = table_pipeline.get_schema(con)
                

                for column in yml_dict["models"][0]["columns"]:
                    column_name = column["name"]
                    data_type = schema[column_name]
                    data_type = get_reverse_type(data_type, database_name)
                    
                    single_column_html = ""
                    
                    tests = column.get('tests', [])
                    for test in tests:
                        if isinstance(test, dict):
                            test_type = list(test.keys())[0]
                            test_value = test[test_type]
                            option_html = "<select>"
                            for value in test_value["values"]:
                                option_html += f"<option value='{value}'>{value}</option>"
                            option_html += "</select>"
                            single_column_html += f'''<span class="tag tag-purple">{test_type}{option_html}</span>'''
                        elif test == 'not_null':
                            single_column_html += f'''<span class="tag tag-red">{test}</span>'''
                        elif test == 'unique':
                            single_column_html += f'''<span class="tag tag-blue">{test}</span>'''
                    
                    if len(tests) > 0:
                        single_column_html += "<br><br>"
                            
                    html_output, javascript_output = build_column_viz(column_name, is_numeric=is_type_numeric(data_type), con=con, table_name=table_pipeline)
                    single_column_html += html_output
                    single_column_html += "<b>📃 Summary:</b> " + column["description"] + "<br>"
                                
                    cocoon_meta = column.get('cocoon_meta', {})
                    if 'missing_reason' in cocoon_meta:
                        if cocoon_meta['missing_reason']:
                            single_column_html += "<b>❓ Missing:</b> " + str(cocoon_meta['missing_reason']) + "<br>"
                        else:
                            single_column_html += "<b>❓ Missing:</b> Reason unknown <br>"
                    if 'unusual_values' in cocoon_meta:
                        single_column_html += "<b>🚧 Erroneous:</b> " + str(cocoon_meta['unusual_values']) + "<br>"
                    if 'uniqueness' in cocoon_meta:
                        single_column_html += "<b>🦄 Unique:</b> " + str(cocoon_meta['uniqueness']) + "<br>"
                    if 'patterns' in cocoon_meta:
                        single_column_html += "<b>🎨 Patterns:</b><br>"
                        single_column_html += "<ul>"
                        for pattern in cocoon_meta['patterns']:
                            summary = pattern['summary']
                            regex = pattern['regex']
                            single_column_html += f"<li><code>{regex}</code><br><i>{summary}</i></li>"
                        single_column_html += "</ul>"
                        
                    column_html += f"""
    <div class="card-item card-item-collapsed">
        <span class="field-name">{column_name}</span>
        <div class="card-controls">
            <span class="toggle">▲</span>
        </div>
    </div>
    <div class="indent" style="display: block;">{single_column_html}<br></div>
    """
                    
                    column_javascript += javascript_output
                    
                html_content = f"""<!DOCTYPE html>
    <html lang="en">
    <head>
    <title>Cocoon</title>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">

    {cocoon_html_style_block}

    </head>
    <body>

    <div class="dashboard">

        <div class="main-panel">
            <div class="container">
                {create_cocoon_logo(header="Data Formatting", footer="Powered by Cocoon")}
                <div><h1>{before_table_name.__repr__(full=False)}</h1>(First 100 rows)</div>
                <div>
                    {cocoon_html_toggle_block}
                </div>
            </div>

            <div class="table-container before active">
                {before_df.to_html()}
            </div>
            <div class="table-container after">
                {after_df.to_html()}
            </div>
        </div>
        <div class="right-panel">
    {column_html}
        </div>

        <div class="bottom-panel">
            {bottom_html}
        </div>
    </div>

    <script src="https://d3js.org/d3.v6.min.js"  charset="utf-8"></script>
    <script src="https://d3js.org/topojson.v3.min.js"></script>
    <script>
    {cocoon_js_expand_block}
    {cocoon_js_toggle_block}
    </script>
    <script>
    {cocoon_js_viz_block}
    {column_javascript}

    </script>
    </body>
    </html>"""
                html_tab = wrap_in_iframe(html_content, width="1000px", height="800px")
                tab_data.append(("html", html_tab))
                
                labels.append("HTML")
                file_names.append(f"{new_table_name}.html")
                contents.append(html_content)
        except Exception as e:
            if cocoon_main_setting['DEBUG_MODE']:
                raise e
        
        tabs = create_dropdown_with_content(tab_data) 
        
    
        
        if "dbt_directory" in self.para:
            if not file_exists(os.path.join(self.para["dbt_directory"], "stage")):
                create_directory(os.path.join(self.para["dbt_directory"], "stage"))
            file_names = [os.path.join(self.para["dbt_directory"], "stage", file_name) for file_name in file_names]
       
        create_progress_bar_with_numbers(4, doc_steps)
        display(HTML(f"🎉 Congratulations! Below is the stage codes."))
        display(tabs)
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
        save_files_click(save_button)  
        
        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                document = {}
                
                for i in range(len(labels)):
                    label = labels[i]
                    content = contents[i]
                    document[label] = content

                callback(document)

        submit_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            overwrite_checkbox.value = True
            save_files_click(save_button)  
            on_button_clicked(submit_button)
            return
        
        
        display(submit_button)

def create_stage_workflow(table_name, con, viewer=True):
    query_widget = QueryWidget(con)

    item = {
        "con": con,
        "query_widget": query_widget
    }

    sql_step = SQLStep(table_name=table_name, con=con)
    pipeline = TransformationSQLPipeline(steps = [sql_step], edges=[])

    para = {"table_pipeline": pipeline}

    main_workflow = Workflow("Data Stage", 
                            item = item, 
                            description="A workflow to stage table",
                            para = para)

    main_workflow.add_to_leaf(DecideProjection())
    main_workflow.add_to_leaf(CreateShortTableSummary())
    main_workflow.add_to_leaf(DecideDuplicate())
    main_workflow.add_to_leaf(DescribeColumns())
    main_workflow.add_to_leaf(DecideMissing())
    main_workflow.add_to_leaf(HandleMissing())
    main_workflow.add_to_leaf(DecideDataType())
    main_workflow.add_to_leaf(TransformTypeForAll())
    main_workflow.add_to_leaf(DecideTrim())
    main_workflow.add_to_leaf(DecideStringUnusualForAll())
    main_workflow.add_to_leaf(CleanUnusualForAll())
    main_workflow.add_to_leaf(WriteStageYMLCode())
    
    return query_widget, main_workflow

    
class CreateShortTableSummary(Node):
    default_name = 'Create Short Table Summary'
    default_description = 'This node creates a short summary of the table.'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        
        display(HTML(f"{running_spinner_html} Generating table summary for <i>{table_name}</i>..."))
        create_progress_bar_with_numbers(0, doc_steps)
        self.progress = show_progress(1)
        
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        
        access_control = self.para['cocoon_stage_options'].get('description_access_control', 'Schema Only Access')
        
        if access_control == 'Read Data Access':
            sample_size = 5
            sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
            sample_df = sample_df.applymap(truncate_cell)
            table_desc = sample_df.to_csv(index=False, quoting=1)
        else:
            table_desc = ", ".join(columns)
        
        source_name = table_pipeline.get_source_step().name

        return table_pipeline, table_desc, source_name, access_control
    
    def run(self, extract_output, use_cache=True):
        table_name, table_desc, source_name, access_control = extract_output
        
        table_object = self.para.get("table_object")
        if table_object and hasattr(table_object, 'table_summary') and table_object.table_summary:
            return table_object.table_summary
        
        if access_control == 'Read Data Access':
            template = f"""You have the table '{source_name}' with samples:
{table_desc}

Summarize the table. If it is a relation with multiple entities, explains how they are related.
If it has single entity, explains what the details are

Example: 
The table is the xx relation between abc...
The table is about x, with details of ...

Now, your summary in short simple SVO sentences and < 500 chars
Return in the following format:
```yml
reasoning: It has single entity/multiple enities ...
table_summary: >-
    The table is about the orders made by customers...
```
"""
        else:
            template = f"""You have the table '{source_name}' with the following columns:
{table_desc}

Based on the column names, summarize what this table might represent. If it appears to be a relation with multiple entities, explain how they might be related.
If it seems to represent a single entity, explain what details it might contain.

Example: 
The table appears to be a relation between xx and yy, likely containing...
The table seems to be about x, possibly containing details such as...

Now, provide your summary in short simple SVO sentences and < 500 chars
Return in the following format:
```yml
reasoning: It appears to have single entity/multiple entities ...
table_summary: >-
    The table likely represents orders made by customers...
```
"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)

        summary = response['choices'][0]['message']['content']
        assistant_message = response['choices'][0]['message']
        messages.append(assistant_message)
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        return summary["table_summary"]
    
    def run_but_fail(self, extract_output, use_cache=True):
        return "Please summarize the table"

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output 
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        self.progress.value += 1

        table_name, _, _, _ = extract_output
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_name)

        display(HTML(f"📝 Here is the summary, please keep it concise:"))
        
        text_area, char_count_label = create_text_area_with_char_count(summary, max_chars=500)
        layout = Layout(display='flex', justify_content='space-between', width='100%')
        display(HBox([text_area, char_count_label], layout=layout))

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                print("Submission received.")
                table_object = self.para["table_object"]
                table_object.table_summary = text_area.value
                callback(text_area.value)

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)

        display(submit_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(submit_button)
            return
            
class SelectMainVocabularyTable(Node):
    default_name = 'Select Main and Vocabulary Table'
    default_description = 'This step allows you to select the main table and vocabulary table for the fuzzy join.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        con = self.input_item["con"]
        query_widget = self.item["query_widget"]
        
        tables = get_table_names(con)
        
        display(HTML(f"""🧐 There are {len(tables)} tables in your database.<br>
🤓 Please select the <b>main table</b> and <b>vocabulary table</b>.<br>
Each row in the <b>main table</b> will be mapped to vocabulary.<br>
If unsure, choose the smaller table as the <b>main table</b>.<br><br>
<b>Main Table:</b>"""))
        

        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        display(HTML("<b>Vocabulary Table:</b>"))
        dropdown_vocabulary = create_explore_button(query_widget, table_name=tables)

        next_button = widgets.Button(description="Next", button_style='success')

        def on_button_click(b):
            with self.output_context():
                main_table = dropdown.value
                vocabulary_table = dropdown_vocabulary.value
                selected_tables = {"main_table": main_table, "vocabulary_table": vocabulary_table}
                callback(selected_tables)
                    
        next_button.on_click(on_button_click)
        display(next_button)
        
        
class DecideFuzzyColumns(Node):
    default_name = 'Decide Fuzzy Columns'
    default_description = 'This allows users to select the columns for fuzzy join.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        con = self.item["con"]
        vocabulary_table = self.get_sibling_document("Select Main and Vocabulary Table")["vocabulary_table"]
        main_table = self.get_sibling_document("Select Main and Vocabulary Table")["main_table"]
        
        vocabulary_schema = get_table_schema(con, vocabulary_table)
        main_schema = get_table_schema(con, main_table)

        print(f"🔍 Exploring the tables:")
        query_widget = self.item["query_widget"]
        create_explore_button(query_widget, table_name=[vocabulary_table, main_table])

        print(f"🧐 Please select the columns for fuzzy match.")
        print(f"😊 For the vocabulary table: {vocabulary_table}")
        vocabulary_columns = list(vocabulary_schema.keys())
        vocabulary_multi_select = create_column_selector_(columns=vocabulary_columns)
        
        print(f"😊 For the main table: {main_table}")
        main_columns = list(main_schema.keys())
        main_multi_select = create_column_selector_(columns=main_columns)
        
        next_button = widgets.Button(description="Next", button_style='success')

        def callback_next(button):
            with self.output_context():
                selected_vocabulary_columns = vocabulary_multi_select.value
                selected_main_columns = main_multi_select.value
                
                if not selected_vocabulary_columns or not selected_main_columns:
                    print("🙁 Please select at least one column for each table.")
                    return
                
                selected_columns = {"vocabulary_columns": [vocabulary_columns[i] for i in selected_vocabulary_columns],
                                    "main_columns": [main_columns[i] for i in selected_main_columns]}
                clear_output(wait=True)            
                callback(selected_columns)
            
        next_button.on_click(callback_next)
        
        display(next_button)
        
class PrepareFuzzyJoin(Node):
    default_name = 'Prepare Fuzzy Join'
    default_description = 'This step allows you to prepare the fuzzy join.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        con = self.item["con"]
        vocabulary_table = self.get_sibling_document("Select Main and Vocabulary Table")["vocabulary_table"]
        main_table = self.get_sibling_document("Select Main and Vocabulary Table")["main_table"]
        selected_columns = self.get_sibling_document("Decide Fuzzy Columns")
        vocabulary_columns = selected_columns["vocabulary_columns"]
        main_columns = selected_columns["main_columns"]
        
        query = f"SELECT DISTINCT {', '.join(vocabulary_columns)} FROM {vocabulary_table}"
        vocabulary_df = run_sql_return_df(con, query)
        vocabulary_df.fillna(' ', inplace=True)
        vocabulary_df['label'] = vocabulary_df.agg(''.join, axis=1)
        query = f"SELECT DISTINCT {', '.join(main_columns)} FROM {main_table}"
        main_df = run_sql_return_df(con, query)
        main_df.fillna(' ', inplace=True)
        main_df['label'] = main_df.agg(''.join, axis=1)
        
        print(f"🔍 There are {len(vocabulary_df) + len(main_df)} unique vocabularies. We are preparing them...")
        
        if len(vocabulary_df) + len(main_df) > 10000:
            print(f"🤔 The number of unique vocabularies is too many.")
            print(f"😊 Support for large vocabulary is under development.")
            return
        
        for table, df in [(vocabulary_table, vocabulary_df), (main_table, main_df)]:
            embedding_file = f"{table}_cocoon_embedding"
            embed_labels(df, f'./{embedding_file}.csv')
            embedding_df = pd.read_csv(f'./{embedding_file}.csv')
            
        callback({})
    
class MatchEntity(Node):
    default_name = 'Match Entity'
    default_description = 'This step allows you to match entities.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        con = self.item["con"]
        vocabulary_table = self.get_sibling_document("Select Main and Vocabulary Table")["vocabulary_table"]
        main_table = self.get_sibling_document("Select Main and Vocabulary Table")["main_table"]
        selected_columns = self.get_sibling_document("Decide Fuzzy Columns")
        vocabulary_columns = selected_columns["vocabulary_columns"]
        main_columns = selected_columns["main_columns"]
        
        vocabulary_df = pd.read_csv(f'./{vocabulary_table}_cocoon_embedding.csv')
        main_df = pd.read_csv(f'./{main_table}_cocoon_embedding.csv')
        main_df = main_df.head(10)
        
        print(f"🔍 Matching rows... We will only match the first 10 rows for trial.")
        print(f"😊 Please let us know if you want the complete feature!")
        index = load_embedding(vocabulary_df, label_embedding='embedding')
        D, I = df_search(main_df, index)
        
        entity_relation_match(input_df = main_df, 
                            attributes = main_columns,
                            I = I,
                            refernece_df = vocabulary_df)
        
        html_content = generate_report(main_df)
        
        display(HTML(wrap_in_iframe(html_content)))
        ask_save_file(f"Cocoon_EM_{main_table}.html", html_content)
        

def create_matching_workflow(con, query_widget=None, viewer=True, output=None):

    if query_widget is None:
        query_widget = QueryWidget(con)

    query_widget = QueryWidget(con)

    item = {
        "con": con,
        "query_widget": query_widget,
    }
    
    if output is None:
        output = widgets.Output()

    para={}

    main_workflow = Workflow("Fuzzy Join Workflow", 
                            item = item, 
                            description="A workflow to perform fuzzy join on two tables.",
                            para=para,
                            output=output)

    main_workflow.add_to_leaf(SelectMainVocabularyTable(output=output))
    main_workflow.add_to_leaf(DecideFuzzyColumns(output=output))
    main_workflow.add_to_leaf(PrepareFuzzyJoin(output=output))
    main_workflow.add_to_leaf(MatchEntity(output=output))
    
    return query_widget, main_workflow


class SelectTable(Node):
    default_name = 'Select Table'
    default_description = 'This step allows you to select the table.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        con = self.input_item["con"]
        query_widget = self.item["query_widget"]

        def get_databases():
            return list_databases(con)
        
        def get_schemas(database):
            return list_schemas(con, database)
        
        def get_tables(database, schema, include_views=False):
            tables = list_tables(con, schema, database)
            if include_views:
                views = list_views(con, schema, database)
                return tables + views
            return tables

        all_databases = get_databases()
        all_schemas = None

        default_database = self.para.get("default_database")
        default_schema = self.para.get("default_schema")

        if default_database not in all_databases:
            default_database = None
            default_schema = None
        else:
            all_schemas = get_schemas(default_database)
            if default_schema not in all_schemas:
                default_schema = None
        
        if not default_database or not default_schema:
            default_database, default_schema = get_default_database_and_schema(con)

        database_label = widgets.HTML(value="<b>Database</b>:")
        database_dropdown = widgets.Dropdown(
            options=all_databases,
            value=default_database,
            layout=widgets.Layout(width='150px')
        )
        
        if not all_schemas:
            all_schemas = get_schemas(default_database)

        schema_label = widgets.HTML(value="<b>Schema</b>:")
        schema_dropdown = widgets.Dropdown(
            options=all_schemas,
            value=default_schema,
            layout=widgets.Layout(width='150px')
        )

        include_views_toggle = widgets.Checkbox(
            value=True,
            description='',
            indent=False,
            layout=widgets.Layout(width='auto', margin='3px 0 0 0')
        )
        
        table_label = widgets.HTML(value="<b>Table</b>:")
        table_dropdown = widgets.Dropdown(
            options=get_tables(default_database, default_schema, include_views_toggle.value),
            layout=widgets.Layout(width='150px')
        )


        
        view_warning = widgets.HTML(
            value='''
            <div style="line-height: 1.2; margin-left: 5px;">
                <div><b>Include Views</b></div>
                <div style="color: orange; font-style: italic; font-size: 0.9em;">
                    ⚠️ Views can be slow to query. We recommend materializing them to tables.
                </div>
            </div>
            '''
        )

        checkbox_with_label = widgets.HBox(
            [include_views_toggle, view_warning], 
            layout=widgets.Layout(align_items='center')
        )

        def on_database_change(change):
            schemas = get_schemas(change['new'])
            schema_dropdown.options = schemas
            schema_dropdown.value = schemas[0] if schemas else None
            update_tables()
        
        def on_schema_change(change):
            update_tables()

        def on_views_toggle(change):
            update_tables()

        def update_tables():
            tables = get_tables(database_dropdown.value, schema_dropdown.value, include_views_toggle.value)
            table_dropdown.options = tables
            table_dropdown.value = tables[0] if tables else None

        database_dropdown.observe(on_database_change, names='value')
        schema_dropdown.observe(on_schema_change, names='value')
        include_views_toggle.observe(on_views_toggle, names='value')

        display(HTML(f"🤓 Please select the table:"))
        display(widgets.HBox([database_label, database_dropdown, schema_label, schema_dropdown, table_label, table_dropdown]))
        display(checkbox_with_label)

        mode_selector = widgets.RadioButtons(
            options=[
                ('💬 Interactive: Interactively provide your feedback for each step', 'Table Provider'),        
                ('⚡ Express: Sit back, relax, and let us provide our best guess', 'Table Viewer'),
            ],
            layout={'width': 'max-content'},
            style={'description_width': 'initial'}
        )
        
        next_button = widgets.Button(description="Next", button_style='success')

        def on_button_click(b):
            with self.output_context():
                table_name = table_dropdown.value
                
                sql_step = SQLStep(table_name=table_name, con=con, database=database_dropdown.value, schema=schema_dropdown.value, materialized=True)
                pipeline = TransformationSQLPipeline(steps=[sql_step], edges=[])
                self.para["table_pipeline"] = pipeline
                
                selected_mode = mode_selector.value
                if selected_mode == 'Table Viewer':
                    self.para["viewer"] = True
                
                callback({"table_name": table_name})
                
        next_button.on_click(on_button_click)
        
        if "table_name" in self.para and self.para["table_name"] is not None:
            table_dropdown.value = self.para["table_name"]
            on_button_click(next_button)
            return
        
        display(HTML(f"🤓 Please select the mode:"))
        display(mode_selector)
        display(next_button)
        
def create_cocoon_documentation_workflow(con, query_widget=None, viewer=False, table_name = None, para=None, output=None):
    if para is None:
        para = {}
            
    if query_widget is None:
        query_widget = QueryWidget(con)
    
    
    item = {
        "con": con,
        "query_widget": query_widget
    }
    
    workflow_para = {"viewer": viewer, 
                     "table_name": table_name,
                     "table_object": Table(),
                     "cocoon_stage_options": {'projection': False,
                            'rename': False, 
                            'deduplication': False, 
                            'standardize_values': True, 
                            'detect_disguised_missing_values': False, 
                            'detect_regex_patterns': True, 
                            'transform_data_type': False, 
                            'parse_variant_types': False, 
                            'unique_test': True, 
                            'category_test': True, 
                            'generate_html_report': True, 
                            'cardinality_threshold': 0}}
    
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("Data Profiling Workflow", 
                            item = item, 
                            description="A workflow to profile dataset",
                            para = workflow_para,
                            output=output)
    
    main_workflow.add_to_leaf(SelectStageOptions(output = output))
    main_workflow.add_to_leaf(SelectTable(output=output))
    main_workflow.add_to_leaf(CreateShortTableSummary(output=output))
    main_workflow.add_to_leaf(DescribeColumnsList(output=output))
    main_workflow.add_to_leaf(DecideStringUnusualForAll(output=output))
    main_workflow.add_to_leaf(DecideRegexForAll(output=output))
    main_workflow.add_to_leaf(DecideMissingList(output=output))
    main_workflow.add_to_leaf(DecideUnique(output=output))
    main_workflow.add_to_leaf(DecideStringCategoricalForAll(output=output))
    main_workflow.add_to_leaf(WriteStageYMLCode(class_para={"include_html": True}, output=output))
    return query_widget, main_workflow


def create_cocoon_stage_workflow(con, query_widget=None, viewer=False, table_name = None, para=None, output=None, table_object=None):
    if para is None:
        para = {}
        
    if query_widget is None:
        query_widget = QueryWidget(con)
    
    if table_object is None:
        table_object = Table()
        
    item = {
        "con": con,
        "query_widget": query_widget
    }
    
    workflow_para = {"viewer": viewer, 
                     "table_name": table_name,
                     "table_object": table_object}
    
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("Data Stage Workflow", 
                            item = item, 
                            description="A workflow to stage table",
                            para = workflow_para,
                            output=output)
    
    main_workflow.add_to_leaf(SelectTable(output=output))
    main_workflow.add_to_leaf(DetectPIIColumns(output=output))
    main_workflow.add_to_leaf(DecideProjection(output=output))
    main_workflow.add_to_leaf(CreateShortTableSummary(output=output))
    main_workflow.add_to_leaf(DescribeColumnsList(output=output))
    main_workflow.add_to_leaf(DecideDuplicate(output=output))
    main_workflow.add_to_leaf(DecideTrim(output=output))
    main_workflow.add_to_leaf(DecideStringUnusualForAll(output=output))
    main_workflow.add_to_leaf(CleanUnusualForAll(output=output))
    main_workflow.add_to_leaf(DecideDMVforAll(output=output))
    main_workflow.add_to_leaf(DecideDataTypeList(output=output))
    
    main_workflow.add_to_leaf(DecideRegexForAll(output=output))
    main_workflow.add_to_leaf(TransformTypeForAll(output=output))
    main_workflow.add_to_leaf(DecideVariantTypes(output=output))
    
    main_workflow.add_to_leaf(DecideMissingList(output=output))
    main_workflow.add_to_leaf(HandleMissing(output=output))
    main_workflow.add_to_leaf(DecideUnique(output=output))
    main_workflow.add_to_leaf(DecideStringCategoricalForAll(output=output))
    main_workflow.add_to_leaf(WriteAcrossColumnTests(output=output))
    main_workflow.add_to_leaf(WriteStageYMLCode(output=output))
    return query_widget, main_workflow







    


def create_cocoon_data_vault_workflow(con, query_widget=None, viewer=False, dbt_directory="./dbt_directory", para=None, output=None):
    if para is None:
        para = {}
        
    if query_widget is None:
        query_widget = QueryWidget(con)

    item = {
        "con": con,
        "query_widget": query_widget
    }
    

    
    workflow_para = {"data_project": DataProject(),
                     "dbt_directory": dbt_directory}
    
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("Data Vault Workflow", 
                            item = item, 
                            description="A workflow to build data vault",
                            para = workflow_para,
                            output = output)
    
    main_workflow.add_to_leaf(SelectSchema(output = output))
    main_workflow.add_to_leaf(DBTProjectConfig(output = output))
    main_workflow.add_to_leaf(SelectTables(output = output))
    main_workflow.add_to_leaf(SelectStageOptions(output = output))
    
    main_workflow.add_to_leaf(DecidePartition(output = output))
    main_workflow.add_to_leaf(DescribePartitionForAll(output = output))
    
    main_workflow.add_to_leaf(StageForAll(output = output))
    main_workflow.add_to_leaf(DecideSCDForAll(output = output))
    main_workflow.add_to_leaf(PerformSCDForAll(output = output))
    main_workflow.add_to_leaf(WriteSCDForAll(output = output))
    
    main_workflow.add_to_leaf(DecideKeysForAll(output = output))
    main_workflow.add_to_leaf(RefinePK(output = output))
    
    main_workflow.add_to_leaf(ConnectPKFKTable(output = output))
    main_workflow.add_to_leaf(DetectReferentialIntegrity(output = output))
    main_workflow.add_to_leaf(DisplayJoinGraph(output = output))

    main_workflow.add_to_leaf(GroupSimilarTables(output = output))

    main_workflow.add_to_leaf(EntityUnderstandingList(output = output))
    main_workflow.add_to_leaf(RelationUnderstandingForAll(output = output))
    main_workflow.add_to_leaf(RefineRelations(output = output))
    
    main_workflow.add_to_leaf(ExtractTableGroups(output = output))
    main_workflow.add_to_leaf(BuildTableHierarchy(output = output))
    main_workflow.add_to_leaf(ReorderRelationToStoryForAll(output = output))
    
    main_workflow.add_to_leaf(BuildERStory(output = output))
    main_workflow.add_to_leaf(DocumentProject(output = output))

    return query_widget, main_workflow


class DescribeColumnsList(ListNode):
    default_name = 'Describe Columns'
    default_description = 'This node allows users to describe the columns of a table.'

    def extract(self, item):
        clear_output(wait=True)

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        display(HTML(f"{running_spinner_html} Checking columns for <i>{table_name}</i>..."))
        
        create_progress_bar_with_numbers(0, doc_steps)
        self.progress = show_progress(1)

        self.input_item = item

        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        
        table_summary = self.get_sibling_document("Create Short Table Summary")
        
        access_control = self.para['cocoon_stage_options'].get('description_access_control', 'Schema Only Access')
        
        outputs = []
        
        for i in range(0, len(columns), 30):
            chunk_columns = columns[i:i + 30]
            if access_control == 'Read Data Access':
                sample_size = 5
                sample_df = table_pipeline.get_samples(con, columns=chunk_columns, sample_size=sample_size)
                sample_df = sample_df.applymap(truncate_cell)
                table_desc = sample_df.to_csv(index=False, quoting=1)
            else:
                table_desc = ", ".join(chunk_columns)
            outputs.append((table_desc, table_summary, chunk_columns, i, access_control))
        
        return outputs
    
    def run(self, extract_output, use_cache=True):
        table_desc, table_summary, column_names, start_index, access_control = extract_output 

        table_object = self.para.get("table_object")
        if table_object and hasattr(table_object, 'column_desc'):
            existing_columns = list(table_object.column_desc.keys())
            if len(existing_columns) >= start_index + len(column_names):
                result = {old_col: [table_object.column_desc[new_col], new_col] 
                        for old_col, new_col in zip(column_names, existing_columns[start_index:start_index+len(column_names)])}
                return result

        if access_control == 'Read Data Access':
            template = f"""You have the following table:
{table_desc}
{table_summary}

Tasks: 
(1) Describe the columns in the table based on the sample data provided.
(2) If the original column name is not descriptive, provide a new name. Otherwise, keep the original name.
Make sure there are no duplicated new column names

Return in the following format:
```json
{{
    "{column_names[0]}": ["Short description in < 10 words", "new_column_name"],
    ...
}}
```"""
        else:
            template = f"""You have a table with the following columns:
{table_desc}
{table_summary}

Tasks: 
(1) Based on the column names, provide a brief description of what each column might represent.
(2) If the original column name is not descriptive, suggest a new name. Otherwise, keep the original name.
Make sure there are no duplicated new column names

Return in the following format:
```json
{{
    "{column_names[0]}": ["Short description in < 10 words", "new_column_name"],
    ...
}}
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
    
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: all(key in column_names for key in jc.keys()), "The dictionary contains keys that are not in the column_names list."),
            (lambda jc: all(isinstance(value, list) and len(value) == 2 for value in jc.values()), "Not all values in the dictionary are lists with exactly 2 elements."),
            (lambda jc: all(isinstance(value[0], str) for value in jc.values()), "Not all column descriptions are strings."),
            (lambda jc: all(isinstance(value[1], str) for value in jc.values()), "Not all new column names are strings."),
            (lambda jc: len(set(value[1] for value in jc.values())) == len(jc), "New column names are not unique."),
        ]

        for check, error_message in checks:
            if not check(json_code):
                raise ValueError(f"Validation failed: {error_message}")
        
        return json_code

    def run_but_fail(self, extract_output, use_cache=True):
        default_response = {column: [column, column] for column in extract_output[2]}
        return default_response
    
    def merge_run_output(self, run_outputs):
        merged_output = {}
        for run_output in run_outputs:
            merged_output.update(run_output)
        return merged_output
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        self.progress.value += 1

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)

        rows_list = []

        for col in schema:
            rows_list.append({
                "Column": col,
                "Summary": json_code[col][0],
                "New Column Name": json_code[col][1],
                "Renamed?": "✔️ Yes" if json_code[col][1] != col else "❌ No"
            })

        df = pd.DataFrame(rows_list)
        
        editable_columns = [False, True, True, False]
        grid = create_dataframe_grid(df, editable_columns, reset=True)

        next_button = widgets.Button(
            description='Accept Rename',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  
        
        reject_button = widgets.Button(
            description='Reject Rename',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
        
        def on_button_clicked2(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                new_df["New Column Name"] = new_df["Column"]
                document = new_df.to_json(orient="split")
                get_column_desc_from_df(new_df)
                callback(document)
            

        def get_column_desc_from_df(df):
            table_object = self.para["table_object"]
            column_desc = {}
            for index, row in df.iterrows():
                table_object.column_desc[row["New Column Name"]] = row["Summary"]

        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                
                new_df["New Column Name"] = new_df["New Column Name"].apply(clean_column_name)
                
                
                get_column_desc_from_df(new_df)
                
                renamed = (new_df["Column"] != new_df["New Column Name"]).any()
                
                if renamed:
                    duplicates = new_df["New Column Name"].duplicated(keep=False)
                    
                    if duplicates.any():
                        print(f"⚠️ Please provide unique names for the columns. The following columns have duplicated names:")
                        duplicate_names = new_df[duplicates]["New Column Name"].unique()
                        print(", ".join(duplicate_names))
                        
                        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
                            new_df.loc[duplicates, "New Column Name"] = new_df.loc[duplicates, "Column"]
                            print("Duplicated names have been reset to their original values.")
                        else:
                            return

                    old_table_name = table_pipeline.__repr__(full=False)
                    new_table_name = old_table_name + "_renamed"
                    comment = f"-- Rename: Renaming columns\n"
                    selection_clauses = []
                    
                    for index, row in new_df.iterrows():
                        old_name = row["Column"]
                        new_name = row["New Column Name"]
                        
                        if old_name != new_name:
                            comment += f"-- {old_name} -> {new_name}\n"
                            selection_clauses.append(f'{enclose_table_name(old_name, con=con)} AS {enclose_table_name(new_name, con=con)}')
                        else:
                            selection_clauses.append(f'{enclose_table_name(old_name, con=con)}')
                    
                    selection_clause = ',\n'.join(selection_clauses)
                    sql_query = f'SELECT \n{indent_paragraph(selection_clause)}'
                    sql_query = comment + sql_query
                    sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"
                    
                    step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con)
                    table_pipeline.add_step_to_final(step)
                
                document = new_df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            if (hasattr(self, 'para') and 
                isinstance(self.para, dict) and 
                isinstance(self.para.get('cocoon_stage_options'), dict) and 
                self.para['cocoon_stage_options'].get('rename') is False):
                on_button_clicked2(reject_button)
                return
            on_button_clicked(next_button)
            return
        
        display(HTML(f"😎 We have described the columns:"))
        display(grid)
        display(HBox([reject_button, next_button]))

            
class DecideMissingList(ListNode):
    default_name = 'Decide Missing Values'
    default_description = 'This node allows users to decide how to handle missing values.'

    def extract(self, item):
        clear_output(wait=True)
        
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)

        display(HTML(f"{running_spinner_html} Checking missing values for <i>{table_name}</i>..."))
        create_progress_bar_with_numbers(2, doc_steps)
        self.progress = show_progress(1)

        explain_missing_values = self.para['cocoon_stage_options'].get('explain_missing_values', False)
        
        if not explain_missing_values:
            return []

        schema = table_pipeline.get_schema(con)
        columns = []
        database_name = get_database_name(con)
        for col, col_type in schema.items():
            if database_name == "SQL Server":
                if col_type.lower() not in ['text', 'ntext', 'image']:
                    columns.append(col)
            else:
                columns.append(col)

        columns = sorted(columns)
        sample_size = 5
        
        with_context = table_pipeline.get_codes(mode="WITH")
        outputs = []
                
        for i in range(0, len(columns), 50):
            chunk_columns = columns[i:i + 50]
            missing_columns = {}
            
            for col in chunk_columns:
                missing_percentage = get_missing_percentage(con, table_pipeline, col, with_context=with_context)
                if missing_percentage > 0:
                    missing_columns[col] = missing_percentage
                    
            sample_df = table_pipeline.get_samples(con, columns=chunk_columns, sample_size=sample_size)
            sample_df = sample_df.applymap(truncate_cell)
            sample_df_str = sample_df.to_csv(index=False, quoting=1)
            
            outputs.append((missing_columns, sample_df_str))

        return outputs

    def run(self, extract_output, use_cache=True):
        
        missing_columns, sample_df_str = extract_output
        
        if len(missing_columns) == 0:
            return {"reasoning": "No missing values found.", "columns_obvious_not_applicable": {}}

        missing_desc = "\n".join([f'{idx+1}. {col}: {missing_columns[col]}' for idx, (col, desc) in enumerate(missing_columns.items())])

        template = f"""You have the following table:
{sample_df_str}

The following columns have percentage of missing values:
{missing_desc}

For each column, decide whether there is an obvious reason for the missing values to be "not applicable"
E.g., if the column is 'Date of Death', it is "not applicable" for patients still alive.
Reasons like not not collected, sensitive info, encryted, data corruption, etc. are not considered 'not applicable'.

Return in the following format:
```json
{{
    "reasoning": "X column has an obvious not applicable reason... The rest don't.",
    "columns_obvious_not_applicable": {{
        "column_name": "Short specific reason why not applicable, in < 10 words.",
        ...
    }}
}}
"""
            
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)

        if not isinstance(json_code, dict) or "reasoning" not in json_code or "columns_obvious_not_applicable" not in json_code:
            raise ValueError("The returned JSON code does not adhere to the required format.")
        
        for col_name in json_code["columns_obvious_not_applicable"]:
            if col_name not in missing_columns:
                raise ValueError(f"The column '{col_name}' specified in 'columns_obvious_not_applicable' is not present in the missing columns.")
            
        return json_code
    
    def merge_run_output(self, run_outputs):
        merged_output = {}
        for run_output in run_outputs:
            merged_output.update(run_output["columns_obvious_not_applicable"])
        return {"columns_obvious_not_applicable": merged_output}
    
    def run_but_fail(self, extract_output, use_cache=True):
        missing_columns, sample_df_str = extract_output
        
        if len(missing_columns) == 0:
            return {"reasoning": "No missing values found. ", "columns_obvious_not_applicable": {}}
        else:
            return {
                "reasoning": "Failed to analyze the data due to an error.",
                "columns_obvious_not_applicable": {}
            }

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        self.progress.value += 1
        json_code = run_output
        
        outputs = extract_output
        missing_columns = {}
        
        for output in outputs:
            missing_columns.update(output[0])
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)

        rows_list = []
        for col in missing_columns:
            missing_percentage = missing_columns[col]
            reason = json_code["columns_obvious_not_applicable"].get(col, "")
            rows_list.append({
                "Column": col,
                "NULL (%)": f"{missing_percentage*100:.2f}",
                "Is NULL Acceptable?": True if reason != "" else False,
                "Explanation": reason
            })
            
        df = pd.DataFrame(rows_list)
        
        if len(rows_list) == 0:
            df = pd.DataFrame(columns=["Column", "NULL (%)", "Is NULL Acceptable?", "Explanation"])
        else:
            df = pd.DataFrame(rows_list)

        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, False, True, True]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                document = new_df.to_json(orient="split")

                table_object = self.para["table_object"]
                for index, row in new_df.iterrows():
                    col = row["Column"]
                    missing_reason = row["Is NULL Acceptable?"]
                    explanation = row["Explanation"]
                    if missing_reason:
                        table_object.missing_reason[col] = explanation
                    else:
                        table_object.missing_reason[col] = "Unknown"
                
                callback(document)
        
        next_button.on_click(on_button_clicked)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
 
        display(HTML(f"😎 We have identified missing values, and potential causes:"))
        display(grid)
        display(next_button)            
            
class DecideDataTypeList(ListNode):
    default_name = 'Decide Data Type'
    default_description = 'This node allows users to decide the data type for each column.'

    def extract(self, item):
        clear_output(wait=True)
        
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        display(HTML(f"{running_spinner_html} Checking data types for <i>{table_name}</i>..."))
        create_progress_bar_with_numbers(1, doc_steps)
        self.progress = show_progress(1)

        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        columns = sorted(columns)
        sample_size = 5
        
        database_name = get_database_name(con)
        all_data_types = list(data_types_database[database_name].keys())
        
        outputs = []
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('transform_data_type') is False):
            return outputs
                
        for i in range(0, len(columns), 30):
            chunk_columns = columns[i:i + 30]
            sample_df = table_pipeline.get_samples(con, columns=chunk_columns, sample_size=sample_size)
            sample_df = sample_df.applymap(truncate_cell)
            sample_desc = sample_df.to_csv(index=False, quoting=1)
            
            table_object = self.para["table_object"]
            table_desc = table_object.print_column_desc(columns=chunk_columns)
            
            chunk_schema = {col: schema[col] for col in chunk_columns}
            outputs.append((sample_desc, table_desc, all_data_types, database_name, chunk_schema))

        return outputs

    def run(self, extract_output, use_cache=True):
        sample_desc, table_desc, all_data_types, database_name, schema = extract_output

        template = f"""You have the following table:
{sample_desc}
{table_desc}

For each column, classify what the column type should be.
The column type should be one of the following:
{all_data_types}

Return in the following format:
```json
{{
    "column1": "INT",
    ...
}}
```"""
        
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        
        if not isinstance(json_code, dict):
            raise ValueError("Validation failed: The returned JSON code is not a dictionary.")

        invalid_types = set(json_code.values()) - set(all_data_types)
        if invalid_types:
            raise ValueError(f"Validation failed: Invalid column type(s) found. "
                            f"Expected types: {all_data_types}. "
                            f"Found types: {set(json_code.values())}. "
                            f"Invalid types: {invalid_types}")

        missing_columns = set(json_code.keys()) - set(schema)
        if missing_columns:
            raise ValueError(f"Validation failed: One or more column names in 'column_type' "
                            f"are not present in the sample DataFrame. "
                            f"Missing columns: {missing_columns}")
            
        return json_code
    
    def merge_run_output(self, run_outputs):
        merged_output = {}
        for run_output in run_outputs:
            merged_output.update(run_output)
        return merged_output
    
    def run_but_fail(self, extract_output, use_cache=True):
        _,_, _, database_name, schema = extract_output
        return {col: get_reverse_type(schema[col], database_name) for col in schema}

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        
        if len(extract_output) == 0:
            with self.output_context():
                clear_output(wait=True)
                df = pd.DataFrame(columns=["Column", "Current Type", "Target Type", "Matched?"])
                callback(df.to_json(orient="split"))
                return
        
        _, _, all_data_types, database_name, _ = extract_output[0]
        
        schema = {}
        outputs = extract_output
        for output in outputs:
            schema.update(output[4])
        
        self.progress.value += 1
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        
        columns = []
        current_types = []
        target_types = []

        for col in schema:
            columns.append(col)
            current_types.append(get_reverse_type(schema[col], database_name))
            target_types.append(json_code[col])

        data = {
            'column_name': columns,
            'current_type': current_types,
            'target_type': target_types
        }

        df = pd.DataFrame(data)
        
        grid = create_data_type_grid(df, all_data_types = all_data_types)

        next_button = widgets.Button(
            description='Accept Cast',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  
        
        reject_button = widgets.Button(
            description='Reject Cast',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
        
        table_object = self.para["table_object"]
        for _, row in df.iterrows():
            column = row['column_name']
            current_type = row['current_type']
            table_object.data_type[column] = {"current_data_type": current_type}
        
        def on_button_clicked2(b):
            with self.output_context():
                clear_output(wait=True)
                df = extract_grid_data_type(grid)
                df["Target Type"] = df["Current Type"]
                callback(df.to_json(orient="split"))

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                df = extract_grid_data_type(grid)
                
                for _, row in df.iterrows():
                    column = row['Column']
                    current_type = row['Current Type']
                    target_type = row['Target Type']
                    if current_type != target_type:
                        table_object.data_type[column] = {"current_data_type": current_type, "expected_data_type": target_type}
                        
                callback(df.to_json(orient="split"))

        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
        display(HTML(f"😎 We have recommended the Data Types for Columns:"))
        display(grid)
        display(HBox([reject_button, next_button]))
        






        

def create_cocoon_profile_workflow(con, query_widget=None, viewer=False, output=None):

    if query_widget is None:
        query_widget = QueryWidget(con)

        
    item = {
        "con": con,
        "query_widget": query_widget,
        "table_object": Table(),
        "viewer": viewer
    }

    main_workflow = Workflow("Data Profiling Workflow", 
                            item = item, 
                            description="A workflow to profile dataset",
                            viewer=viewer,
                            para = {},
                            output=output)

    main_workflow.add_to_leaf(SelectTable(output=output))
    main_workflow.add_to_leaf(DecideProjection(output=output))
    main_workflow.add_to_leaf(DecideDuplicate(output=output))
    main_workflow.add_to_leaf(CreateTableSummary(output=output))
    main_workflow.add_to_leaf(DescribeColumns(output=output))
    main_workflow.add_to_leaf(CreateColumnGrouping(output=output))
    main_workflow.add_to_leaf(DecideDataType(output=output))
    main_workflow.add_to_leaf(DecideMissing(output=output))
    main_workflow.add_to_leaf(DecideUnique(output=output))
    main_workflow.add_to_leaf(DecideUnusualForAll(output=output))
    main_workflow.add_to_leaf(DecideColumnRange(output=output))
    main_workflow.add_to_leaf(DecideLongitudeLatitude(output=output))
    main_workflow.add_to_leaf(GenerateProfileReport(output=output))
    
    return query_widget, main_workflow

        

    


class CocoonBranchStep(Node):
    default_name = 'Product Steps'
    default_description = 'This is typically the dicussion result from business analysts and engineers'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        header_html = f'''
<div style="display: flex; align-items: center;">
<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">
<div style="margin: 0; padding: 0;">
<h1 style="margin: 0; padding: 0;">Cocoon</h1>
<p style="margin: 0; padding: 0;">Organize Data Warehouses with LLMs</p>
</div>
</div><hr>
🤓 Please select the data task:
'''

        display(HTML(header_html))
        
        html_labels = [
        "🤖 <b>Chatbot:</b> Give us catalogs and dbt projects, we'll create a cursor-style chatbot.<br><i>🛡️ Access Control (just codes): <u>read-only</u> to dbt project </u></i>",
        "✨ <b>Clean:</b> Give us a table, we'll clean and document it.<br> <i>🛡️ Access Control (database): read, and <u>write only under specified schema </u></i>",
        "🧩 <b>Table Catalog:</b> Give us tables, we'll clean, integrate and catalog them for future RAG.<br> <i>🛡️ Access Control (database): read, and <u>write only under specified schema </u></i>",
        "🔧 <b>Transform:</b> Give us the catalogs of source + target database, we transform.<br> <i>🛡️ Access Control (database): read, and <u>write only under specified schema </u></i>",
        "🔗 <b>(Preview) Standardization:</b> Give us a vocabulary, we will standardize tables. <br>",
        ]
        coming_labels = [
        "🔄 <b>(Coming Soon) Integrate:</b> Give us tables, we'll integrate them.",
        "🛠️ <b>(Coming Soon) Pipeline Maintenance:</b> Give us a broken Pipeline, we'll repair it.",
        "📈 <b>(Coming Soon) Semantic:</b> Give us a database, we'll build metrics.",
        ]
        
        next_nodes = [
            "Multi-Catalog Explore Workflow",
            "Data Format Workflow",
            "Data Vault Workflow",
            "Single Table Transformation Workflow",
            "Fuzzy Join Workflow",
        ]

        radio_buttons_widget, checkboxes = create_html_radio_buttons(html_labels)
        
        radio_buttons_widget2, _ = create_html_radio_buttons(coming_labels, disabled=True)

        display(radio_buttons_widget)

        next_button = widgets.Button(description="Start", button_style='success', icon="check")
        
        def on_button_click(b):
            with self.output_context():
                selected_index = get_selected_index(checkboxes)
                
                if selected_index is None:
                    print("⚠️ Please select one option")
                    return
                
                callback({"next_node": next_nodes[selected_index]})

        next_button.on_click(on_button_click)
        
        hint_html = widgets.HTML(
            value='<p style="margin-top: 10px; font-style: italic; color: #666;">'
                    'Not Responding? Some Jupyter Notebooks need an output widget. <br>'
                    'Try <code>query_widget, cocoon_workflow = create_cocoon_workflow(con=con, output=widgets.Output())</code>'
                    '</p>'
        )
        
        display(VBox([next_button, hint_html]))
        
def create_cocoon_workflow(con= None, para = None, output=None):
    
    if para is None:
        para = {}
    
    item = {}
    query_widget = None
    
    
    if con is not None:
        query_widget = QueryWidget(con)
        item["con"] = con
        item["query_widget"] = query_widget

    main_workflow = Workflow("Cocoon Workflow", 
                            item = item, 
                            description="This is the main workflow for the Cocoon tool.",
                            output=output)
    
    branch_node = CocoonBranchStep()
    main_workflow.add_to_leaf(branch_node)
    

    _, stage_workflow = create_cocoon_data_format_workflow(con=con, query_widget=query_widget, para=para, output=output)
    _, profile_workflow = create_cocoon_documentation_workflow(con=con, query_widget=query_widget, output=output)
    _, fuzzy_join_workflow = create_matching_workflow(con=con, query_widget=query_widget, output=output)    
    catalog_explore_workflow = create_cocoon_mul_catalog_explore_workflow(para=para, output=output)
    _, table_transform_workflow = create_cocoon_table_transform_workflow(con=con, query_widget=query_widget, para=para, output=output)
    _, data_vault_workflow = create_cocoon_data_vault_workflow(con=con, query_widget=query_widget, para=para, output=output)
    
    main_workflow.register(stage_workflow, parent=branch_node)
    main_workflow.register(profile_workflow, parent=branch_node)
    main_workflow.register(fuzzy_join_workflow, parent=branch_node)
    main_workflow.register(catalog_explore_workflow, parent=branch_node)
    main_workflow.register(table_transform_workflow, parent=branch_node)
    main_workflow.register(data_vault_workflow, parent=branch_node)
    
    return query_widget, main_workflow

def create_cocoon_data_format_workflow(con, query_widget=None, viewer=False, dbt_directory="./dbt_directory", para=None, output=None):
    if para is None:
        para = {}
        
    if query_widget is None:
        query_widget = QueryWidget(con)

    item = {
        "con": con,
        "query_widget": query_widget
    }
    

    
    workflow_para = {"data_project": DataProject(),
                     "dbt_directory": dbt_directory}
    
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("Data Format Workflow", 
                            item = item, 
                            description="A workflow to format",
                            para = workflow_para,
                            output = output)
    
    main_workflow.add_to_leaf(SelectSchema(output = output))
    main_workflow.add_to_leaf(DBTProjectConfig(output = output))
    main_workflow.add_to_leaf(SelectTables(output = output))
    main_workflow.add_to_leaf(SelectStageOptions(output = output))
    main_workflow.add_to_leaf(StageForAllAndEnd(output = output))

    return query_widget, main_workflow


def get_columns_pattern(columns, use_cache=True):
    template = f"""You have the following columns:
{columns}

Are there column groups that:
1. Column names follow the same pattern, but have varying *numbers*, that can be expressed by regular expression pattern with \d. 
2. Is large with > 10 columns
E.g., columns "year_1", "year_2", ... follow the pattern: ^year_\d+$

Return the result in yml
```yml
reasoning: >
    The columns follow ... They have varying numbers...

patterns:
    - ^year_\d+$
    - ...
```"""

    messages = [{"role": "user", "content": template}]
    response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
    messages.append(response['choices'][0]['message'])
    yml_code = extract_yml_code(response['choices'][0]['message']["content"])
    summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
    return summary


class DecideColumnsPattern(Node):
    default_name = 'Decide Columns Pattern'
    default_description = 'This node allows users to decide the columns pattern.'

    def extract(self, item):
        clear_output(wait=True)
        print("🔍 Analyzing columns ...")
        create_progress_bar_with_numbers(2, doc_steps)
        
        self.progress = show_progress(1)

        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        columns = sorted(columns)
        
        return columns
    
    def run(self, extract_output, use_cache=True):
        columns = extract_output
        
        if len(columns) <= 50:
            return {}
        
        print(f"😲 Your table has {len(columns)} columns!")
        print(f"🧐 Don't worry! We will find if there are some patterns!")
        
        columns = sorted(columns)

        start = 0
        column_group = {}

        def get_columns(pattern, columns):
            return [col for col in columns if re.match(pattern, col)]

        use_cache = True

        while start < len(columns):
            
            
            summary = get_columns_pattern(columns[start:start+50], use_cache) 
            use_cache = True
            
            if len(summary["patterns"]) > 0:
                for pattern in summary["patterns"]:
                    fitted_columns = get_columns(pattern, columns)
                    if len(fitted_columns) > 0:
                        column_group[pattern] = fitted_columns
                        columns = [col for col in columns if col not in fitted_columns]
                        use_cache = True
                    else:
                        use_cache = False 
                    
            else:
                start += 50
                
        return column_group
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        self.progress.value += 1
        
        column_group = run_output
        columns = extract_output
        
        if not column_group:
            callback(column_group)
            return
        
        df_group = pd.DataFrame(column_group.items(), columns=["Pattern", "Columns"])
        df_group["Remove?"] = True
        editable_columns = [False, True, True]
        grid = create_dataframe_grid(df_group, editable_columns, reset=True, lists=['Columns'])

        print("😎 We have identified column groups with the same pattern:")
        print("Cocoon currently struggles with wide tables. We recommend removing them.")
        display(grid)
        
        def on_button_clicked(b):
            with self.output_context():
                new_df = grid_to_updated_dataframe(grid, lists=["Columns"])
                table_pipeline = self.para["table_pipeline"]
                con = self.item["con"]

                columns_to_remove = []
                for index, row in new_df.iterrows():
                    if row["Remove?"]:
                        columns_to_remove.extend(row["Columns"])

                if len(columns_to_remove) > 0:
                    old_table_name = table_pipeline.__repr__(full=False)
                    new_table_name = old_table_name + "_removeWideColumns"
                    selected_columns = [{enclose_table_name(col, con=con)} for col in columns if col not in columns_to_remove]

                    selection_clause = ',\n'.join(selected_columns)
                    sql_query = f'SELECT \n{indent_paragraph(selection_clause)}'

                    comment = "-- Remove wide columns with pattern. The regex and columns are:\n"
                    for index, row in new_df.iterrows():
                        if row["Remove?"]:
                            if len(row["Columns"]) > 10:
                                comment += f"-- {row['Pattern']}: {', '.join(row['Columns'][0:10])} ...\n"
                            else:
                                comment += f"-- {row['Pattern']}: {', '.join(row['Columns'])}\n"

                    sql_query = f"{comment}{sql_query}"
                    sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"
                    step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con)
                    step.run_codes()
                    table_pipeline.add_step_to_final(step)
                    
                document = new_df.to_json(orient="split")

                callback(document)
        
        next_button = widgets.Button(
            description='Accept Remove',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        reject_button = widgets.Button(
            description='Reject Remove',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
        
        def on_button_clicked2(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                new_df["Remove?"] = False
                document = new_df.to_json(orient="split")
                callback(document)
        
        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        display(HBox([reject_button, next_button]))
        
        if "viewer" in self.para and self.para["viewer"]:
            on_button_clicked(next_button)

class SpecifyDirectory(Node):
    default_name = 'Specify Directory'
    default_description = 'This step allows you to specify the directory.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        print(f"🧐 Please specify the directory:")
        
        directory_widget = widgets.Text(value='', placeholder='Please provide the directory', disabled=False)
        
        next_button = widgets.Button(description="Next", button_style='success')

        def on_button_click(b):
            with self.output_context():
                directory = directory_widget.value
                
                if not file_exists(directory):
                    print(f"❌ The directory {directory} does not exist.")
                    return
                
                callback({"directory": directory})
                
        next_button.on_click(on_button_click)
        display(directory_widget, next_button)
        
class ProcessDirectory(Node):
    default_name = 'Process Directory'
    default_description = 'This step processes the directory.'

    def extract(self, item):
        directory = self.get_sibling_document('Specify Directory')["directory"]
        
        print(f"🚀 Processing the directory: {directory}")
        
        dbt_project = DbtProject()
        dbt_project.process_dbt_files(directory)
        
        self.item["dbt_project"] = dbt_project
        
        return dbt_project
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        dbt_project = extract_output
        
        print(f"🚀 The directory has been processed successfully. We've found {len(dbt_project.tables)} tables")
        
        tables = dbt_project.tables.keys()
        
        table_widget = widgets.Dropdown(options=tables, disabled=False)
        display(table_widget)
        
        print("🧐 Next, we will understand these tables!")
        
        next_button = widgets.Button(description="Next", button_style='success')
        
        def on_button_click(b):
            with self.output_context():
                callback({})
                
        next_button.on_click(on_button_click)
        display(next_button)

                
class TagTableForAll(MultipleNode):
    default_name = 'Tag Table For All'
    default_description = 'This node tags all the table.'
    
    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["table_name"] = element_name
        para["table_idx"] = idx
        para["total_tables"] = total
        node = TagTable(para=para, id_para="table_name")
        node.inherit(self)
        return node
    
    def extract(self, item):
        tables = list(self.item["dbt_project"].tables.keys())
        self.elements = tables
        total = len(tables)
        self.nodes =  {table_name: self.construct_node(table_name, idx, total) for idx, table_name in enumerate(tables)}
                                            

class TagTable(Node):
    default_name = 'Tag Table'
    default_description = 'This node tags the table.'

    def extract(self, item):
        table_name = self.para["table_name"]
        dbt_project = self.item["dbt_project"]
        
        clear_output(wait=True)
        print("🔍 Tagging table:", table_name)
        
        idx = self.para["table_idx"]
        total = self.para["total_tables"]
        show_progress(max_value=total, value=idx)
        
        sql_query = dbt_project.tables[table_name].sql_query

        return sql_query
    
    def run(self, extract_output, use_cache=True):
        
        sql_query = extract_output
        
        if sql_query is  None or sql_query == "":
            return {"Filtering": {"Reasoning": "No SQL query found", "Label": False},
                    "Cleaning": {"Reasoning": "No SQL query found", "Label": False},
                    "Featurization": {"Reasoning": "No SQL query found", "Label": False},
                    "Integration": {"Reasoning": "No SQL query found", "Label": False},
                    "Other": {"Reasoning": "No SQL query found", "Label": False}}
        
        template = f"""## SQL query
{sql_query}

## Label the dbt script.
1. Filtering: the script selects (or semi-join) the table based on some criteria. 
2. Cleaning: the script deduplicate the table, or clean the existing columns by, e.g., trim, standardizing, formating...
3. Featurization: the script extract new features from the existing columns. E.g., the script extracts whether weekend/weekday from date, or aggregate the table
4. Integration: the script join and union tables to integrate information. Note that semi-join is not considered as integration
5. Other: there is other significant tasks performed beyond the above.

## Return a json:
```json
{{
	"Filtering": {{
		"Reasoning": …
		"Label": true/false
    }},
    ...
}}
```
"""
        
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        
        def verify_json_result(json_code):
            expected_labels = ['Filtering', 'Cleaning', 'Featurization', 'Integration', 'Other']

            if not isinstance(json_code, dict):
                raise ValueError("JSON code should be a dictionary.")

            for label in expected_labels:
                if label not in json_code:
                    raise ValueError(f"Key '{label}' is missing.")

                if not isinstance(json_code[label], dict):
                    raise ValueError(f"'{label}' should be a dictionary.")

                if 'Reasoning' not in json_code[label] or 'Label' not in json_code[label]:
                    raise ValueError(f"'{label}' must contain 'Reasoning' and 'Label' keys.")

                if not isinstance(json_code[label]['Label'], bool):
                    raise ValueError(f"'Label' under '{label}' should be a boolean.")


            return True

        verify_json_result(json_code)
        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"Filtering": {"Reasoning": "Failed", "Label": False},
                    "Cleaning": {"Reasoning": "Failed", "Label": False},
                    "Featurization": {"Reasoning": "Failed", "Label": False},
                    "Integration": {"Reasoning": "Failed", "Label": False},
                    "Other": {"Reasoning": "Failed", "Label": False}}
        
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)
    

class InputRelationshipForAll(MultipleNode):
    default_name = 'Input Relationship For All'
    default_description = 'This node tags all the table.'        
        
    def construct_node(self, element_name, tags = None, idx=0, total=0):
        if tags is None:
            tags = {}
        para = self.para.copy()
        para["table_name"] = element_name
        para["table_idx"] = idx
        para["tags"] = tags
        para["total_tables"] = total
        node = InputRelationship(para=para, id_para="table_name")
        node.inherit(self)
        return node
    
    def extract(self, item):
        tables = list(self.item["dbt_project"].tables.keys())
        self.elements = tables
        total = len(tables)
        tag_map = self.get_sibling_document('Tag Table For All')['Tag Table']
        self.nodes =  {table_name: self.construct_node(table_name, tag_map[table_name], idx, total) for idx, table_name in enumerate(tables)}
                                     
        
class InputRelationship(Node):
    default_name = 'Input Relationship'
    default_description = 'This node understands the input relationship.'

    def extract(self, item):
        table_name = self.para["table_name"]
        dbt_project = self.item["dbt_project"]
        
        clear_output(wait=True)
        print("🔍 Understanding table relationship...")
        
        idx = self.para["table_idx"]
        total = self.para["total_tables"]
        show_progress(max_value=total, value=idx)
        
        sql_query = dbt_project.tables[table_name].sql_query
        input_tables = dbt_project.tables[table_name].referenced_models + dbt_project.tables[table_name].referenced_sources
        input_tables = sorted(input_tables) 
        tags = self.para["tags"]
        
        return sql_query, input_tables, tags
    
    def run(self, extract_output, use_cache=True):
        sql_query, input_tables, tags = extract_output
        
        if sql_query is  None or sql_query == "":
            return {}
        
        purpose_summary = ""
        for key, value in tags.items():
            if value["Label"] == True:
                purpose_summary += f"{key}: {value['Reasoning']}\n"

        output_format = ""
        for input_table in input_tables:
            output_format += f"    \"{input_table}\": \"...\",\n"
        output_format = output_format[:-2]
        
        template = f"""## SQL query
{sql_query}

## SQL query purpose
{purpose_summary}

## Given the SQL query, summarize how each input table is used:
E.g., table A is the main table, table B is to enrich table A, table C is to filter table A, etc.

## Return a json:
```json
{{
{output_format}
}}
```
"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        
        def verify_json_result(json_code):
            for input_table in input_tables:
                if input_table not in json_code:
                    raise ValueError(f"Table {input_table} is not in the json code.")
        
        verify_json_result(json_code)
        return json_code
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)

class BuildFinalLineage(Node):
    default_name = 'Build Final Lineage'
    default_description = 'This node builds the final lineage.'

    def extract(self, item):
        
        clear_output(wait=True)
        print("🚀 Your DBT project has been analyzed:")
        dbt_project = self.item["dbt_project"]
        
        tag_json = self.get_sibling_document('Tag Table For All')['Tag Table']
        for table_name in tag_json:
            dbt_project.tables[table_name].add_tag_result(tag_json[table_name])
            
        source_purpose_json = self.get_sibling_document('Input Relationship For All')['Input Relationship']
        for table_name in source_purpose_json:
            dbt_project.tables[table_name].add_source_purpose(source_purpose_json[table_name])
            
        dbt_project.update_tagged_names()

        dbt_project.display()
        return 
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        pass
    
    
    
class SelectSourceTargetTable(Node):
    default_name = 'Select Source and Target Table'
    default_description = 'This step allows you to select the source and target table for the transformation.'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        con = self.input_item["con"]
        query_widget = self.item["query_widget"]
        
        tables = get_table_names(con)
        
        header_html = f'<div style="display: flex; align-items: center;">' \
            f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
            f'<div style="margin: 0; padding: 0;">' \
            f'<h1 style="margin: 0; padding: 0;">Cocoon</h1>' \
            f'<p style="margin: 0; padding: 0;">Table Transformation</p>' \
            f'</div>' \
            f'</div><hr>'

        display(HTML(header_html))

        html_content = f"""🤓 Please select the <b>Source</b> and (example of) <b>Target</b> tables for transformation.<br>
✨ We recommend cleaning the source table before transforming it.<br>"""
        display(HTML(html_content))
        
        display(HTML(f"<b>Source Table:</b>"))
        
        dropdown_source = create_explore_button(query_widget, table_name=tables)

        clean_checkbox_source = widgets.Checkbox(
            value=True,
            description='✨ Clean this?',
            disabled=False,
            indent=False
        )
        
        display(clean_checkbox_source)

        display(HTML(f"<b>Target Table:</b>"))
        dropdown_target = create_explore_button(query_widget, table_name=tables)
        
        clean_checkbox_target = widgets.Checkbox(
            value=False,
            description='✨ Clean this?',
            disabled=False,
            indent=False
        )
        
        display(clean_checkbox_target)

        next_button = widgets.Button(description="Next", button_style='success')

        def on_button_click(b):
            with self.output_context():
                source_table = dropdown_source.value
                target_table = dropdown_target.value
                
                if source_table == target_table:
                    print("❌ Source and target tables cannot be the same.")
                    return
                
                selected_tables = {"source_table": source_table, "target_table": target_table,
                                "source_clean": clean_checkbox_source.value, 
                                "target_clean": clean_checkbox_target.value}
                callback(selected_tables)
                
        next_button.on_click(on_button_click)
        display(next_button)

class StageForSourceTarget(MultipleNode):
    
    default_name = 'Stage For All'
    default_description = 'This stages the source tables'

    def construct_node(self, element_name, all_tables=None, idx=0, total=1, stage_viewer=None, clean=None):
        if all_tables is None:
            all_tables = []
        if self.item is not None:
            con = self.item.get("con", None)
            query_widget = self.item.get("query_widget", None)
        else:
            con = None
            query_widget = None
            
        if clean is None:
            clean = [True] * total
        
        
        table_name = element_name
        
        para = {"table_name": table_name, 
                "table_idx": idx, 
                "all_tables": all_tables,
                "table_total": total, 
                "stage_viewer": stage_viewer,
                "clean": clean}
        
        for key in self.para:
            para[key] = self.para[key]
            
            
        if clean[idx]:
            _, workflow = create_cocoon_stage_workflow(con=con, query_widget=query_widget, 
                                                        table_name=table_name, para=para, output=self.output)
        else:
            _, workflow = create_cocoon_profile_yml_workflow(con=con, query_widget=query_widget, 
                                                       table_name=table_name, para=para, output=self.output)
            
        workflow.add_as_root(StageSourceTargetProgress())
        
        return workflow

    def extract(self, item):
        table_document = self.get_sibling_document('Select Source and Target Table')
        
        self.elements = [table_document["source_table"], table_document["target_table"]]
        
        clean = [table_document["source_clean"], table_document["target_clean"]]
        stage_viewer = [False]
        self.nodes = {table_document["source_table"]: self.construct_node(table_document["source_table"], 
                                          all_tables=self.elements, idx=0, total=2, 
                                          stage_viewer=stage_viewer, clean=clean),
                      table_document["target_table"]: self.construct_node(table_document["target_table"], 
                                          all_tables=self.elements, idx=1, total=2, 
                                          stage_viewer=stage_viewer, clean=clean)}
    
    def display_after_finish_workflow(self, callback, document):
        data_project = self.para["data_project"]
        
        source_table_object = self.nodes[self.elements[0]].para["table_object"]
        data_project.table_object["source"] = source_table_object
        target_table_object = self.nodes[self.elements[1]].para["table_object"]
        data_project.table_object["target"] = target_table_object
        
        callback({})


class CreateShortSourceTableSummary(CreateShortTableSummary):
    default_name = 'Create Short Source Table Summary'
    default_description = 'Create a short summary of the source table'
    
    def extract(self, input_item):
        clear_output(wait=True)

        print("📝 Generating table summary ...")
    
        self.progress = show_progress(1)

        con = self.item["con"]
        source_table = self.get_sibling_document('Select Source and Target Table')["source_table"]
        sample_size = 5

        sample_query_str = f'SELECT * FROM "{source_table}"'
        sample_df_query = sample_query(con, sample_query_str, sample_size)
        sample_df = run_sql_return_df(con, sample_df_query)
        sample_df = sample_df.applymap(truncate_cell)
        
        table_desc = sample_df.to_csv(index=False, quoting=1)
        
        self.sample_df = sample_df

        return source_table, table_desc
    
class CreateShortTargetTableSummary(CreateShortTableSummary):
    default_name = 'Create Short Target Table Summary'
    default_description = 'Create a short summary of the target table'
    
    def extract(self, input_item):
        clear_output(wait=True)

        print("📝 Generating table summary ...")
    
        self.progress = show_progress(1)

        con = self.item["con"]
        target_table = self.get_sibling_document('Select Source and Target Table')["target_table"]
        sample_size = 5

        sample_df = run_sql_return_df(con, f'SELECT * FROM "{target_table}" LIMIT {sample_size}')
        sample_df = sample_df.applymap(truncate_cell)
        table_desc = sample_df.to_csv(index=False, quoting=1)

        self.sample_df = sample_df

        return target_table, table_desc
    
    
class UnderstandSourceToTargetTransform(Node):
    default_name = 'Understand Source to Target Transform'
    default_description = 'Understand the transformation from source to target table'
    
    def extract(self, item):
        clear_output(wait=True)
        
        create_progress_bar_with_numbers(0, transform_steps)
        print("🔍 Understanding the transform...")
        self.progress = show_progress(1)
        
        self.item = item
        con = self.item["con"]
        
        data_project = self.para["data_project"]
        
        source_table_object = data_project.table_object["source"]
        target_table_object = data_project.table_object["target"]
        
        source_table_summary = source_table_object.table_summary
        target_table_summary = target_table_object.table_summary
        
        sample_size = 5
        source_table = source_table_object.table_name
        target_table = target_table_object.table_name

        source_sample_df = run_sql_return_df(con, f'SELECT * FROM "{source_table}" LIMIT {sample_size}')
        source_sample_df = source_sample_df.applymap(truncate_cell)
        source_table_desc = source_sample_df.to_csv(index=False, quoting=1)
        
        target_sample_df = run_sql_return_df(con, f'SELECT * FROM "{target_table}" LIMIT {sample_size}')
        target_sample_df = target_sample_df.applymap(truncate_cell)
        target_table_desc = target_sample_df.to_csv(index=False, quoting=1)
        
        source_column_desc = source_table_object.print_column_desc()
        target_column_desc = target_table_object.print_column_desc()
        
        target_columns = target_table_object.columns
        
        return source_table, target_table, source_table_desc, source_table_summary, \
                target_table_desc, target_table_summary, source_column_desc, target_column_desc, target_columns
    
    def run(self, extract_output, use_cache=True):
        _, _, source_table_sample, source_table_description, target_table_sample, target_table_description, \
            source_column_desc, target_column_desc, _ = extract_output
        
        template = f"""You have a source table:
{source_table_sample}
{source_column_desc}

And target table:
{target_table_sample}
{target_column_desc}

Now, verify if any of the target table columns can be derived from source table through SQL. 
If so, describe the detailed instructions for these transformable columns.
Respond in the following json:
```json
{{  
    "reasoning": "The transformation is (not) possible through SQL for any column...",
    "selection_instruction": {{ # leave empty if no transformation
        "target_column": "String manipulation/Case When clauses/Direct copy... from X column", # short in 10 words
    }}, 
    "where_instruction": "Select ..." # Empty if no where
    "group_by_instruction": "Group by ..." # Empty if no group by
    "having_instruction": "Having ..." # Empty if no having
    "transformable": true/false,
    
}}
"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)
        assistant_message = response['choices'][0]['message']
        messages.append(assistant_message)
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        summary = json.loads(json_code)
        
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"reasoning": "There is some issue with this transformation. Please specify the steps.", 
                "selection_instruction": {},
                "where_instruction": "",
                "group_by_instruction": "",
                "having_instruction": "",
                "transformable": False}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        source_table, target_table, _, _, _, _, _, _, target_columns = extract_output
        
        self.progress.value += 1
        query_widget =  self.item["query_widget"]
        create_explore_button(query_widget, table_name=[source_table, target_table])
        
        summary = run_output
        
        if summary["transformable"]:
            display(HTML(f"✔️ The transformation is possible through SQL: <i>{summary['reasoning']}"))
        else:
            display(HTML("❌ The transformation doesn't seem to be possible through SQL.<br>Please edit below to provide more information."))
        
        data = {
            'Column': [],
            'Can Transform?': [],
            'Instruction': [],
        }
        
        for column in target_columns:
            data['Column'].append(column)
            data['Can Transform?'].append(True if column in summary['selection_instruction'] else False)
            data['Instruction'].append(summary['selection_instruction'].get(column, ''))
            
        df = pd.DataFrame(data)
        editable_columns = [False, True, True]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        display(grid)
        
        data = {
            'Clause': ["WHERE", "GROUP BY", "HAVING"],
            'Instruction': [summary.get("where_instruction", ""), 
                            summary.get("group_by_instruction", ""), 
                            summary.get("having_instruction", "")]
        }
        
        df = pd.DataFrame(data)
        editable_columns = [False, True]
        grid_clause = create_dataframe_grid(df, editable_columns, reset=True)
        display(grid_clause)
        

        
        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
        )
        
        def on_next_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                new_df =  grid_to_updated_dataframe(grid)
                document = {}
                document["selection"] = new_df.to_json(orient="split")
                
                clause_df = grid_to_updated_dataframe(grid_clause)
                document["clause"] = clause_df.to_json(orient="split")
                
                callback(document)
        
        next_button.on_click(on_next_button_clicked)
        display(next_button)
      
        


class WriteCode(Node):
    default_name = 'Write Code'
    default_description = 'Write code for table transformation'

    def extract(self, input_item):
        create_progress_bar_with_numbers(1, transform_steps)
        print("💻 Writing the codes...")
        self.progress = show_progress(1)
        con = self.item["con"]
        
        data_project = self.para["data_project"]
        
        source_table_object = data_project.table_object["source"]
        target_table_object = data_project.table_object["target"]
        
        transform_df = pd.read_json(self.get_sibling_document('Understand Source to Target Transform')["selection"], orient="split")
        clause_df = pd.read_json(self.get_sibling_document('Understand Source to Target Transform')["clause"], orient="split")
        
        transform_df = transform_df[transform_df['Can Transform?']]
        target_columns = transform_df['Column'].tolist()
        
        sample_size = 5
        source_table = source_table_object.table_name
        target_table = target_table_object.table_name

        source_sample_df = run_sql_return_df(con, f'SELECT * FROM "{source_table}" LIMIT {sample_size}')
        source_sample_df = source_sample_df.applymap(truncate_cell)
        source_table_sample = source_sample_df.to_csv(index=False, quoting=1)
        
        target_sample_df = run_sql_return_df(con, f"""SELECT {', '.join({enclose_table_name(col, con=con)} for col in target_columns)} FROM "{target_table}" LIMIT {sample_size}""")
        target_sample_df = target_sample_df.applymap(truncate_cell)
        target_table_sample = target_sample_df.to_csv(index=False, quoting=1)    
        
        source_column_desc = source_table_object.print_column_desc()
        target_column_desc = target_table_object.print_column_desc(target_columns)
        
        transformation_reasoning = ""
        for column in target_columns:
            transformation_reasoning += f"{column}: {transform_df.loc[transform_df['Column'] == column, 'Instruction'].values[0]}\n"
        
        for index, row in clause_df.iterrows():
            if row["Instruction"]:
                transformation_reasoning += f"{row['Clause']}: {row['Instruction']}\n"
        
        return source_table, source_table_sample, target_table_sample, source_column_desc, target_column_desc, transformation_reasoning
    
    def run(self, extract_output, use_cache=True):
        source_table, source_table_sample, target_table_sample, source_column_desc, target_column_desc, transformation_reasoning = extract_output
        
        template = f"""You have a source table: 
{source_table_sample}
{source_column_desc}
  
And target table:
{target_table_sample}
{target_column_desc}

{transformation_reasoning}

Now, write the SQL clause to transform the source table to the target table.

SELECT {{need_distinct}} {{selection_clause}}
FROM source_table
WHERE {{condition_clause}}
GROUP BY {{group_by_clause}}
HAVING {{having_clause}}

Respond in the following json:
```json
{{  
    "reasoning": "To transform, we (don't) need aggregation. We select ...",
    "need_aggregation": true/false,
    "need_distinct": true/false,
    "selection_clause": "A as A, B as B, ...", # ignore not transformable columns
    "condition_clause": "A > 10 AND B < 20", # "" if no condition
    "group_by_clause": "A, B",  # "" if no group by
    "having_clause": "COUNT(*) > 1" # "" if no having
}}
"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        
        assistant_message = response['choices'][0]['message']
        messages.append(assistant_message)
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        summary = json.loads(json_code)
        
        summary["source_table"] = source_table
        
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        source_table, _, _, _, _, _ = extract_output
        return {"reasoning": "There is some issue with this transformation", 
                "need_aggregation": False,
                "need_distinct": False,
                "selection_clause": "*",
                "condition_clause": "",
                "group_by_clause": "",
                "having_clause": "",
                "source_table": source_table
          }
        
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
        
class DebugCode(Node):
    default_name = 'Debug Code'
    default_description = 'Debug the code'

    def extract(self, input_item):
        clear_output(wait=True)
        display(HTML(f"{running_spinner_html} Verifying the codes..."))
        self.progress = show_progress(1)
        
        con = self.item["con"]
        code_clauses = self.get_sibling_document("Write Code")
        
        code_clauses = code_clauses.copy()
        
        return con, code_clauses
    
    def run(self, extract_output, use_cache=True):
        
        con, code_clauses = extract_output
        max_iterations = 10
        self.messages = []
        
        for i in range(max_iterations):
            try:
                sql = f"""SELECT {code_clauses['selection_clause']}
FROM {code_clauses['source_table']}
{'WHERE ' + code_clauses['condition_clause'] if code_clauses['condition_clause'] else ''}
{'' if not code_clauses['group_by_clause'] else 'GROUP BY ' + code_clauses['group_by_clause']}
{'' if not code_clauses['having_clause'] else 'HAVING ' + code_clauses['having_clause']}"""
                df = run_sql_return_df(con, sql)
                code_clauses["output_columns"] = list(df.columns)
                break
            except Exception: 
                detailed_error_info = get_detailed_error_info()
                json_template =""
                if "condition_clause" in code_clauses and code_clauses["condition_clause"]:
                    json_template += f',\n    "condition_clause": "{code_clauses["condition_clause"]}"'
                if "group_by_clause" in code_clauses and code_clauses["group_by_clause"]:
                    json_template += f',\n    "group_by_clause": "{code_clauses["group_by_clause"]}"'
                if "having_clause" in code_clauses and code_clauses["having_clause"]:
                    json_template += f',\n    "having_clause": "{code_clauses["having_clause"]}"'
                template = f"""You have the following SQL:
{sql}
It has an error: {detailed_error_info}

Please correct the SQL, but don't change the logic. Respond in the following json:
```json
{{  
    "reasoning": "The error is caused by ...",
    "selection_clause": "{code_clauses['selection_clause']}" (correct it){json_template}
}}
"""

                messages = [{"role": "user", "content": template}]
                response = call_llm_chat(messages, temperature=0.1, top_p=0.1)
                
                json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
                summry = json.loads(json_code)
                
                assistant_message = response['choices'][0]['message']
                messages.append(assistant_message)
                self.messages.append(messages)
                
                if "selection_clause" in summry:
                    code_clauses["selection_clause"] = summry["selection_clause"]
                if "condition_clause" in summry:
                    code_clauses["condition_clause"] = summry["condition_clause"]
                if "group_by_clause" in summry:
                    code_clauses["group_by_clause"] = summry["group_by_clause"]
                if "having_clause" in summry:
                    code_clauses["having_clause"] = summry["having_clause"]
                    
        return code_clauses
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        print("🎉 We have finished the coding!")
        
        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"
        border_style = """
        <style>
        .border-class {
            border: 1px solid black;
            padding: 10px;
            margin: 10px;
        }
        </style>
        """
        combined_css = css_style + border_style
        
        selection_clause = ',\n'.join(run_output['selection_clause'].split(','))
        
        sql_query = f"""SELECT 
{indent_paragraph(selection_clause)}
FROM {run_output['source_table']}
{'WHERE ' + run_output['condition_clause'] if run_output['condition_clause'] else ''}
{'' if not run_output['group_by_clause'] else 'GROUP BY ' + run_output['group_by_clause']} 
{'' if not run_output['having_clause'] else 'HAVING ' + run_output['having_clause']}"""

        highlighted_sql = wrap_in_scrollable_div(highlight(sql_query, SqlLexer(), formatter))
        bordered_content = f'<div class="border-class">{highlighted_sql}</div>'
        display(HTML(combined_css + bordered_content))
        
        test_button = widgets.Button(
            description='Test Transform',
            disabled=False,
            button_style='info',
            tooltip='Click to test',
            icon='play'
        )
        query_widget = self.item["query_widget"]
        def on_button_clicked(b):
            with self.output_context():
                query_widget.run_query(sql_query)
                print("😎 Query submitted. Check out the data widget!")
        
        test_button.on_click(on_button_clicked)
        print("🧪 Please test the cast and ensure the result is as expected.")
        display(test_button)
        
        
        
        
        
            

class DecideStringCategoricalForAll(MultipleNode):
    default_name = 'Decide If Categorical For All'
    default_description = 'This node allows users to decide if a string column should be free text or categorical.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        node = DecideStringCategorical(para=para, id_para ="column_name")
        node.inherit(self)
        return node

    def extract(self, item):
        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        database_name = get_database_name(con)
        
        schema = table_pipeline.get_schema(con)
        
        columns = []
        for col, col_type in schema.items():
            reverse_type = get_reverse_type(col_type, database_name)
            if is_type_string(reverse_type):
                if database_name == "SQL Server":
                    if col_type.lower() not in ['text', 'ntext', 'image']:
                        columns.append(col)
                else:
                    columns.append(col)
        
        self.elements = []
        self.nodes = {}
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('category_test') is False):
            return
        
        idx = 0
        for col in columns:
            self.elements.append(col)
            self.nodes[col] = self.construct_node(col, idx, len(columns))
            idx += 1
            
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]
        
        create_progress_bar_with_numbers(3, doc_steps)
        create_explore_button(query_widget, table_pipeline)
        
        data = {
            'Column': [],
            'Should Categorical?': [],
            'Current Domain': [],
            'Acceptable Values': [],
            'Explanation': []
        }

        if 'Decide If Categorical' in document:
            for column_name, details in document['Decide If Categorical'].items():
                if details['can_enumerate']:
                    data['Column'].append(column_name)
                    data['Should Categorical?'].append(True)
                    data['Current Domain'].append(details['current_domain'])
                    if details['current_full']:
                        domain_list = [str(item) for item in details['domain']]
                        data['Acceptable Values'].append(domain_list)
                    else:
                        domain_list = [str(item) for item in details['full_domain']]
                        data['Acceptable Values'].append(domain_list)
                    data['Explanation'].append(details['explanation'])

        df = pd.DataFrame(data)

        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return

        editable_columns = [False, True, True, True, False]
        reset = True
        lists = ['Current Domain']
        long_text = ['Explanation']
        editable_list = {
            'Acceptable Values': {}
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset, lists=lists, 
                                     long_text=long_text, editable_list=editable_list)


        next_button = widgets.Button(
            description='Accept',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        reject_button = widgets.Button(
            description='Reject',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
        
        def on_reject_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, reset=reset, lists=lists, editable_list=editable_list)
                new_df['Should Categorical?'] = False
                document = new_df.to_json(orient="split")
                callback(document)
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, reset=reset, lists=lists, editable_list=editable_list)
                document = new_df.to_json(orient="split")

                table_object = self.para["table_object"]
                for idx, row in new_df.iterrows():
                    column = row['Column']
                    if row['Should Categorical?']:
                        table_object.category[column] = row['Current Domain']
                        table_object.potential_category[column] = [item for item in row['Acceptable Values'] if item not in row['Current Domain']]
                
                callback(document)
        
        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_reject_button_clicked)

        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
            
        
        display(HTML(f"😎 Please verify column accepted values. We will add column tests:"))
        display(grid)
        display(HBox([reject_button, next_button]))
        
        
class DecideStringCategoricalForAllContinue(DecideStringCategoricalForAll):
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)

        create_progress_bar_with_numbers(2, doc_steps)
        
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        
        data = {
            'Column': [],
            'Is Categorical?': [],
            'Acceptable Values': [],
            'Explanation': []
        }

        if 'Decide If Categorical' in document:
            for column_name, details in document['Decide If Categorical'].items():
                if details['can_enumerate']:
                    data['Column'].append(column_name)
                    data['Is Categorical?'].append(True)
                    if details['current_full']:
                        domain_list = [str(item) for item in details['domain']]
                        data['Acceptable Values'].append(domain_list)
                    else:
                        domain_list = [str(item) for item in details['full_domain']]
                        data['Acceptable Values'].append(domain_list)
                    data['Explanation'].append(details['explanation'])

        df = pd.DataFrame(data)

        callback(df.to_json(orient="split"))

        
class VerifyTests(Node):
    default_name = 'Verify Tests'
    default_description = 'Verify the tests'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        unique_df = pd.read_json(self.get_sibling_document('Decide Unique Columns'), orient="split")
        
        unique_editable_columns = [False, False, True, True]
        unique_grid = create_dataframe_grid(unique_df, unique_editable_columns, reset=True)
        
        print("😎 We have identified columns that should be unique. Please verify:")
        display(unique_grid)
        
        category_df = pd.read_json(self.get_sibling_document('Decide If Categorical For All'), orient="split")
        category_editable_columns = [False, True, True, False]
        category_reset = True
        category_lists = ['Current Domain']
        category_long_text = ['Explanation']
        category_grid = create_dataframe_grid(category_df, category_editable_columns, 
                                     reset=category_reset, lists=category_lists, 
                                     long_text=category_long_text)

        print("😎 We have identified columns that should be categorical. Please verify:")
        display(category_grid)
        
        next_button = widgets.Button(
            description='Accept Unique',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                summary = {}
                unique_df =  grid_to_updated_dataframe(unique_grid)
                summary["uniqueness"] = unique_df.to_json(orient="split")
                
                table_object = self.para["table_object"]
                
                for index, row in unique_df.iterrows():
                    table_object.uniqueness[row["Column"]] = {}
                    if row["Is Unique?"]:
                        table_object.uniqueness[row["Column"]]["current_unique"] = True
                    if row["Should Unique?"]:
                        table_object.uniqueness[row["Column"]]["unique_reason"] = row["Explanation"]
                
                category_df =  grid_to_updated_dataframe(category_grid, 
                                                        reset=category_reset, 
                                                        lists=category_lists)
                summary["category"] = category_df.to_json(orient="split")
                
                for idx, row in category_df.iterrows():
                    column = row['Column']
                    table_object.category[column] = row['Acceptable Values']
                callback(summary)
        
        next_button.on_click(on_button_clicked)

        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        

class DecideStringCategorical(Node):
    default_name = 'Decide If Categorical'
    default_description = 'This node allows users to decide if a string column should be free text or categorical.'
    default_sample_size = 30
    max_length_threshold = 200

    def extract(self, input_item):
        clear_output(wait=True)
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        table_object = self.para["table_object"]
        
        column_name = self.para["column_name"]
        column_desc = table_object.column_desc.get(column_name, "")
        
        display(HTML(f"{running_spinner_html} Detecting category for <i>{table_name}[{column_name}]</i>..."))
        create_progress_bar_with_numbers(3, doc_steps)

        idx = self.para["column_idx"]
        total = self.para["total_columns"]
        show_progress(max_value=total, value=idx)

        self.input_item = input_item
        
        sample_size = self.class_para.get("sample_size", self.default_sample_size)

        query = create_sample_distinct_query(con, table_pipeline, column_name, sample_size)
        with_context = table_pipeline.get_codes(mode="WITH")
        query = with_context + "\n" + query
        sample_values = run_sql_return_df(con,query)
        
        query =  construct_distinct_count_query(con, table_pipeline, column_name)
        query = with_context + "\n" + query
        total_distinct_count = run_sql_return_df(con,query).iloc[0, 0]
        
        return column_name, column_desc, sample_values, sample_size, total_distinct_count


    def run(self, extract_output, use_cache=True):
        column_name, column_desc, sample_values, sample_size, total_distinct_count = extract_output
        
        if len(sample_values) == 0:
            return {"explanation": "Column is fully missing", "can_enumerate": False}
        
        if sample_size < total_distinct_count:
            return {"explanation": "The column has too many distinct values.", "can_enumerate": False}

        max_length = sample_values[column_name].apply(len).max()
        
        if max_length > self.max_length_threshold:
            return {"explanation": "Columns have long strings", "can_enumerate": False}
        
        


        
        template = f"""{column_name} is: {column_desc}
        
The CURRENT DOMAIN: {sample_values.to_csv(index=False, header=False, quoting=1, quotechar="'")}

Task: from CURRENT DOMAIN, extrapolate FULL DOMAIN of all possible values

First, check if the potential FULL DOMAIN has
(1) a limited domain size (<{sample_size}) 
(2) well-know values you CAN enumerate.
If so, enumerate the FULL DOMAIN:

Now, respond in yml:
```yml
explanation: >
    The column means ... The CURRENT DOMAIN contains ...
    The FULL DOMAIN will contains X elements, which is above/below {sample_size}
    The domain values are (not) well known to enumerate
limited_size: true/false # depend on whether below/above {sample_size}
well_known: true/false # are the values well known?
current_full: true/false # if both above are true, is the current domain full
full_domain: # only if limited_size, well_known, and not current_full
    - '%'
    - "O'Neil" # escape single quote
    - ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "explanation" in jc, "The 'explanation' key is missing in the JSON code."),
            (lambda jc: "limited_size" in jc, "The 'can_enumerate' key is missing in the JSON code."),
            (lambda jc: "well_known" in jc, "The 'can_enumerate' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["limited_size"], bool), "The value of 'can_enumerate' must be a boolean."),
            (lambda jc: isinstance(jc["well_known"], bool), "The value of 'can_enumerate' must be a boolean."),

        ]

        for check, error_message in checks:
            if not check(summary):
                raise ValueError(f"Validation failed: {error_message}")
        
        if summary["limited_size"] and summary["well_known"]:
            summary["can_enumerate"] = True
        else:
            summary["can_enumerate"] = False
        
        summary["domain"] = sample_values[column_name].tolist()
        
            
        
        if (not summary["can_enumerate"]) or summary["current_full"]:
            summary["full_domain"] = []
        
        if "full_domain" in summary and len(summary["full_domain"]) == 0 :
            summary["current_full"] = True
            
        if "full_domain" in summary and len(summary["full_domain"]) > 100:
            summary["can_enumerate"] = False
            summary["explanation"] = "The domain is too large to enumerate."
            del summary["full_domain"]
            del summary["current_full"]
            
        if "full_domain" in summary and summary["can_enumerate"] and not summary["current_full"]:
            missing_values = set(summary["domain"]) - set(summary["full_domain"])
            if missing_values:
                summary["full_domain"].extend(list(missing_values))
        
        summary["current_domain"] = sample_values[column_name].tolist()
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"explanation": "Fail to run", "can_enumerate": False}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
        
        
class SelectTables(Node):
    default_name = 'Select Tables'
    default_description = 'This step allows you to select multiple tables.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        con = self.input_item["con"]
        query_widget = self.item["query_widget"]
        
        create_progress_bar_with_numbers(0, model_steps)

        def get_databases():
            return list_databases(con)
        
        def get_schemas(database):
            return list_schemas(con, database)
        
        def get_tables(database, schema, include_views):
            tables = list_tables(con, schema, database)
            if include_views:
                tables += list_views(con, schema, database)
            return tables
        
        def read_from_sources_yml():
            if "dbt_directory" in self.para:
                try:
                    dbt_directory = self.para["dbt_directory"]
                    sources_file_path = os.path.join(dbt_directory, "sources.yml")
                    return read_source_file(sources_file_path)
                except Exception as e:
                    pass
            
            return None, None, None

        default_database, default_schema, selected_tables = read_from_sources_yml()

        all_databases = get_databases()

        if default_database and default_database not in all_databases:
            print(f"⚠️ Can't find the database '{default_database}' specified in sources.yml")
            default_database = None
            

        if default_database:
            all_schemas = get_schemas(default_database)
            if default_schema not in all_schemas:
                print(f"⚠️ Can't find the schema '{default_schema}' specified in sources.yml")
                default_schema = None
        else:
            all_schemas = []

        if not default_database or not default_schema:
            default_database = self.para.get("default_database")
            default_schema = self.para.get("default_schema")

            if default_database not in all_databases:
                default_database = None
            if default_database and default_schema not in get_schemas(default_database):
                default_schema = None

            if not default_database or not default_schema:
                default_database, default_schema = get_default_database_and_schema(con)

        all_schemas = get_schemas(default_database)

        database_label = widgets.HTML(value="<b>Database</b>:")
        database_dropdown = widgets.Dropdown(
            options=all_databases,
            value=default_database,
            layout=widgets.Layout(width='150px')
        )
        
        schema_label = widgets.HTML(value="<b>Schema</b>:")
        schema_dropdown = widgets.Dropdown(
            options=all_schemas,
            value=default_schema,
            layout=widgets.Layout(width='150px')
        )
        
        include_views_toggle = widgets.Checkbox(
            value=False,
            description='',
            indent=False,
            layout=widgets.Layout(width='auto', margin='3px 0 0 0')
        )
        
        view_warning = widgets.HTML(
    value='''
    <div style="line-height: 1.2; margin-left: 5px;">
        <div><b>Include Views</b></div>
        <div style="color: orange; font-style: italic; font-size: 0.9em;">
            ⚠️ Views can be slow to query. We recommend materializing them to tables.
        </div>
    </div>
    '''
)
        
        def on_database_change(change):
            schemas = get_schemas(change['new'])
            schema_dropdown.options = schemas
            schema_dropdown.value = schemas[0] if schemas else None
            update_tables()
        
        def on_schema_change(change):
            update_tables()
        
        def on_include_views_change(change):
            update_tables()
        
        
        database_dropdown.observe(on_database_change, names='value')
        schema_dropdown.observe(on_schema_change, names='value')
        include_views_toggle.observe(on_include_views_change, names='value')
        
        display(widgets.HBox([database_label, database_dropdown, schema_label, schema_dropdown]))
        checkbox_with_label = widgets.HBox(
            [include_views_toggle, view_warning], 
            layout=widgets.Layout(align_items='center')
        )
        
        
        tables = get_tables(default_database, default_schema, include_views_toggle.value)
        
        def update_tables():
            nonlocal tables
            tables = get_tables(database_dropdown.value, schema_dropdown.value, include_views_toggle.value)
            multi_select.options = [(table, i) for i, table in enumerate(tables)]

        display(HTML(f"🤓 Please select the tables:"))
        multi_select = create_column_selector_(columns=tables, default=True, selected=selected_tables)
        
        if selected_tables and not set(selected_tables).issubset(set(tables)):
            include_views_toggle.value = True
            multi_select.value = [option[1] for option in multi_select.options if option[0] in selected_tables]
        
    
        display(checkbox_with_label)
        

        
        next_button = widgets.Button(description="Next", button_style='success')
        
        def on_button_click(b):
            with self.output_context():
                selected_tables = [tables[i] for i in multi_select.value]
                
                if not selected_tables:
                    display(HTML(f"<div style='color: red;'>❌ No table selected. Please select at least 1 table</div>"))
                    return
                
                data_project = self.para["data_project"]
                display(HTML(f"{running_spinner_html} Reading the selected tables..."))
                
                self.para["default_database"] = database_dropdown.value
                self.para["default_schema"] = schema_dropdown.value
                
                for table_name in selected_tables:
                    data_project.add_table(table_name, get_table_schema(con, table_name, 
                                           database=database_dropdown.value, schema=schema_dropdown.value))
                    
                    sql_step = SQLStep(table_name=table_name, con=con, database=database_dropdown.value, 
                                       schema=schema_dropdown.value, materialized=True)
                    pipeline = TransformationSQLPipeline(steps=[sql_step], edges=[])
                    data_project.table_pipelines[table_name] = pipeline
                    
                if "dbt_directory" in self.para:
                    yml_dict = OrderedDict()
                    yml_dict["version"] = 2
                    yml_dict["sources"] = []
                    yml_dict["sources"].append(OrderedDict({
                        "name": "cocoon",
                        "database": database_dropdown.value,
                        "schema": schema_dropdown.value,
                        "tables": [{"name": table_name} for table_name in selected_tables]
                    }))
                    
                    yml_content = yaml.dump(yml_dict, default_flow_style=False)
                    labels = ["YAML"]
                    file_names = [os.path.join(self.para["dbt_directory"], "sources.yml")]
                    contents = [yml_content]
                    
                    if not file_exists(self.para["dbt_directory"]):
                        create_directory(self.para["dbt_directory"])
                    
                    overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
                    overwrite_checkbox.value = True
                    save_files_click(save_button)  
                
                callback({"selected_tables": selected_tables})
        
        next_button.on_click(on_button_click)
        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
class StageTablesForAll(MultipleNode):
    default_name = 'Stage Tables For All'
    default_description = 'This node stages the tables for all selected tables.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["table_name"] = element_name
        para["table_idx"] = idx
        para["total_tables"] = total
        
        if self.item is not None:
            con = self.item.get("con", None)
        else:
            con = None
        
        para["viewer"] = True
        sql_step = SQLStep(table_name=element_name, con=con)
        pipeline = TransformationSQLPipeline(steps = [sql_step], edges=[])
        para["table_pipeline"] = pipeline
        
        node = CreateShortTableSummary(para=para, id_para ="table_name")
        node.inherit(self)
        return node

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Understanding the tables...")

        self.input_item = item
        data_project = self.para["data_project"]
        selected_tables = self.get_sibling_document('Select Tables')["selected_tables"]
        self.elements = []
        self.nodes = {}

        idx = 0
        for table_name in selected_tables:
            self.elements.append(table_name)
            self.nodes[table_name] = self.construct_node(table_name, idx, len(selected_tables))
            idx += 1

    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
    
        callback(document)


class SourceProjectStoryUnderstand(Node):
    default_name = 'Source Project Understand'
    default_description = 'This understands the source tables and dbt project'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(0, self.get_sibling_document('Product Steps'))

        print("🤓 Generating the story of the project ...")
        self.progress = show_progress(1)

        data_project = self.para["data_project"]

        
        table_summary_document = self.get_sibling_document('Stage Tables For All')
        description = ""
        
        if "Create Short Table Summary" in table_summary_document:
            for table_name in table_summary_document["Create Short Table Summary"]:
                table_summary = table_summary_document["Create Short Table Summary"][table_name]
                description += f"{table_name}: {table_summary}\n"
        
        tables = data_project.list_tables()

        return description, tables

    def run(self, extract_output, use_cache=True):
        description, tables = extract_output
        template = f"""You have tables:
{description}
Desribe the story behind the tables, in a short sequence (1-10) of steps, each as simple SVO sentences (~5 words). 
Focus on the big picture. It's fine to skip some unimportant tables. Use consistent terms (e.g., customer/user may mean the same). 
Please disregard the system side of the tables (e.g., DONT: data are collected, stored in s3, used for analysis)

Return in the following format:
```json
{{
    "reasoning": "The tables are ... First, this happens. Then, that happens",
    "story": [ 
        "Customer buys items in the store",
        "Customer returns items",
        ...
    ]
}}
```"""
        
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"story": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        self.progress.value += 1
        
        _, tables = extract_output
        query_widget = self.input_item["query_widget"]
        
        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        print("🎉 Based on the table, we've generated the story!")
        print("😊 We will use it to understand the project. Please correct it but keep it simple:")
        display_container = create_list_of_strings(summary["story"])
        display(display_container)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                summary = {}
                current_strings_displayed = extract_strings_from_display(display_container)
                if len(current_strings_displayed) == 0:
                    print("⚠️ Please provide a story with at least one line")
                    return 

                summary["story"] = current_strings_displayed
                data_project = self.para["data_project"]
                data_project.set_story(summary["story"])
                callback(summary)

        next_button.on_click(on_button_click)

        display(next_button)
        
class DecideHubs(Node):
    default_name = 'Decide Hubs'
    default_description = 'This step allows you to decide the hubs.'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item

        print("🤓 Deciding the important concepts...")
        self.progress = show_progress(1)

        data_project = self.para["data_project"]

        description = data_project.describe_project()
        tables = data_project.list_tables()
        story = data_project.get_story()

        return description, tables, story
    
    def run(self, extract_output, use_cache=True):
        description, tables, story = extract_output
        story_text = ",\n".join(story)
        template = f"""You have a list of tables:
{description}

Based on the story:
{story_text}

Please recommend the hubs for the data vault:
1. The hubs are the core concepts appear in *MANY* tables. E.g., Customer, Product
2. Time, location are not hubs.

Return in the following format:
```json
{{
    "reasoning": "The stories are ... XX appears in many tables, and is not time/location",
    "hubs": {{
        "Product": "Product that are produced and sold", # short desc in < 10 words 
    }}
}}
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"hubs": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        self.progress.value += 1
        
        _, tables, story = extract_output
        query_widget = self.input_item["query_widget"]
        
        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        
        
        story_html = f"<ol>{''.join([f'<li>{s}</li>' for s in story])}</ol>"
        
        html_content = f"""🎉 We've recommended the hubs, based on the story:<br>
{story_html}
😊 These would be the core concept. Please refine them, but keep them short and simple.<br>
⚠️ Please don't create hubs for time/location. We will treat them differently."""
        
        display(HTML(html_content))

        data = {
            'Hub': [],
            'Description': [],
        }
        
        for hub, desc in summary["hubs"].items():
            data['Hub'].append(hub)
            data['Description'].append(desc)
            
        initial_df = pd.DataFrame(data)
        can_be_empty= [False, True]
        df_widget = create_df_strings(initial_df, can_be_empty=can_be_empty)
        display(df_widget)

        next_button = widgets.Button(description="Next Step", 
                                     button_style='success',
                                     icon='check')

        def on_button_click(b):
            with self.output_context():
                df_new = extract_df_from_display(df_widget)
                
                if len(df_new) == 0:
                    print("⚠️ Please add at least 1 hub.")
                    return
                
                document = df_new.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_click)

        display(next_button)

class DecideSingleMultiple(Node):
    default_name = 'Decide Single/Multiple'
    default_description = 'This step allows you to decide whether each table is related to a single hub or multiple hubs.'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item

        print("🤓 Analyzing tables based on the hubs...")
        
        self.progress = show_progress(1)
        
        data_project = self.para["data_project"]
        description = data_project.describe_project()
        tables = data_project.list_tables()
        story = data_project.get_story()
        hubs = self.get_sibling_document('Decide Hubs')["hubs"]
        
        return description, tables, story, hubs
    
    def run(self, extract_output, use_cache=True):
        description, tables, story, hubs = extract_output
        story_text = ",\n".join(story)
        template = f"""You have a list of tables:
{description}

Based on the story:
{story_text}

And the hubs:
{hubs}

Please decide whether each table is related to multiple hubs (potentially for links).
Return in the following format:
```json
{{
    {tables[0]}: ["it is about ..., and is related to ...", # < 20 words 
                  True/False # if it is related to multiple hubs
                  ], 
    ...
}}
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        description, tables, story, hubs = extract_output
        return {table: ["", False] for table in tables}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        self.progress.value += 1
        
        _, tables, story, _ = extract_output
        query_widget = self.input_item["query_widget"]
        
        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        print("🎉 Based on the story and hubs, we've decided how each table is related!")
        print("😊 Please refine the relationship for each table, but keep it simple.")
        
        
        data = {
            'Column': [],
            'Relation to Hubs': [],
            'Related to Many Hubs?': [],

        }
        
        for table in tables:
            data['Column'].append(table)
            data['Related to Many Hubs?'].append(summary[table][1])
            data['Relation to Hubs'].append(summary[table][0])
            
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset)
        display(grid)
        
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, reset=reset)
                document = new_df.to_json(orient="split")

                callback(document)
        
        next_button.on_click(on_button_clicked)

        display(next_button)



class DecideTableHubRelationForAll(MultipleNode):
    default_name = 'Decide Table Hub Relation For All'
    default_description = 'This node allows users to decide the relation between tables and hubs.'

    def construct_node(self, element_name, idx=0, total=0, hubs=None):
        if hubs is None:
            hubs = []
            
        para = self.para.copy()
        para["table_name"] = element_name
        para["table_idx"] = idx
        para["total_tables"] = total
        para["hubs"] = hubs
        node = DecideTableHubRelation(para=para, id_para ="table_name")
        node.inherit(self)
        return node

    def extract(self, item):
        clear_output(wait=True)

        print("🔍 Deciding the relation between tables and hubs...")

        self.input_item = item

        data_project = self.para["data_project"]
        tables = data_project.list_tables()
        
        reference_tables_document = self.get_sibling_document('Decide Reference Tables')
        reference_tables_df = pd.read_json(reference_tables_document, orient="split")
        reference_tables = reference_tables_df[reference_tables_df["Is Reference Table?"] == True]["Table"].tolist()
        
        tables = [table for table in tables if table not in reference_tables]
        
        hubs = self.get_sibling_document('Decide Hubs')
        self.elements = []
        self.nodes = {}

        idx = 0
        for table in tables:
            self.elements.append(table)
            self.nodes[table] = self.construct_node(table, idx, len(tables), hubs)
            idx += 1
            
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        
        tables = self.para["data_project"].list_tables()
        query_widget = self.item["query_widget"]
        
        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        hub_document = document.get('Decide Table Hub Relation', {})
        
        data = {
            'Table': [],
            'Related Hubs': [],
            'Explanation': [],
        }

        for table_name, details in hub_document.items():
            data['Table'].append(table_name)
            data['Related Hubs'].append(list(details["contain_hubs"].keys()))
            data['Explanation'].append(details["description"])
            
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        hub_document = self.get_sibling_document('Decide Hubs')
        hub_df = pd.read_json(hub_document, orient="split")
        hubs = hub_df["Hub"].tolist()
        
        editable_columns = [False, True, True, True]
        reset = True
        long_text = []
        
        editable_list = {
            'Related Hubs': {
                'allowed_tags': hubs,
                'allow_duplicates': False
            }
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset, long_text=long_text, editable_list=editable_list)
        
        
        hub_desc = ", ".join(hubs)
        print(f"🤓 The hubs are: {hub_desc}")
        
        print("😎 We have decided the relation between tables and hubs. Please verify:")
        display(grid)
        
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                try:
                    new_df =  grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                except Exception as e:
                    print(f"{str(e)}")
                    return
                
                document = new_df.to_json(orient="split")
                callback(document)
                
        next_button.on_click(on_button_clicked)
        display(next_button)

        
class DecideTableHubRelation(Node):
    default_name = 'Decide Table Hub Relation'
    default_description = 'This node allows users to decide the relation between a table and hubs.'

    def extract(self, input_item):
        clear_output(wait=True)

        print("🔍 Deciding the relation between a table and hubs...")
        
        idx = self.para["table_idx"]
        total = self.para["total_tables"]
        show_progress(max_value=total, value=idx)
        
        table_name = self.para["table_name"]
        con = self.item["con"]
        
        hub_document = self.para["hubs"]
        hub_df = pd.read_json(hub_document, orient="split")
        hub_desc = hub_df.apply(lambda x: f"{x['Hub']}: {x['Description']}", axis=1).tolist()
        hub_desc = "\n".join([f"{i+1}. {desc}" for i, desc in enumerate(hub_desc)])
        
        hubs = hub_df["Hub"].tolist()
        
        sample_size = 5
        sample_df = run_sql_return_df(con, f'SELECT * FROM "{table_name}" LIMIT {sample_size}')
        sample_df = sample_df.applymap(truncate_cell)
        table_desc = sample_df.to_csv(index=False, quoting=1)

        return table_name, table_desc, hub_desc, hubs

    def run(self, extract_output, use_cache=True):
        table_name, table_desc, hub_desc, hubs = extract_output

        template = f"""You have a source table {table_name}:
{table_desc}

And the hubs: 
{hub_desc}

This source table contains attribbutes about which hubs, listed above?
If it contains multiple hubs, describe how the hubs are related, in simple SVO sentences (~5 words) where each hub is mentioned.

Now, respond in yml:
```yml
explanation: >
    Among the {len(hubs)} hubs given, the table directly contains ...
contain_hubs: 
    hub_name: [list of attributes directly related to the hub]
    ...
description: A do B, C do D... # leave empty if contain_hubs < 2
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "explanation" in jc, "The 'explanation' key is missing in the JSON code."),
            (lambda jc: "contain_hubs" in jc, "The 'contain_hubs' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["contain_hubs"], dict), "The value of 'contain_hubs' must be a dictionary."),
            (lambda jc: all([hub in hubs for hub in jc["contain_hubs"].keys()]), "The hubs in contain_hubs should be in the hubs"),
        ]

        for check, error_message in checks:
            if not check(summary):
                raise ValueError(f"Validation failed: {error_message}")
            
        summary["contain_hubs"] = {k: v for k, v in summary["contain_hubs"].items() if len(v) > 0}
        
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"explanation": "Fail to run", "contain_hubs": {}, "description": ""}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)

class DecideReferenceTables(Node):
    default_name = 'Decide Reference Tables'
    default_description = 'This step allows you to decide the reference tables.'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item

        print("🤓 Deciding the reference tables...")
        
        self.progress = show_progress(1)
        
        data_project = self.para["data_project"]
        
        table_summary_document = self.get_sibling_document('Stage Tables For All')
        description = ""
        
        if "Create Short Table Summary" in table_summary_document:
            for table_name in table_summary_document["Create Short Table Summary"]:
                table_summary = table_summary_document["Create Short Table Summary"][table_name]
                description += f"{table_name}: {table_summary}\n"
                
        tables = data_project.list_tables()
        story = data_project.get_story()
        hub_document = self.get_sibling_document('Decide Hubs')
        hub_df = pd.read_json(hub_document, orient="split")
        hubs = hub_df["Hub"].tolist()
        
        return description, tables, story, hubs
    
    def run(self, extract_output, use_cache=True):
        description, tables, story, hubs = extract_output
        story_text = ",\n".join(story)
        template = f"""You have a list of tables:
{description}

Reference tables are hub-like tabls but for non-business concepts.
E.g., Locations, Codes (e.g. Airport Short Codes), Categories (e.g., Item category codes), Calendar dates
Finad all the reference tables. Respond in yml:

```yml
explanation: >
    The tables are ... They are reference tables because ...
reference_tables: 
    table_name1: explanation in < 5 words
```"""

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "explanation" in jc, "The 'explanation' key is missing in the JSON code."),
            (lambda jc: "reference_tables" in jc, "The 'reference_tables' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["reference_tables"], dict), "The value of 'reference_tables' must be a dictionary."),
        ]
        
        for check, error_message in checks:
            if not check(summary):
                raise ValueError(f"Validation failed: {error_message}")
            
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"explanation": "Fail to run", "reference_tables": {}}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        self.progress.value += 1
        
        _, tables, story, _ = extract_output
        query_widget = self.input_item["query_widget"]
        
        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        print("🤓 Reference tables store codes with non-business concepts (e.g., Time/Location/Category).")
        print("😊 Please verify the reference tables:")
        
        data = {
            'Table': [],
            "Is Reference Table?": [],
            'Explanation': [],
        }
        
        for table in tables:
            data['Table'].append(table)
            data['Is Reference Table?'].append(table in summary["reference_tables"])
            data['Explanation'].append(summary["reference_tables"].get(table, ""))
            
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset)
        display(grid)
        
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, reset=reset)
                document = new_df.to_json(orient="split")

                callback(document)
        
        next_button.on_click(on_button_clicked)

        display(next_button)


class SynthesizeLinks(Node):
    default_name = 'Synthesize Links'
    default_description = 'This step synthesizes the links between tables.'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item

        print("🤓 Synthesizing the links between tables...")
        
        self.progress = show_progress(1)
        
        data_project = self.para["data_project"]
        description = data_project.describe_project()
        tables = data_project.list_tables()
        hub_document = self.get_sibling_document('Decide Hubs')
        hub_df = pd.read_json(hub_document, orient="split")
        hub_desc = hub_df.apply(lambda x: f"{x['Hub']}: {x['Description']}", axis=1).tolist()
        hub_desc = "\n".join(hub_desc)
        
        links_document = self.get_sibling_document('Decide Table Hub Relation For All')
        links_df = pd.read_json(links_document, orient="split")
        
        links_df = links_df[links_df["Related Hubs"].apply(lambda x: len(x) > 1)]
        
        return description, tables, hub_desc, links_df
    
    def run(self, extract_output, use_cache=True):
        description, tables, hub_desc, links_df = extract_output
        
        template = f"""You have a list of tables:
{description}

And the hubs: {hub_desc}

The following tables link multiple hubs:
{links_df.to_csv(index=False)}

Identify if there are multiple redundant tables (links) for the similar purpose (clarified with SVO).

Return in yml:
```yml
explanation: >
    The links are ... They serve the similar purpose because they both record ...
similar_links:
    Customer buys Product: # simple SVO that mentioedn related all hubs
        tables: [table1, table2, ...] # tables are link these hubs based on the purpose
    ...
```"""
        
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "explanation" in jc, "The 'explanation' key is missing in the JSON code."),
            (lambda jc: "similar_links" in jc, "The 'similar_links' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["similar_links"], dict), "The value of 'similar_links' must be a dictionary."),
        ]
        
        for check, error_message in checks:
            if not check(summary):
                raise ValueError(f"Validation failed: {error_message}")
            
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"explanation": "Fail to run", "similar_links": {}}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        self.progress.value += 1
        
        _, tables, _, links_df = extract_output
        query_widget = self.input_item["query_widget"]
        
        dropdown =  create_explore_button(query_widget, table_name=tables)
        
        data = {
            'Purpose': [],
            'Tables': [],
            'Could be Unioned?': [],
            'Explanation': [],
        }
        
        for purpose, details in summary["similar_links"].items():
            data['Purpose'].append(purpose)
            data['Tables'].append(",".join(details["tables"]))
            data['Could be Unioned?'].append(True)
            data['Explanation'].append("")
            
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        print("🤔 We have found tables that link hubs in a similar way. ")
        print("😊 Please verify if these can be unioned, and explain why:")
        
        editable_columns = [False, False, True, True]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset)
        display(grid)
        
        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid, reset=reset)
                document = new_df.to_json(orient="split")

                callback(document)
        
        next_button.on_click(on_button_clicked)

        display(next_button)    
    
    

class ConstructDataVaultViz(Node):
    default_name = 'Construct Data Vault Viz'
    default_description = 'This step constructs the data vault visualization.'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        data_project = self.para["data_project"]
        tables = data_project.list_tables()
        
        hub_document = self.get_sibling_document('Decide Hubs')
        hub_df = pd.read_json(hub_document, orient="split")
        hubs = hub_df["Hub"].tolist()
        
        table_hub_relation = self.get_sibling_document('Decide Table Hub Relation For All')
        table_hub_relation_df = pd.read_json(table_hub_relation, orient="split")
        
        
        synthethized_links = self.get_sibling_document('Synthesize Links')
        synthethized_links_df = pd.read_json(synthethized_links, orient="split")
        
        
        synthethized_links_df["Related Hubs"] = None
        
        to_remove_tables = []
        for idx, row in synthethized_links_df.iterrows():
            tables = row["Tables"].split(",")
            related_hubs = set()
            
            for table in tables:
                related_hubs.update(table_hub_relation_df[table_hub_relation_df["Table"] == table]["Related Hubs"].values[0])
                to_remove_tables.append(table)
            
            synthethized_links_df.at[idx, "Related Hubs"] = list(related_hubs)
            
        table_hub_relation_df = table_hub_relation_df[~table_hub_relation_df["Table"].isin(to_remove_tables)]
        



        
        links = []
        satellites = []
        hub_link_edges = {}
        link_satellite_edges = {}
        hub_satellite_edges = {}

        for idx, row in table_hub_relation_df.iterrows():


            related_hubs = row["Related Hubs"]
            table = row["Table"]
            explanation = row["Explanation"]
            
            if len(related_hubs) == 0:
                continue

            if len(related_hubs) > 1:
                
                if explanation not in links:
                    links.append(explanation)
                    link_satellite_edges[links.index(explanation)] = []
                    hub_link_edges[links.index(explanation)] = []

                hubs_idx = [hubs.index(hub) for hub in related_hubs]

                if table not in satellites:
                    satellites.append(table)

                link_satellite_edges[links.index(explanation)].append(satellites.index(table))

                hub_link_edges[links.index(explanation)].extend(hubs_idx)
            
            else:
                hub = related_hubs[0]

                if table not in satellites:
                    satellites.append(table)
                    
                hub_idx = hubs.index(hub)
                
                if hub_idx not in hub_satellite_edges:
                    hub_satellite_edges[hub_idx] = []

                hub_satellite_edges[hubs.index(hub)].append(satellites.index(table))
        
        for idx, row in synthethized_links_df.iterrows():
            purpose = row["Purpose"]
            tables = row["Tables"].split(",")

            if purpose not in links:
                links.append(purpose)
                link_satellite_edges[links.index(purpose)] = []
                hub_link_edges[links.index(purpose)] = []

            hubs_idx = [hubs.index(hub) for hub in row["Related Hubs"]]

            for table in tables:
                if table not in satellites:
                    satellites.append(table)

                link_satellite_edges[links.index(purpose)].append(satellites.index(table))

            hub_link_edges[links.index(purpose)].extend(hubs_idx)


        print("🎉 We have designed the data vault. Please verify the visualization:")
        
        
        hubs = [f"Hub: {hub}" for hub in hubs]
        
        links = [f"Link: {link}" for link in links]

        def create_graph(show_satellites):
            if show_satellites:
                html_content = create_graph_data(hubs, links, satellites, hub_link_edges, link_satellite_edges, hub_satellite_edges)
            else:
                html_content = create_graph_data(hubs, links, [], hub_link_edges, {}, {})
            
            return html_content

        checkbox = widgets.Checkbox(
            value=False,
            description='Show Satellites',
            disabled=False,
            indent=False
        )

        graph_widget = widgets.HTML()

        def on_checkbox_change(change):
            html_content = create_graph(change['new'])
            graph_widget.value = html_content

        checkbox.observe(on_checkbox_change, names='value')

        display(checkbox)
        display(graph_widget)

        on_checkbox_change({'new': checkbox.value})
        


class DecideFunctionalDependencyForAll(MultipleNode):
    default_name = 'Decide If Functional Dependency For All'
    default_description = 'This node allows users to decide if there is a functional dependency between two columns.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        node = DecideFunctionalDependency(para=para, id_para ="column_name")
        node.inherit(self)
        return node

    def extract(self, item):
        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        
        schema = table_pipeline.get_schema(con)
        table_name = table_pipeline.get_final_step().name
        all_columns = list(schema.keys())
        
        with_context = table_pipeline.get_codes(mode="WITH")
        sql_query = generate_count_distinct_query(table_name, all_columns, con)
        sql_query = with_context + "\n" + sql_query
        distinct_count_df = run_sql_return_df(con, sql_query)

        distinct_count_df = distinct_count_df.loc[:, (distinct_count_df.iloc[0] < 0.1) & (distinct_count_df.iloc[0] > 0)]
        columns = distinct_count_df.columns
        
        self.elements = []
        self.nodes = {}

        idx = 0
        for col in columns:
            self.elements.append(col)
            self.nodes[col] = self.construct_node(col, idx, len(columns))
            idx += 1
            
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        callback(document)
        
        
                
        
        
        

        
        
        
        
        
            
        
        
    

class DecideFunctionalDependency(Node):
    default_name = 'Decide If Functional Dependency'
    default_description = 'This node allows users to decide if there is a functional dependency between two columns.'

    def extract(self, input_item):
        clear_output(wait=True)

        idx = self.para["column_idx"]
        total = self.para["total_columns"]

        self.input_item = input_item

        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.get_final_step().name
        column_name = self.para["column_name"]
        schema = table_pipeline.get_schema(con)
        all_columns = list(schema.keys())
        table_summary = self.get_multi_node_parent_sibling_document("Create Short Table Summary")
        
        display(HTML(f"{running_spinner_html} Detecting functional dependencies for <i>{column_name}</i>..."))
        show_progress(max_value=total, value=idx)

        decide_column_candidates = []
        decide_columns = [c for c in all_columns if c != column_name]

        if len(decide_columns) > 0:
            sql_query = generate_group_ratio_query(table_name, column_name, decide_columns)
            with_context = table_pipeline.get_codes(mode="WITH")
            sql_query = with_context + "\n" + sql_query
            ratio_df = run_sql_return_df(con, sql_query)
            print(ratio_df)
            
            ratio_df = ratio_df.loc[:, ratio_df.iloc[0] < 0.9]
            decide_column_candidates = list(ratio_df.columns)
            
            
        table_desc = ""
        
        
        return column_name, decide_column_candidates, table_summary, table_desc


    def run(self, extract_output, use_cache=True):
        column_name, decide_column_candidates, table_summary, table_desc = extract_output
        
        if len(decide_column_candidates) == 0:
            return {"explanation": "No candidates", "one_one_columns": []}

        template = f"""You have a table: {table_summary}

Reason about whether, for each '{column_name}', there is a unique value for: '{decide_column_candidates}'.
E.g., 'country' -> 'city' is not one_one, because one country can have multiple cities.
'order_id' -> 'order_date' is one_one, because one order_id has one order_date.

Respond in yml:
```yml
explanation: >
    For each value in '{column_name}', there possibly is a unique value in ...
one_one_columns: [column1, column2, ...] 
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: "explanation" in jc, "The 'explanation' key is missing in the JSON code."),
            (lambda jc: "one_one_columns" in jc, "The 'one_one_columns' key is missing in the JSON code."),
            (lambda jc: isinstance(jc["one_one_columns"], list), "The value of 'one_one_columns' must be a list."),
        ]

        for check, error_message in checks:
            if not check(summary):
                raise ValueError(f"Validation failed: {error_message}")
        
        summary["one_one_columns"] = [col for col in summary["one_one_columns"] if col != column_name]
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"explanation": "Fail to run", "one_one_columns": []}

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        column_name, _, _, _ = extract_output
        
        
        print("Explanation", summary["explanation"])
        print("FD Columns", summary["one_one_columns"])
        groupby = column_name
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)
        all_columns = list(schema.keys())
        
        
        repair_selection_clauses = {}
        
        summary["repair_dfs"] = {}
        
        for decide_column in summary["one_one_columns"]:
            if decide_column not in all_columns:
                continue
            
            def create_repair_pair_sql(table_name, groupby, decide_col):
                sql = f"""SELECT {groupby}, mode({decide_col}) AS {decide_col}
FROM {table_name}
WHERE {groupby} IS NOT NULL
GROUP BY {groupby}
HAVING count(distinct(coalesce({decide_col}, 'NULL'))) > 1"""

                return sql
            
            with_context = table_pipeline.get_codes(mode="WITH")
            repair_pair_sql = create_repair_pair_sql(table_name, groupby, decide_column)
            repair_pair_sql = with_context + "\n" + repair_pair_sql
            repair_df = run_sql_return_df(con, repair_pair_sql)
            
            if len(repair_df) == 0:
                continue
            
            summary["repair_dfs"][decide_column] = repair_df.to_json(orient="split")
            
            def create_repair_selection_str(df, groupby, decide_col):
                selection_str = "CASE\n"
                for i, row in df.iterrows():
                    def escape_value(value):
                        if isinstance(value, (int, float)):
                            return value
                        elif isinstance(value, str):
                            return escape_value_single_quotes(value, con)
                        else:
                            return str(value)

                    old_value = escape_value(row[groupby])
                    new_value = escape_value(row[decide_col])

                    if isinstance(old_value, (int, float)) and isinstance(new_value, (int, float)):
                        selection_str += f"    WHEN {groupby} = {old_value} THEN {new_value}\n"
                    else:
                        selection_str += f"    WHEN {groupby} = '{old_value}' THEN '{new_value}'\n"
                
                selection_str += f"    ELSE {decide_col}\n"
                selection_str += "END AS " + decide_col

                return selection_str

            repair_selection_clause = create_repair_selection_str(repair_df, groupby, decide_column)
            repair_selection_clauses[decide_column] = repair_selection_clause
            
        if len(repair_selection_clauses) == 0:
            callback(summary)
            return
        
        selections = []

        old_table_name = table_pipeline.__repr__(full=False)
        new_table_name = old_table_name + "_fd"
        
        for col in all_columns:
            if col in repair_selection_clauses:
                selections.append(repair_selection_clauses[col])
            else:
                selections.append(col)
        selection_sql = indent_paragraph(",\n".join(selections))
        
        sql_query = f"""SELECT
{selection_sql}"""    

        comment = f"-- FD: functional dependency\n"
        comment += f"-- {groupby} -> {', '.join(summary['one_one_columns'])}\n"
        
        sql_query = comment + sql_query
        sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"
        step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=self.item["con"])
        table_pipeline.add_step_to_final(step)
        
        submit_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                callback(summary)
            
        submit_button.on_click(on_button_clicked)
        
        display(submit_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(submit_button) 
            return
        
        
class SourceProjectStoryUnderstanding(Node):
    default_name = 'Source Project Understand'
    default_description = 'This understands the source tables and dbt project'

    def extract(self, item):
        clear_output(wait=True)

        create_progress_bar_with_numbers(0, model_steps)
        display(HTML(f"{running_spinner_html} Generating the story of the project ..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]

        description = data_project.describe_project()
        
        table_summary_document = self.get_sibling_document('Stage Tables For All')

        tables = data_project.list_tables()

        return description, tables

    def run(self, extract_output, use_cache=True):
        description, tables = extract_output
        template = f"""You have tables:
{description}

First, identify the core concepts, which appear frequently in the table names or columns. Give a short descriptio  (~5 words).
Then, desribe the story connects these concepts, in a short sequence (1-10) of steps, each as simple SVO sentences (~5 words). 

Focus on the big picture. It's fine to skip some unimportant tables.
If there are synomyms, keep them in the brackets. E.g., Customers (Users) buy Items (Products)
Please disregard the system side of the tables (DONT: data are collected, stored in s3, used for analysis)/

Return in the following format:
```json
{{
    "reasoning": "The tables are ... The core concepts are ...",
    "core_concepts": {{
        "Customers (Users)": "Individuals need the service",
        ... 
    }}
    "story": [ 
        "Customers (Users) buy Items (Products)",
        "Customers (Users) returns Items (Products)",
        ...
    ]
}}
```"""
        
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"story": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        
        
        



        def on_button_click(b):
            with self.output_context():
                

                data_project = self.para["data_project"]
                data_project.set_story(summary["story"])
                callback(summary)

        on_button_click(None) 
        

class SourceTableUnderstand(Node):
    default_name = 'Source Table Understand'
    default_description = 'This understands the source tables'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        create_progress_bar_with_numbers(0, model_steps)

        display(HTML(f"{running_spinner_html} Understanding the database ..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]

        table_description = data_project.describe_project()
        story = data_project.get_story()
        story_description = "\n".join([f"{idx+1}. {story}" for idx, story in enumerate(story)])
        tables = data_project.list_tables()
        
        
        return tables, story_description, table_description,

    def run(self, extract_output, use_cache=True):
        tables, story_description, table_description = extract_output

        template = f"""You have a list of tables:
{table_description}

The tables are for the following story:
{story_description}

Now, for each table, please first describe what it is.
Then decide if its relation to table is clear. If so, how?
Relation is a short sentence < 10 words.

Return in the following format:
```json
{{
    "{tables[0]}: {{
        "description": "This table stores user address, where user is likely the customer",
        "relation_clear": true/false,
        "relation": "Customer address for delivery" # if relation_clear is true
    }}
    ...
}}```"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1)
        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        summary = json.loads(json_code)

        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        return summary, tables, story_description
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):

        summary, tables, story_description = run_output
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        self.progress.value += 1

        dropdown = widgets.Dropdown(
            options=tables,
            disabled=False,
        )

        explore_button = widgets.Button(
            description='Explore',
            disabled=False,
            button_style='info',
            tooltip='Click to explore',
            icon='search'
        )


        query_widget = self.item["query_widget"]

        def on_explore_button_clicked(b):
            with self.output_context():
                selected_table = dropdown.value

                print("😎 Query submitted. Check out the data widget!")
                query_widget.run_query(f'SELECT * FROM "{selected_table}"')

        explore_button.on_click(on_explore_button_clicked)

        display(HBox([dropdown, explore_button]))
        
        data_project = self.para["data_project"]
        story = data_project.get_story()
        
        print("🎉 Based on the table, we've generated the project story:")
        
        text_area, char_count_label = create_text_area_with_char_count("\n".join(story), max_chars=500)
        layout = Layout(display='flex', justify_content='space-between', width='100%')
        display(HBox([text_area, char_count_label], layout=layout))

        print("🎉 Here are the table descriptions, and how they fit in the story:")

        table_desc = summary


        
        data = {
            'Table': [],
            'Description': [],
        }
        
        for table in table_desc:
            data['Table'].append(table)
            data['Description'].append(table_desc[table]["description"])
            
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True]
        reset = True
        long_text = []
        editable_list = {
        }
        
                
        grid = create_dataframe_grid(df, editable_columns, reset=reset, long_text=long_text, editable_list=editable_list)
        display(grid)

        print("🤓 Please edit above, but keep it short and simple!")

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                summary = {}
                
                current_strings_displayed = text_area.value
                current_strings_displayed = current_strings_displayed.split("\n")
                
                if len(current_strings_displayed) == 0:
                    print("⚠️ Please provide a story with at least one line")
                    return 

                summary["story"] = current_strings_displayed
                
                data_project = self.para["data_project"]
                data_project.set_story(summary["story"])
                
                df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                document = df.to_json(orient="split")
                summary["table_desc"] = document
                
                callback(summary)

        next_button.on_click(on_button_click)

        display(next_button)
        

class Table:
    def __init__(self, table_name="", table_summary="", columns=None, column_desc=None, missing_reason=None, unusualness=None, 
                 category=None, uniqueness=None, patterns=None, variant=None, potential_category = None, data_type=None):
        self.table_name = table_name
        self.table_summary = table_summary
        self.columns = columns or []
        self.column_desc = column_desc or OrderedDict()
        self.missing_reason = missing_reason or {}
        self.data_type = data_type or {}
        self.unusualness = unusualness or {}
        self.category = category or {}
        self.potential_category = potential_category or {}
        self.uniqueness = uniqueness or {}
        self.patterns = patterns or {}
        self.variant = variant or {}
        self.true_expressions = []
        self.pii = []
        
    def deep_copy(self):
        return copy.deepcopy(self)
    
    def get_alerts(self, column):
        alerts = []
        
        if column in self.missing_reason:
            alert_name = f"❓ {self.table_name}[{column}] has Missing Value"
            if self.missing_reason[column]:
                explanation = f"{self.missing_reason[column]}"
            else:
                explanation = "Unknown"
            alerts.append((alert_name, explanation))
        
        if column in self.unusualness and self.unusualness[column]:
            alert_name = f"⚠️ {self.table_name}[{column}] has Unusual Value"
            explanation = self.unusualness[column]
            alerts.append((alert_name, explanation))
        
        return alerts
    
    def print_category(self):
        category_string = ""
        for column, categories in self.category.items():
            category_string += f"'{column}': {categories}\n"
        return category_string.strip()
    
    def get_column_desc_yml(self, columns=None, show_category=False, 
                        show_unique=False, show_pattern=False):
        result = []
        if columns is None:
            columns = self.columns
        
        for column in columns:
            if column in self.column_desc:
                column_info = OrderedDict()
                column_info['name'] = column
                column_info['description'] = self.column_desc.get(column, "")
                
                if show_category and column in self.category:
                    column_info['domain'] = str(self.category[column])
                
                if show_unique and column in self.uniqueness:
                    column_info['unique'] = self.uniqueness[column]
                
                if show_pattern and column in self.patterns:
                    column_info['patterns'] = [
                        OrderedDict([
                            ('summary', pattern['summary']),
                            ('regex', pattern['regex'])
                        ])
                        for pattern in self.patterns[column]
                    ]
                
                result.append(column_info)
        
        return result

    def print_column_desc(self, columns=None, show_category=False, 
                        show_unique=False, show_pattern=False,
                        show_type=False, show_table_summary=False):
        column_desc_string = ""
        
        if show_table_summary and self.table_summary:
            column_desc_string += f"{self.table_summary}\n"
        
        if columns is None:
            columns = sorted(self.columns)
        else:
            columns = sorted(col for col in columns if col in self.column_desc)
        
        for i, column in enumerate(columns, 1):
            if column in self.column_desc:
                desc = f"- '{column}': {self.column_desc.get(column, '')}\n"
                
                if show_category and column in self.category:
                    categories = self.category[column]
                    desc += f"   Column domain: {categories}\n"
                
                if show_unique and column in self.uniqueness:
                    current_unique = self.uniqueness[column].get("current_unique", False)
                    unique_reason = self.uniqueness[column].get("unique_reason", None)
                    if current_unique:
                        desc += "   Column is already unique\n"
                    else:
                        desc += "   Column is not unique\n"
                        
                    if unique_reason:
                        desc += f"   It should be unique because: {unique_reason}\n"
                
                if show_pattern and column in self.patterns:
                    desc += "   All column values already follow regex patterns:\n"
                    for pattern in self.patterns[column]:
                        desc += f"      - {pattern['summary']}: {pattern['regex']}\n"
                
                if show_type and column in self.data_type:
                    data_type = self.data_type[column]["current_data_type"]
                    desc += f"   Current type: {data_type}\n"
                
                column_desc_string += desc
        
        return column_desc_string.strip()
        
    def create_dbt_schema_dict(self, table_cocoon_meta=None):
        data = OrderedDict([
            ("version", 2),
            ("models", [
                OrderedDict([
                    ("name", self.table_name),
                    ("description", self.table_summary),
                    ("columns", []),
                    ("tests", []) 
                ])
            ]),
        ])
        
        if table_cocoon_meta is not None:
            data["cocoon_meta"] = table_cocoon_meta
            

        for expression in self.true_expressions:
            test = OrderedDict([
                ("dbt_utils.expression_is_true", OrderedDict([
                    ("test_name", expression["test_name"]),
                    ("test_description", expression["explanation"]),
                    ("expression", expression["where_clause"])
                ]))
            ])
            data["models"][0]["tests"].append(test)
        
        for column in self.columns:
            columndesc_key = next((key for key in self.column_desc if key.lower() == column.lower()), None)
            description = self.column_desc[columndesc_key] if columndesc_key else ""
            tests = []
            cocoon_meta = OrderedDict()

            missing_reason_key = next((key for key in self.missing_reason if key.lower() == column.lower()), None)
            if missing_reason_key:
                explanation = self.missing_reason[missing_reason_key]
                if not explanation:
                    tests.append("not_null")
                cocoon_meta["missing_reason"] = explanation
            else:
                tests.append("not_null")

            unusualness_key = next((key for key in self.unusualness if key.lower() == column.lower()), None)
            if unusualness_key:
                unusual_reason = self.unusualness[unusualness_key]
                if unusual_reason:
                    cocoon_meta["unusual_values"] = unusual_reason

            uniqueness_key = next((key for key in self.uniqueness if key.lower() == column.lower()), None)
            if uniqueness_key:
                current_unique = self.uniqueness[column].get("current_unique", False)
                unique_reason = self.uniqueness[column].get("unique_reason", None)
                if current_unique and unique_reason:
                    tests.append("unique")
                
                if unique_reason:
                    cocoon_meta["unique_reason"] = unique_reason

            category_key = next((key for key in self.category if key.lower() == column.lower()), None)
            if category_key:
                accepted_values = self.category[category_key]
                if accepted_values:
                    if not isinstance(accepted_values, list):
                        if accepted_values.startswith("[") and accepted_values.endswith("]"):
                            accepted_values = ast.literal_eval(accepted_values)
                        else:
                            accepted_values = accepted_values.split(",")
                    tests.append(OrderedDict([("accepted_values", OrderedDict([("values", accepted_values)]))]))

            patterns_key = next((key for key in self.patterns if key.lower() == column.lower()), None)
            if patterns_key:
                patterns = self.patterns[patterns_key]
                if patterns:
                    cocoon_meta["patterns"] = patterns
                    
            variant_key = next((key for key in self.variant if key.lower() == column.lower()), None)
            if variant_key:
                variant_info = self.variant[variant_key]
                if variant_info:
                    cocoon_meta["variant"] = variant_info
                    
            potential_category_key = next((key for key in self.potential_category if key.lower() == column.lower()), None)
            if potential_category_key:
                potential_values = self.potential_category[potential_category_key]
                if potential_values:
                    if not isinstance(potential_values, list):
                        if potential_values.startswith("[") and potential_values.endswith("]"):
                            potential_values = ast.literal_eval(potential_values)
                        else:
                            potential_values = potential_values.split(",")
                    cocoon_meta["future_accepted_values"] = potential_values

            data_type_key = next((key for key in self.data_type if key.lower() == column.lower()), None)
            if data_type_key:
                cocoon_meta["data_type"] = self.data_type[data_type_key]

            if any(pii_column.lower() == column.lower() for pii_column in self.pii):
                cocoon_meta["contains_pii"] = True
            
            column_data = OrderedDict([
                ("name", column),
                ("description", description)
            ])

            if tests:
                column_data["tests"] = tests

            if cocoon_meta:
                column_data["cocoon_meta"] = cocoon_meta

            data["models"][0]["columns"].append(column_data)

        return data
    
    def create_dbt_schema_yml(self):
        data = self.create_dbt_schema_dict()
        yml_content = yaml.dump(data, default_flow_style=False)
        return yml_content
    
    def read_attributes_from_dbt_schema_yml(self, yml_data):
        if isinstance(yml_data, str):
            yml_data = yaml.safe_load(yml_data)

        for model in yml_data.get('models', []):
            self.table_name = model.get('name', '')
            self.table_summary = model.get('description', '')

            for test in model.get('tests', []):
                if isinstance(test, dict) and 'dbt_utils.expression_is_true' in test:
                    expression = test['dbt_utils.expression_is_true']
                    self.true_expressions.append({
                        "test_name": expression.get('test_name', ''),
                        "explanation": expression.get('test_description', ''),
                        "where_clause": expression.get('expression', '')
                    })
                    
            for column in model.get('columns', []):
                column_name = column.get('name', '')
                self.columns.append(column_name)
                self.column_desc[column_name] = column.get('description', '')

                tests = column.get('tests', [])
                for test in tests:
                    if isinstance(test, dict):
                        test_type = list(test.keys())[0]
                        test_value = test[test_type]
                        if test_type == 'accepted_values':
                            accepted_values = test_value['values']
                            self.category[column_name] = accepted_values
                        elif test_type == 'unique':
                            if column_name not in self.uniqueness:
                                self.uniqueness[column_name] = {}
                            self.uniqueness[column_name]["current_unique"] = True
                    elif test == 'unique':
                        if column_name not in self.uniqueness:
                            self.uniqueness[column_name] = {}
                        self.uniqueness[column_name]["current_unique"] = True
                
                cocoon_meta = column.get('cocoon_meta', {})
                if 'missing_reason' in cocoon_meta:
                    self.missing_reason[column_name] = cocoon_meta['missing_reason']
                if 'unusual_values' in cocoon_meta:
                    self.unusualness[column_name] = cocoon_meta['unusual_values']
                if 'uniqueness' in cocoon_meta:
                    if column_name not in self.uniqueness:
                        self.uniqueness[column_name] = {}
                    self.uniqueness[column_name]["unique_reason"] = cocoon_meta['uniqueness']
                if 'unique_reason' in cocoon_meta:
                    if column_name not in self.uniqueness:
                        self.uniqueness[column_name] = {}
                    self.uniqueness[column_name]["unique_reason"] = cocoon_meta['unique_reason']
                if 'patterns' in cocoon_meta:
                    self.patterns[column_name] = cocoon_meta['patterns']
                if 'variant' in cocoon_meta:
                    self.variant[column_name] = cocoon_meta['variant']
                if 'future_accepted_values' in cocoon_meta:
                    self.potential_category[column_name] = cocoon_meta['future_accepted_values']
                if "data_type" in cocoon_meta:
                    self.data_type[column_name] = cocoon_meta["data_type"]
                if "contains_pii" in cocoon_meta and cocoon_meta["contains_pii"]:
                    self.pii.append(column_name)

                    
    def add_column(self, column_name, description=None, missing_desc=None, unusual_desc=None, categories=None):
        if column_name not in self.columns:
            self.columns.append(column_name)
        if description is not None:
            self.column_desc[column_name] = description
        if missing_desc is not None:
            self.missing_reason[column_name] = missing_desc
        if unusual_desc is not None:
            self.unusualness[column_name] = unusual_desc
        if categories is not None:
            self.category[column_name] = categories

    def remove_column(self, column_name):
        if column_name in self.columns:
            self.columns.remove(column_name)
        self.column_desc.pop(column_name, None)
        self.missing_reason.pop(column_name, None)
        self.unusualness.pop(column_name, None)
        self.category.pop(column_name, None)

    def update_column(self, column_name, description=None, missing_desc=None, unusual_desc=None, categories=None):
        if column_name in self.columns:
            if description is not None:
                self.column_desc[column_name] = description
            if missing_desc is not None:
                self.missing_reason[column_name] = missing_desc
            if unusual_desc is not None:
                self.unusualness[column_name] = unusual_desc
            if categories is not None:
                self.category[column_name] = categories

    def __repr__(self):
        return f"Table(table_summary={self.table_summary}, columns={self.columns})"

class PartitionTable(Table):
    def __init__(self, table_name="", table_summary="", columns=None, column_desc=None, missing_reason=None, unusualness=None, 
                 category=None, uniqueness=None, patterns=None, variant=None, potential_category=None, data_type=None):
        super().__init__(table_name, table_summary, columns, column_desc, missing_reason, unusualness, 
                         category, uniqueness, patterns, variant, potential_category, data_type)
        self.partitions = []
        
    def create_dbt_schema_dict(self, table_cocoon_meta=None):
        combined_cocoon_meta = OrderedDict()

        if self.partitions:
            combined_cocoon_meta["partitions"] = self.partitions

        if table_cocoon_meta:
            combined_cocoon_meta.update(table_cocoon_meta)

        data = super().create_dbt_schema_dict(table_cocoon_meta=combined_cocoon_meta)

        return data
    
    def create_table_copy(self):
        table_copy = Table(
            table_name=self.table_name,
            table_summary=self.table_summary,
            columns=copy.deepcopy(self.columns),
            column_desc=copy.deepcopy(self.column_desc),
            missing_reason=copy.deepcopy(self.missing_reason),
            unusualness=copy.deepcopy(self.unusualness),
            category=copy.deepcopy(self.category),
            uniqueness=copy.deepcopy(self.uniqueness),
            patterns=copy.deepcopy(self.patterns),
            variant=copy.deepcopy(self.variant),
            potential_category=copy.deepcopy(self.potential_category),
            data_type=copy.deepcopy(self.data_type)
        )
        return table_copy
    
    def read_attributes_from_dbt_schema_yml(self, yml_data):
        super().read_attributes_from_dbt_schema_yml(yml_data)

        if isinstance(yml_data, str):
            yml_data = yaml.safe_load(yml_data)

        cocoon_meta = yml_data.get('cocoon_meta', {})
        if 'partitions' in cocoon_meta:
            self.partitions = cocoon_meta['partitions']

        return self
    

class DescribeColumnsListAndTableSummaryNoRename(DescribeColumnsList):
    default_name = 'Describe Columns'
    default_description = 'This node allows users to describe the columns of a table.'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        
        self.progress.value += 1

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)
        
        table_summary = self.get_sibling_document('Create Short Table Summary')
        
        text_area, char_count_label = create_text_area_with_char_count(table_summary, max_chars=500)
        layout = Layout(display='flex', justify_content='space-between', width='100%')

        rows_list = []

        for col in schema:
            rows_list.append({
                "Column": col,
                "Summary": json_code[col][0]
            })

        df = pd.DataFrame(rows_list)
        
        editable_columns = [False, True]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        create_progress_bar_with_numbers(0, doc_steps)

        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  
        
        
        def on_button_clicked2(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                new_df["New Column Name"] = new_df["Column"]
                document = new_df.to_json(orient="split")
                get_column_desc_from_df(new_df)
                get_table_summary_from_text()
                callback(document)
            

        def get_column_desc_from_df(df):
            table_object = self.para["table_object"]
            column_desc = {}
            for index, row in df.iterrows():
                table_object.column_desc[row["New Column Name"]] = row["Summary"]
                
        def get_table_summary_from_text():
            table_object = self.para["table_object"]
            table_object.table_summary = text_area.value

        next_button.on_click(on_button_clicked2)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked2(next_button)
            return
        
        display(HTML(f"📝 Here is the table summary. Please keep it concise:"))
        display(HBox([text_area, char_count_label], layout=layout))
        display(HTML(f"📝 Here are the summary for each column."))
        display(grid)
        display(next_button)
    
class DescribeColumnsListAndTableSummary(DescribeColumnsList):
    default_name = 'Describe Columns'
    default_description = 'This node allows users to describe the columns of a table.'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        
        self.progress.value += 1

        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, table_pipeline)
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)
        
        table_summary = self.get_sibling_document('Create Short Table Summary')
        
        text_area, char_count_label = create_text_area_with_char_count(table_summary, max_chars=500)
        layout = Layout(display='flex', justify_content='space-between', width='100%')

        rows_list = []

        for col in schema:
            rows_list.append({
                "Column": col,
                "Summary": json_code[col][0],
                "New Column Name": json_code[col][1],
                "Renamed?": "✔️ Yes" if json_code[col][1] != col else "❌ No"
            })

        df = pd.DataFrame(rows_list)
        
        editable_columns = [False, True, True, False]
        grid = create_dataframe_grid(df, editable_columns, reset=True)
        create_progress_bar_with_numbers(0, doc_steps)

        next_button = widgets.Button(
            description='Accept Rename',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  
        
        reject_button = widgets.Button(
            description='Reject Rename',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
        
        def on_button_clicked2(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                new_df["New Column Name"] = new_df["Column"]
                document = new_df.to_json(orient="split")
                get_column_desc_from_df(new_df)
                get_table_summary_from_text()
                callback(document)
                

        def get_column_desc_from_df(df):
            table_object = self.para["table_object"]
            column_desc = {}
            for index, row in df.iterrows():
                table_object.column_desc[row["New Column Name"]] = row["Summary"]
                
        def get_table_summary_from_text():
            table_object = self.para["table_object"]
            table_object.table_summary = text_area.value

        
        def on_button_clicked(b):
            with self.output_context():
                new_df =  grid_to_updated_dataframe(grid)
                
                new_df["New Column Name"] = new_df["New Column Name"].apply(clean_column_name)
                
                get_column_desc_from_df(new_df)
                get_table_summary_from_text()
                
                renamed = (new_df["Column"] != new_df["New Column Name"]).any()
                
                if renamed:
                    if new_df["New Column Name"].duplicated().any():
                        print(f"⚠️ Please provide unique names for the columns. The following columns have duplicated names:")
                        duplicate_names = new_df[new_df["New Column Name"].duplicated()]
                        print(", ".join(duplicate_names["New Column Name"].tolist()))
                        return

                    old_table_name = table_pipeline.__repr__(full=False)
                    new_table_name = old_table_name + "_renamed"
                    comment = f"-- Rename: Renaming columns\n"
                    selection_clauses = []
                    
                    for index, row in new_df.iterrows():
                        old_name = row["Column"]
                        new_name = row["New Column Name"]
                        
                        if old_name != new_name:
                            comment += f"-- {old_name} -> {new_name}\n"
                            selection_clauses.append(f"\"{old_name}\" AS \"{new_name}\"")
                        else:
                            selection_clauses.append(f"\"{old_name}\"")
                    
                    selection_clause = ',\n'.join(selection_clauses)
                    sql_query = f'SELECT \n{indent_paragraph(selection_clause)}'
                    sql_query = comment + sql_query
                    sql_query = lambda *args, sql_query=sql_query: f"{sql_query}\nFROM {args[0]}"
                    con = self.item["con"]
                    step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con)
                    step.run_codes()
                    table_pipeline.add_step_to_final(step)
                
                document = new_df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)

        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
        display(HTML(f"📝 Here is the table summary. Please keep it concise:"))
        display(HBox([text_area, char_count_label], layout=layout))
        display(HTML(f"📝 Here are the summary for each column."))
        display(grid)
        display(HBox([reject_button, next_button]))
        
class CreateShortTableSummaryContinue(CreateShortTableSummary):
    default_name = 'Create Short Table Summary'
    default_description = 'This node creates a short summary of the table.'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output 
        self.progress.value += 1
        
        callback(summary)
        

class StageProgress(Node):
    default_name = 'Stage Progress'
    default_description = 'This shows the progress of staging'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        create_progress_bar_with_numbers(1, model_steps)

        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        table_name = self.para['table_name']
        idx = self.para["table_idx"]
        all_tables = self.para["all_tables"]


        df = pd.DataFrame(all_tables, columns=["table"])

        df["status"] = "🕒"
        df.loc[:idx, "status"] = "✅"
        df.loc[idx, "status"] = "🚀"


        next_button = widgets.Button(description="Next",
                                    button_style='success')
        
        html_mode_content = """<h3>Two modes to proceed: </h3>
<ul>
<li><b>💬 Interactive</b>: Interactively provide your feedback for each step.</li>
<li><b>⚡ Express</b>: Sit back, relax, and let us provide our best guess.</li>
</ul>
"""


        
        interactive_next_button = widgets.Button(description="Interactive Next", 
                                     button_style='success',
                                     icon='check')

        express_next_button = widgets.Button(description="Express Next",
                                        button_style='info',
                                        icon='bolt')
        
        express_all_button = widgets.Button(description="Express All",
                                        button_style='warning',
                                        icon='fast-forward')
        
        
        def go_to_next_node():
            callback({})
            
        def skip_this_workflow():
            table_name = self.para["table_name"]
            new_table_name = rename_for_stg(table_name)
            
            stg_file_names = [f"{new_table_name}.sql", f"{new_table_name}.yml"]
            original_file_names = [f"{table_name}.yml"]
            
            if "dbt_directory" in self.para:
                stg_file_names = [os.path.join(self.para["dbt_directory"], "stage", file_name) for file_name in stg_file_names]
                original_file_names = [os.path.join(self.para["dbt_directory"], "stage", file_name) for file_name in original_file_names]
            
            stg_files_exist = all(file_exists(file_name) for file_name in stg_file_names)
            original_yml_exists = file_exists(original_file_names[0])
            
            if stg_files_exist or original_yml_exists:
                con = self.item["con"]
                database = self.para.get("database", None)
                schema = self.para.get("schema", None)
                
                source_step = SQLStep(table_name=table_name, con=con, database=database, schema=schema)
                table_pipeline = TransformationSQLPipeline(steps=[source_step], edges=[])
                
                if stg_files_exist:
                    sql_query = read_from(stg_file_names[0])
                    sql_step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con, database=database, schema=schema)
                    table_pipeline.add_step_to_final(sql_step)
                    
                    yml_data = read_from(stg_file_names[1])
                else:
                    yml_data = read_from(original_file_names[0])
                
                self.para["table_pipeline"] = table_pipeline
                
                table_object = self.para["table_object"]
                table_object.read_attributes_from_dbt_schema_yml(yml_data)
                
                callback({"next_node": "COCOON_END_WORKFLOW"})
            else:
                callback({})
        
        def on_interactive_next_button_click(b):
            with self.output_context():
                go_to_next_node()
        
        def on_express_next_button_click(b):
            with self.output_context():
                self.para["viewer"] = True
                skip_this_workflow()
                
        def on_express_all_button_click(b):
            with self.output_context():
                self.para["viewer"] = True
                self.para["stage_viewer"][0] = True
                skip_this_workflow()
                
        interactive_next_button.on_click(on_interactive_next_button_click)
        express_next_button.on_click(on_express_next_button_click)
        express_all_button.on_click(on_express_all_button_click)
        
        if self.para["stage_viewer"][0]:
            self.para["viewer"] = True
            skip_this_workflow()
            return
        
        if self.para["viewer"]:
            skip_this_workflow()
            return
        
        display(HTML(f"🧐 We will clean all the tables ... The next table is <b>{table_name}</b> ..."))
        display(df)
        display(HTML(html_mode_content))
        display(HBox([interactive_next_button, express_next_button, express_all_button]))


class StageSourceTargetProgress(Node):
    default_name = 'Stage Progress'
    default_description = 'This shows the progress of staging'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        create_progress_bar_with_numbers(1, model_steps)

        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        table_name = self.para['table_name']
        idx = self.para["table_idx"]
        all_tables = self.para["all_tables"]
        clean = self.para["clean"]
        

        df = pd.DataFrame()
        
        df["type"] = ["Source", "Target"]
        df["table"] = all_tables
        
        df["status"] = "🕒"
        df.loc[:idx, "status"] = "✅"
        df.loc[idx, "status"] = "🚀"
        
        
        df["step"] = ["✨Clean" if clean[i] else "🔍Profile" for i in range(len(all_tables))]
        

        next_button = widgets.Button(description="Next",
                                    button_style='success')
        
        
        html_mode_content = """<h3>Two modes to proceed: </h3>
<ul>
<li><b>💬 Interactive</b>: Interatively provide your feedback for each step.</li>
<li><b>⚡ Express</b>: Sit back, relax, and let us provide our best guess.</li>
</ul>
"""

        
        interactive_next_button = widgets.Button(description="Interactive Next", 
                                     button_style='success',
                                     icon='check')

        express_next_button = widgets.Button(description="Express Next",
                                        button_style='info',
                                        icon='bolt')
        
        express_all_button = widgets.Button(description="Express All",
                                        button_style='warning',
                                        icon='fast-forward')
        
        def go_to_next_node():
            callback({})
            
        def skip_this_workflow():
            table_name = self.para["table_name"]
            new_table_name = rename_for_stg(table_name)
            
            file_names = [f"{new_table_name}.sql",  f"{new_table_name}.yml",]
            
            if "dbt_directory" in self.para:
                file_names = [os.path.join(self.para["dbt_directory"], "stage", file_name) for file_name in file_names]
            
            if all([file_exists(file_name) for file_name in file_names]):
                sql_query = read_from(file_names[0])
                con = self.item["con"]
                sql_step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con)
                table_pipeline = TransformationSQLPipeline(steps = [sql_step], edges=[])
                self.para["table_pipeline"] = table_pipeline
                    
                
                yml_data = read_from(file_names[1])
                table_object = self.para["table_object"]
                table_object.read_attributes_from_dbt_schema_yml(yml_data)
                    
                callback({"next_node": "COCOON_END_WORKFLOW"})
                
            else:
                callback({})
        
        def on_interactive_next_button_click(b):
            with self.output_context():
                go_to_next_node()
        
        def on_express_next_button_click(b):
            with self.output_context():
                self.para["viewer"] = True
                skip_this_workflow()
                
        def on_express_all_button_click(b):
            with self.output_context():
                self.para["viewer"] = True
                self.para["stage_viewer"][0] = True
                skip_this_workflow()
                
        interactive_next_button.on_click(on_interactive_next_button_click)
        express_next_button.on_click(on_express_next_button_click)
        express_all_button.on_click(on_express_all_button_click)
        
        
        if self.para["stage_viewer"][0]:
            self.para["viewer"] = True
            skip_this_workflow()
            return
        
        display(HTML(f"🧐 We will clean all the tables ... The next table is <b>{table_name}</b> ..."))
        display(HTML(df.to_html(index=False)))
        display(HTML(html_mode_content))
        display(HBox([interactive_next_button, express_next_button, express_all_button]))

        
class StageForAll(MultipleNode):
    
    default_name = 'Stage For All'
    default_description = 'This stages the source tables'

    def construct_node(self, element_name, all_tables=None, idx=0, total=0, stage_viewer=None):
        if all_tables is None:
            all_tables = []
        if self.item is not None:
            con = self.item.get("con", None)
            query_widget = self.item.get("query_widget", None)
        else:
            con = None
            query_widget = None
        
        table_name = element_name
        
        para = {"table_name": table_name, 
                "table_idx": idx, 
                "all_tables": all_tables,
                "table_total": total, 
                "stage_viewer": stage_viewer}
        
        for key in self.para:
            para[key] = self.para[key]
            
        table_object = None
        if "data_project" in self.para:
            data_project = self.para["data_project"]
            
            for partition_name, tables in data_project.partition_mapping.items():
                if table_name in tables:
                    table_object = data_project.table_object[partition_name].create_table_copy()
                    break
        
        _, workflow = create_cocoon_stage_workflow(con=con, query_widget=query_widget, 
                                                   table_name=table_name, para=para, output=self.output,
                                                   table_object=table_object)
        workflow.add_as_root(StageProgress())
        
        return workflow

    def extract(self, item):
        data_project = self.para["data_project"]
        tables = data_project.list_tables()
        stage_viewer = [False]
        self.elements = tables
        self.nodes = {element: self.construct_node(element, self.elements, idx, 
                                                   len(self.elements), stage_viewer=stage_viewer)
                      for idx, element in enumerate(self.elements)}

    
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        create_progress_bar_with_numbers(1, model_steps)
        
        view_button = widgets.Button(description="As View", button_style='info', icon='eye')
        table_button = widgets.Button(description="As Table", button_style='info', icon='table')
        
        data_project = self.para["data_project"]
        con = self.item["con"] 
        
        data_project.tables = {}
        
        table_data = []

        tag_order = ["Projection", "Renaming", "Deduplication", "Casting", "Missing", "Cleaning"]

        all_tags = OrderedDict((tag, "") for tag in tag_order)
        
        for node in self.nodes.values():
            table_object = node.para["table_object"]
            new_table_name = table_object.table_name
            data_project.table_object[new_table_name] = table_object
            
            table_pipeline = node.para["table_pipeline"]
            codes = table_pipeline.get_codes(mode="dbt")
            tags = get_tags(codes)
            table_dict = {"table": new_table_name}
            for tag in all_tags:
                table_dict[tag] = "✔️" if tag in tags else "❌"
            table_data.append(table_dict)
            
            data_project.table_pipelines[new_table_name] = table_pipeline
            
        
        def on_materialize_click(b, mode):
            with self.output_context():
                display(HTML(f'{running_spinner_html}</i> Creating {mode[5:].lower()}s.'))
                self.progress = show_progress(len(self.nodes))
                
                con = self.item["con"]
                errors = []

                for node_id, node in self.nodes.items():
                    try:
                        table_pipeline = node.para["table_pipeline"]
                        if table_pipeline.is_materialized():
                            continue

                        database = self.para.get("database", None)
                        schema = self.para.get("schema", None)
                        table_pipeline.materialize(con=con, database=database, schema=schema, mode=mode)

                    except Exception as e:
                        error_msg = f"Error processing {node.para['table_name']}"
                        print(error_msg)
                        traceback.print_exc()
                    finally:
                        self.progress.value += 1
                
                display(HTML(f"Tables materialized as {mode[5:].lower()}s."))
        
        view_button.on_click(lambda b: on_materialize_click(b, "WITH_VIEW"))
        table_button.on_click(lambda b: on_materialize_click(b, "WITH_TABLE"))
        
        df = pd.DataFrame(table_data)

        column_order = ["table"] + list(all_tags.keys())
        df = df[column_order]
        
        submit_button = widgets.Button(description="Next", button_style='success', icon='check')
        
        def on_submit_button_click(b):
            with self.output_context():
                schema_only = self.para.get("cocoon_catalog_options", {}).get("schema_only", False)
                
                display(HTML(f'{running_spinner_html} Verifying tables ...'))
                
                for node in self.nodes.values():
                    table_object = node.para["table_object"]
                    table_pipeline = node.para["table_pipeline"]
                    old_table_name = node.para["table_name"]
                    new_table_name = table_object.table_name
                    old_table_found = False
                    for partition_name, table_list in data_project.partition_mapping.items():
                        if old_table_name in table_list:
                            old_table_found = True
                            table_list[table_list.index(old_table_name)] = new_table_name
                            
                            if partition_name not in data_project.tables or partition_name not in data_project.table_pipelines:
                                if schema_only:
                                    data_project.add_table(partition_name, table_object.columns)
                                    data_project.table_pipelines[partition_name] = table_pipeline
                                else:
                                    try:
                                        table_schema = table_pipeline.get_schema(con)
                                        if not table_schema:
                                            print(f"⚠️ Can't read {partition_name}; Is it materialized?")
                                            return
                                        data_project.add_table(partition_name, table_schema)
                                        data_project.table_pipelines[partition_name] = table_pipeline
                                    except Exception as e:
                                        print(f"⚠️ Error reading {partition_name}; Is it materialized?\n {str(e)}")
                                        return
                            break
                    
                    if not old_table_found:
                        if schema_only:
                            data_project.add_table(new_table_name, table_object.columns)
                        else:
                            try:
                                table_schema = table_pipeline.get_schema(con)
                                if not table_schema:
                                    print(f"⚠️ Can't read {new_table_name}; Is it materialized?")
                                    return
                                data_project.add_table(new_table_name, table_schema)
                            except Exception as e:
                                print(f"⚠️ Error reading {new_table_name}; Is it materialized?\n {str(e)}")
                                return
                
                callback({})
                return
                
        
        submit_button.on_click(on_submit_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_materialize_click(table_button, "WITH_TABLE")
            on_submit_button_click(submit_button)
            return
        
        display(HTML("💯 All the tables are staged! Here are what we have performed ..."))
        display(df)
        display(HTML("😎 Next, we will integrate the tables ... <br> ⚠️ You need to materialize the tables before proceeding."))

        display(widgets.HBox([table_button, view_button]))
        display(submit_button)
        
class StageForAllAndEnd(StageForAll):
    
    default_name = 'Stage For All'
    default_description = 'This stages the source tables'
    
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        create_progress_bar_with_numbers(1, model_steps)
        
        view_button = widgets.Button(description="As View", button_style='info', icon='eye')
        table_button = widgets.Button(description="As Table", button_style='info', icon='table')
        
        data_project = self.para["data_project"]
        con = self.item["con"] 
        data_project.tables = {}
        
        table_data = []

        tag_order = ["Projection", "Renaming", "Deduplication", "Casting", "Missing", "Cleaning"]

        all_tags = OrderedDict((tag, "") for tag in tag_order)
        
        for node in self.nodes.values():
            table_object = node.para["table_object"]
            new_table_name = table_object.table_name
            data_project.table_object[new_table_name] = table_object
            
            table_pipeline = node.para["table_pipeline"]
            codes = table_pipeline.get_codes(mode="dbt")
            tags = get_tags(codes)
            table_dict = {"table": new_table_name}
            for tag in all_tags:
                table_dict[tag] = "✔️" if tag in tags else "❌"
            table_data.append(table_dict)
            
            data_project.table_pipelines[new_table_name] = table_pipeline
        
        df = pd.DataFrame(table_data)

        column_order = ["table"] + list(all_tags.keys())
        df = df[column_order]

        display(HTML("💯 All the tables are staged! Here are what we have performed ..."))
        display(df)
        display(HTML("😎 The results are saved under the stage folder of dbt project!"))
        








def generate_join_graph_yaml(df, cocoon_story, table_descriptions, document):
    join_graph = {
        'cocoon_story': cocoon_story,
        'models': []
    }

    for table, primary_key in df.values:
        model = {
            'name': table,
            'description': table_descriptions.get(table[4:], ''),
        }

        if primary_key:
            model['primary_key'] = {
                'column': primary_key,
                'foreign_keys': []
            }

            if table in document['Decide FK']:
                for fk_table, fk_info in document['Decide FK'][table].items():
                    fk = {
                        'table': fk_table,
                        'column': fk_info['fk']
                    }
                    if fk_info['missing_pk'] > 0:
                        fk['cocoon_meta'] = {
                            'orphaned_record': fk_info['missing_pk']
                        }
                    model['primary_key']['foreign_keys'].append(fk)

        join_graph['models'].append(model)

    return yaml.dump(join_graph, default_flow_style=False)
       
                  
class DecideFKForAll(MultipleNode):
    default_name = 'Decide FK For All'
    default_description = 'This decides the FK for all tables'

    def construct_node(self, element_name, idx=0, total=0, fks=None):
        if fks is None:
            fks = []
        para = self.para.copy()
        para["table_name"] = element_name
        para["table_idx"] = idx
        para["table_total"] = total
        para["fks"] = fks
        
        node = DecideFK(para=para, id_para ="table_name")
        node.inherit(self)
        
        return node

    def extract(self, item):
        pk_df = pd.read_json(self.get_sibling_document('Decide PK for All'), orient="split")
        pk_df = pk_df[pk_df['Foreign Keys'].apply(lambda x: len(x) > 0)]
        
        table_to_fks = {}
        for idx, row in pk_df.iterrows():
            table_to_fks[row['Table']] = row['Foreign Keys']
        
        self.elements = list(pk_df['Table'])
        self.nodes = {element: self.construct_node(element, idx, len(self.elements), table_to_fks[element])
                      for idx, element in enumerate(self.elements)}
    
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        create_progress_bar_with_numbers(2, model_steps)
        
        display(HTML(f"🧐 For each table, we have identified the PK/FK:"))
        df = pd.read_json(self.get_sibling_document('Decide PK for All'), orient="split")
        editable_columns = [False, True, True]
        reset = True
        editable_list = {
            'Foreign Keys': {}
        }
        grid = create_dataframe_grid(df, editable_columns, reset=reset, editable_list=editable_list)
        display(grid)
        
        display(HTML(f"🧐 For each FK, we have identified the matched PK:"))
        
        data_project = self.para["data_project"]
        
        all_pks = [""]
        table_to_pk = data_project.primary_key
        for table in table_to_pk:
            pk = table_to_pk[table]
            all_pks.append(f"{table}[{pk}]")
        
        data = {
            'Table[FK]': [],
            'Table[PK]': [],
            'Referential Integrity': [],
        }
        
        if 'Decide FK' in document:
            for fk_table in document['Decide FK']:
                for fk in document['Decide FK'][fk_table]:
                    fk_info = document['Decide FK'][fk_table][fk]

                    if fk_info is None:
                        data['Table[FK]'].append(f"{fk_table}[{fk}]")
                        data['Table[PK]'].append(all_pks)
                        data['Referential Integrity'].append("")
                    else:
                        pk_table, pk, only, total = fk_info
                        data['Table[FK]'].append(f"{fk_table}[{fk}]")
                        new_list = [f"{pk_table}[{pk}]"] + [item for item in all_pks if item != f"{pk_table}[{pk}]"]
                        data['Table[PK]'].append(new_list)
                        data['Referential Integrity'].append(f"{'✅' if only == 0 else f'⚠️ {only} out of {total} are orphaned'}")
        
        df = pd.DataFrame(data)
        
        editable_columns = [False, True, False]
        reset = True
        lists = ['Table[PK]']
        grid = create_dataframe_grid(df, editable_columns, reset=reset, lists=lists)
        
        display(grid)
        
        next_button = widgets.Button(description="Next", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset)
                
                for idx, row in df.iterrows():
                    if row['Table[PK]'] == "":
                        continue
                    print(row['Table[PK]'])
                    fk_table, fk = row['Table[FK]'].split("[")
                    fk = fk[:-1]
                    pk_table, pk = row['Table[PK]'].split("[")
                    pk = pk[:-1]
                    if pk:
                        data_project.add_foreign_key_to_primary_key(fk_table, fk, pk_table, pk)
                
                callback(df.to_json(orient="split"))
        
        next_button.on_click(on_button_click)
        display(next_button)
        
        
        
class DecideFK(Node):
    default_name = 'Decide FK'
    default_description = 'This decides the FK for the table'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        create_progress_bar_with_numbers(2, model_steps)
        
        data_project = self.para["data_project"]
        con = self.item["con"]
        database_name = get_database_name(con) 
        
        table_name = self.para["table_name"]
        table_idx = self.para["table_idx"]
        table_total = self.para["table_total"]
        
        fks = self.para["fks"]
        table_to_pk = data_project.primary_key
        
        display(HTML(f"🧐 Identifying FK matching for table: {table_name}..."))
        show_progress(max_value=table_total, value=table_idx)
        
        matched_fks = {}
        
        for fk in fks:
            fk_type = data_project.tables[table_name][fk]
            fk_type = get_reverse_type(fk_type, database_name)
            
            for pk_table in table_to_pk:
                pk = table_to_pk[pk_table]
                pk_type = data_project.tables[pk_table][pk]
                pk_type = get_reverse_type(pk_type, database_name)
                
                if fk.lower() == pk.lower() and ((is_type_numeric(fk_type) and is_type_numeric(pk_type)) or (is_type_string(fk_type) and is_type_string(pk_type))):
                    only1, only2, overlap = generate_queries_for_overlap(pk_table, [fk], table_name, [fk], con)
                    if only2 + overlap == 0:
                        continue
                    ratio = only2 / (only2 + overlap)
                    matched_fks[fk] = (pk_table, table_to_pk[pk_table], only2, only2 + overlap)
                    break
                
            if fk in matched_fks:
                continue
                
            overlap_ratio = []
            
            for pk_table in table_to_pk:
                pk = table_to_pk[pk_table]
                
                pk_type = data_project.tables[pk_table][pk]
                pk_type = get_reverse_type(pk_type, database_name)
                
                if fk_type != pk_type:
                    continue
                
                only1, only2, overlap = generate_queries_for_overlap(pk_table, [pk], table_name, [fk], con)
                if only2 + overlap == 0:
                    continue
                ratio = only2 / (only2 + overlap)
                if ratio > 0.2:
                    continue
                overlap_ratio.append((pk_table, pk, only2, only2 + overlap))
                
            if len(overlap_ratio) > 0:
                matched_fks[fk] = min(overlap_ratio, key=lambda x: x[2] / x[3])
            else:
                matched_fks[fk] = None
            
        callback(matched_fks)
        
        
def create_cocoon_profile_yml_workflow(con, query_widget=None, viewer=False, table_name = None, para=None):
    if para is None:
        para = {}
        
    if query_widget is None:
        query_widget = QueryWidget(con)

    item = {
        "con": con,
        "query_widget": query_widget
    }
    
    workflow_para = {"viewer": viewer, 
                     "table_name": table_name,
                     "table_object": Table()}
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("Data Stage Workflow", 
                            item = item, 
                            description="A workflow to stage table",
                            para = workflow_para)
    
    main_workflow.add_to_leaf(SelectTable())
    main_workflow.add_to_leaf(DecideProjection(viewer=True))
    main_workflow.add_to_leaf(CreateShortTableSummaryContinue())
    main_workflow.add_to_leaf(DescribeColumnsListAndTableSummaryNoRename())
    main_workflow.add_to_leaf(DecideMissingList())
    main_workflow.add_to_leaf(DecideUnique())
    main_workflow.add_to_leaf(DecideStringCategoricalForAll())
    main_workflow.add_to_leaf(WriteStageYMLCode(class_para={"yaml_only": True}))
    
    return query_widget, main_workflow


class DisplayJoinGraph(Node):
    default_name = 'Display Join Graph'
    default_description = 'This displays the join graph'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        create_progress_bar_with_numbers(2, model_steps)
        
        display(HTML(f"🎉 Congratulations! We have integrate your data warehouses:"))
        
        data_project = self.para["data_project"]
        
        data_project.construct_links_from_pk_fk()
        data_project.display_graph_static()
        yml_content = data_project.join_graph_yaml()
        
        highlighted_yml = highlight_yml(yml_content)

        display(HTML(highlighted_yml))
        
        labels = ["YAML"]
        file_names = ["cocoon_join.yml"]
        contents = [yml_content]
        
        if "dbt_directory" in self.para:
            if not file_exists(os.path.join(self.para["dbt_directory"], "join")):
                create_directory(os.path.join(self.para["dbt_directory"], "join"))
            file_names = [os.path.join(self.para["dbt_directory"], "join", file_name) for file_name in file_names]
       
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
        save_files_click(save_button) 
        
        next_button = widgets.Button(description="Next", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                callback({})
        
        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(HTML("🎉 Next, we will model your data warehouses!"))
        display(next_button)


class StartDisplaySteps(Node):
    default_name = 'Product Steps'
    default_description = 'This is the start of the product steps'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        clear_output(wait=True)

        header_html = f'<div style="display: flex; align-items: center;">' \
            f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
            f'<div style="margin: 0; padding: 0;">' \
            f'<h1 style="margin: 0; padding: 0;">Cocoon</h1>' \
            f'<p style="margin: 0; padding: 0;">Organize Data Warehouses with LLM agents</p>' \
            f'</div>' \
            f'</div><hr>'

        description_html = f"""
        <p>Welcome to Cocoon! There are 3 main steps:</p>
        <img src="data:image/png;base64,{cocoon_steps}" style="width:350px">
        """

        display(HTML(header_html + description_html))



        next_button = widgets.Button(description="Start", 
                                     button_style='success',
                                    icon='check')
        
        def on_button_click(b):
            with self.output_context():
                callback({})

        next_button.on_click(on_button_click)
        
        
        hint_html = widgets.HTML(
            value='<p style="margin-top: 10px; font-style: italic; color: #666;">'
                    'Not Responding? Some Jupyter Notebooks need an output widget. <br>'
                    'Try <code>create_cocoon_workflow(con=con, output=widgets.Output())</code>'
                    '</p>'
        )
        
        display(VBox([next_button, hint_html]))


class DecidePKforAll(Node):
    default_name = 'Decide PK for All'
    default_description = 'This decides the PK for all tables'
    
    def extract(self, item):
        clear_output(wait=True)
        
        create_progress_bar_with_numbers(2, model_steps)
        display(HTML(f"🤓 Identifying the primary key..."))
        self.progress = show_progress(1)
        
        data_project = self.para["data_project"]
        
        table_unique_columns = {}
        table_summarys = {}
        for table_name, table_object in data_project.table_object.items():
            table_unique_columns[table_name] = [
                col for col, info in (table_object.uniqueness or {}).items() 
                if info.get("current_unique", False)
            ]
            table_summarys[table_name] = table_object.table_summary
            
        return table_unique_columns, table_summarys
    
    def run(self, extract_output, use_cache=True):
        table_unique_columns, table_summarys = extract_output
        
        all_table_desc = ""
        for table_name in table_unique_columns:

            unique_columns = table_unique_columns[table_name]
            if len(unique_columns) > 0:
                all_table_desc += f"'{table_name}': {table_summarys[table_name]}\n"
                all_table_desc += f"Unique Columns: {unique_columns}\n\n"
                
        template = f"""You have the following tables and candidate keys
{all_table_desc}
Now, for each table, choose the primary key from the candidate keys, based on the semantical meaning.
Return in the following format:
```yml
# This table is about ... The candidate keys are ... x column is the primary key
{next(iter(table_unique_columns))}: {next(iter(table_unique_columns[next(iter(table_unique_columns))]))}                
```"""
        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.safe_load(yml_code)
        
        return summary        

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):

        table_unique_columns, table_summarys = extract_output
        table_to_pk_mapping = run_output
        
        self.progress.value += 1
        
        data_project = self.para["data_project"]
        
        query_widget = self.item["query_widget"]
        dropdown = create_explore_button(query_widget, 
                                         table_name=list(data_project.table_object.keys()))
        
        data = {
            'Table': [],
            'Primary Key': [],
            'Foreign Keys': [],
        }
        
        for table_name, table_object in data_project.table_object.items():
            data['Table'].append(table_name)
            
            columns = [""] + table_object.columns

            primary_key = table_to_pk_mapping.get(table_name, "")
            
            foreign_keys = [col for col in columns if col.lower().endswith("_id")]

            columns.remove(primary_key)
            columns.insert(0, primary_key)
            
            if primary_key in foreign_keys:
                foreign_keys.remove(primary_key)
            
            data['Primary Key'].append(columns) 
            data['Foreign Keys'].append(foreign_keys)
            
            
        df = pd.DataFrame(data)
        
        editable_columns = [False, True, True]
        reset = True
        lists = ['Primary Key']
        editable_list = {
            'Foreign Keys': {}
        }
        grid = create_dataframe_grid(df, editable_columns, reset=reset, lists=lists, editable_list=editable_list)
        
        next_button = widgets.Button(description="Next", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                
                data_project = self.para["data_project"]
                
                for idx, row in df.iterrows():
                    table_name = row['Table']
                    primary_key = row['Primary Key']
                    foreign_keys = row['Foreign Keys']
                    if primary_key:
                        data_project.add_table_primay_key(table_name, primary_key)
                    if foreign_keys:
                        data_project.fks[table_name] = foreign_keys
                
                document = df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(grid)
        
        display(HTML(f"😎 Next, we construct the join graph."))
        display(next_button)
        
 
class EntityUnderstandingList(ListNode):
    default_name = 'Entity Understand'
    default_description = 'This node understands the entities of the tables in batches.'

    def extract(self, item):
        clear_output(wait=True)

        create_progress_bar_with_numbers(3, model_steps)
        display(HTML(f"🤓 Identifying the entities..."))
        

        data_project = self.para["data_project"]
        pk_table_column_pairs = list(data_project.foreign_key.keys())
        
        outputs = []
        batch_size = 50

        for i in range(0, len(pk_table_column_pairs), batch_size):
            batch_pairs = pk_table_column_pairs[i:i + batch_size]
            
            pk_table_description = ""
            for pk_table, pk in batch_pairs:
                table_object = data_project.table_object[pk_table]
                table_summary = table_object.table_summary
                pk_summary = table_object.print_column_desc(columns=[pk])
                pk_table_description += f"{pk_table}: {table_summary}\n   Its primary key is '{pk}': {pk_summary}\n"

            outputs.append((batch_pairs, pk_table_description))

        self.progress = show_progress(len(outputs))
        
        return outputs

    def run(self, extract_output, use_cache=True):
        pk_table_column_pairs, pk_table_description = extract_output
        
        if not pk_table_column_pairs:
            return {}

        template = f"""You have the following tables with primary keys:
{pk_table_description}

Now, for each table with the primary key, choose the name of the entity (e.g., Customers, Orders, Products) that it represents.
Return in the following format:
```yml
{pk_table_column_pairs[0][0]}:
    entity_name: ...
    entity_description: ...
...
```"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(summary, dict):
            raise TypeError("summary must be a dictionary")

        for table, data in summary.items():
            if not isinstance(data, dict):
                raise TypeError(f"Data for table '{table}' must be a dictionary")
            if 'entity_name' not in data or 'entity_description' not in data:
                raise KeyError(f"Data for table '{table}' is missing 'entity_name' or 'entity_description'")
            if not isinstance(data['entity_name'], str) or not isinstance(data['entity_description'], str):
                raise TypeError(f"'entity_name' and 'entity_description' for table '{table}' must be strings")

        missing_tables = set(pair[0] for pair in pk_table_column_pairs) - set(summary.keys())
        if missing_tables:
            raise ValueError(f"The following tables are missing from the LLM response: {missing_tables}")
        
        unexpected_tables = set(summary.keys()) - set(pair[0] for pair in pk_table_column_pairs)
        if unexpected_tables:
            raise ValueError(f"The following tables in the LLM response were not in the input: {unexpected_tables}")
        
        self.progress.value += 1
        
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        pk_table_column_pairs, pk_table_description = extract_output
        
        result = {}
        for pk_table, pk in pk_table_column_pairs:
            result[pk_table] = {"entity_name": pk, "entity_description": pk}
        return result

    def merge_run_output(self, run_outputs):
        merged_output = {}
        for run_output in run_outputs:
            merged_output.update(run_output)
        return merged_output

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        pk_table_column_pairs = []
        for single_extract_output in extract_output:
            pk_table_column_pairs.extend(single_extract_output[0])
            
        summary = run_output
        
        data = {
            'Table': [],
            'Primary Key': [],
            'Entity Name': [],
            'Entity Description': [],
        }
        
        for pk_table, pk in pk_table_column_pairs:
            data['Table'].append(pk_table)
            data['Primary Key'].append(pk)
            data['Entity Name'].append(summary[pk_table]["entity_name"])
            data['Entity Description'].append(summary[pk_table]["entity_description"])
            
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, False, True, True]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset)
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset)
                document = df.to_json(orient="split")
                
                entity_map = {}
                for idx, row in df.iterrows():
                    entity_map[row['Table']] = row['Entity Name']
                
                data_project = self.para["data_project"]
                data_project.entities = entity_map
                
                callback(document)
            
        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(grid)
        display(next_button)
        

class RelationUnderstandingForAll(MultipleNode):
    default_name = 'Relation Understand For All'
    default_description = 'This understands the relation of the tables'

    def construct_node(self, element_name, idx=0, total=0, entities=None):
        if entities is None:
            entities = []
            
        para = self.para.copy()
        para["table_name"] = element_name
        para["table_idx"] = idx
        para["table_total"] = total
        para["entities"] = entities
        
        node = RelationUnderstanding(para=para, id_para ="table_name")
        node.inherit(self)
        
        return node

    def extract(self, item):
        data_project = self.para["data_project"]

        result = get_table_to_entities_mapping(data_project.foreign_key, data_project.entities)

        for table in data_project.entities:
            if len(result[table]) == 1:
                del result[table]
        
        self.elements = list(result.keys())
        
        self.nodes = {element: self.construct_node(element, idx, len(self.elements), result[element])
                      for idx, element in enumerate(self.elements)}
    
    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        create_progress_bar_with_numbers(3, model_steps)
        
        data_project = self.para["data_project"]
        query_widget = self.item["query_widget"]
        
        dropdown = create_explore_button(query_widget, 
                                    table_name=list(data_project.table_pipelines.keys()),
                                    logical_to_physical=data_project.table_pipelines)
        
        data = {
            'Table': [],
            'Entities': [],
            'Relation Name': [],
            'Relation Description': [],
        }
        
        if 'Relation Understand' in document:
            for table in document['Relation Understand']:
                summary = document['Relation Understand'][table]
                data['Table'].append(table)
                data['Entities'].append(summary['entities'])
                data['Relation Name'].append(summary.get('relation_name', ""))
                data['Relation Description'].append(summary['relation_desc'])
                
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True, True]
        lists = ['Entities']
        reset = True
        relation_grid = create_dataframe_grid(df, editable_columns, reset=reset, lists=lists)
        
        next_button = widgets.Button(description="Next", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                relation_df = grid_to_updated_dataframe(relation_grid, reset=reset, lists=lists)
                document = relation_df.to_json(orient="split")

                relation_df = relation_df[relation_df['Relation Name'] != ""]
                
                
                
                callback(document)
            
        next_button.on_click(on_button_click)
        on_button_click(next_button)
        

        


        




        
        
        
        
                    
                    
    
        
        
                

        



    
   
      









def generate_workflow_graph(relation_map, relation_details, highlight_indices=None):
    class Node:
        def __init__(self, name, node_type):
            self.name = name
            self.type = node_type
            self.id = f"{name}_{node_type}"

    nodes = []
    edges = []

    for item in relation_details:
        name = item.get('Name') or item.get('name')
        node_type = item.get('Type') or item.get('type')

        new_node = Node(name, node_type.lower())

        if not any(node.id == new_node.id for node in nodes):
            nodes.append(new_node)

    for relation, entities in relation_map.items():
        relation_node = next((node for node in nodes if node.name == relation and node.type == 'relation'), None)
        if relation_node:
            relation_index = nodes.index(relation_node)
            for entity in entities:
                entity_node = next((node for node in nodes if node.name == entity and node.type == 'entity'), None)
                if not entity_node:
                    entity_node = Node(entity, 'entity')
                    nodes.append(entity_node)
                entity_index = nodes.index(entity_node)
                edges.append((relation_index, entity_index))

    node_shapes = [
        "box" if node.type == 'relation' else
        "oval" if node.type == 'entity' else
        "octagon" if node.type == 'group' else
        "ellipse"
        for node in nodes
    ]

    nodes = [node.name for node in nodes]

    highlight_nodes_indices = []
    highlight_edges_indices = []
    
    if highlight_indices is None:
        highlight_indices = [len(relation_details) - 1]
    
    for index in highlight_indices:
        if 0 <= index < len(relation_details):
            highlight_item = relation_details[index]
            highlight_item_name = highlight_item.get('Name') or highlight_item.get('name')
            
            if highlight_item_name in nodes:
                highlight_nodes_indices.append(nodes.index(highlight_item_name))
            
            is_relation = (
                highlight_item.get('Type', '').lower() == 'relation' or
                highlight_item.get('type', '').lower() == 'relation'
            )
            if is_relation and highlight_item_name in relation_map:
                related_entities = relation_map[highlight_item_name]
                for entity in related_entities:
                    if entity in nodes:
                        entity_index = nodes.index(entity)
                        highlight_nodes_indices.append(entity_index)
                        edge = (nodes.index(highlight_item_name), entity_index)
                        if edge in edges:
                            highlight_edges_indices.append(edges.index(edge))
    
    highlight_nodes_indices = list(set(highlight_nodes_indices))
    highlight_edges_indices = list(set(highlight_edges_indices))
    
    html_output = generate_workflow_html_multiple(nodes, edges, node_shape=node_shapes, directional=False,
                                                  highlight_nodes_indices=highlight_nodes_indices,
                                                  highlight_edges_indices=highlight_edges_indices)
    
    return html_output



def create_html_content_er_story(relation_map, relation_details, df_display=None, page_no=0):
    list_of_descriptions = '<p>Story behind the relationships <small class="text-muted"> (only for those connect >= 2 entities)</small></p><ol class="small">'

    def get_name(entry):
        return entry.get('Name') or entry.get('name')

    def get_description(entry):
        return entry.get('Description') or entry.get('description')

    for i in range(page_no):
        entry = relation_details[i]
        relation_name, relation_desc = get_name(entry), get_description(entry)
        list_of_descriptions += f"<li>[{relation_name}]: {relation_desc}</li>"
    
    entry = relation_details[page_no]
    relation_name, relation_desc = get_name(entry), get_description(entry)
    list_of_descriptions += f"<li><b>[{relation_name}]: {relation_desc}</b></li>"
    list_of_descriptions += "</ol>"

    relation_details = relation_details[:page_no+1]

    included_relations = [get_name(detail) for detail in relation_details]

    graph_html = generate_workflow_graph(relation_map, relation_details)
    
    if df_display is not None:
        df_html = df_display[page_no]
    else:
        df_html = ""
        
    html_content = f"""
    {list_of_descriptions}
    {graph_html}
    <br>
    {df_html}
    """
    return html_content



class BuildERStory(Node):
    default_name = 'Build ER Story'
    default_description = 'This builds the ER story'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        create_progress_bar_with_numbers(3, model_steps)
        
        data_project = self.para["data_project"]
        con = self.item["con"]
        
        relation_df = pd.read_json(self.get_sibling_document('Refine Relations'), orient="split")
        entity_df = pd.read_json(self.get_sibling_document('Entity Understand'), orient="split")
        group_df = data_project.groups
        
        document = self.get_sibling_document('Reorder Relation to Story for All').get('Reorder Relation To Story', {})
        
        extracted_category = self.get_sibling_document('Build Table Hierarchy')
        extracted_category = copy.deepcopy(extracted_category)

        for path, content in document.items():
            path = ast.literal_eval(path)
            current = extracted_category
            
            for i, key in enumerate(path):
                if i == len(path) - 1:
                    current[key]["children"] = content
                else:
                    current = current[key]["children"]

        first_key = next(iter(extracted_category))

        story_summary = extracted_category[first_key]["children"]
        
        display(HTML(f"🥳 We have modeled your data:"))
        
        
        
        
        
        yml_dict = build_story_yml_dict(relation_df, entity_df, story_summary, group_df)
        yml_content = yaml.dump(yml_dict, default_flow_style=False)
        
        highlighted_yml = highlight_yml(yml_content)

        display(HTML(highlighted_yml))
        
        labels = ["YAML"]
        file_names = ["cocoon_er.yml"]
        contents = [yml_content]
        
        if "dbt_directory" in self.para:
            if not file_exists(os.path.join(self.para["dbt_directory"], "er")):
                create_directory(os.path.join(self.para["dbt_directory"], "er"))
            file_names = [os.path.join(self.para["dbt_directory"], "er", file_name) for file_name in file_names]
       
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
        save_files_click(save_button) 
        
        next_button = widgets.Button(description="Next", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                callback({})
            
        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return

        display(next_button)


class DecideDataTypeForAll(MultipleNode):
    default_name = 'Decide Data Type'
    default_description = 'This node allows users to decide the data type for all columns.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["column_name"] = element_name
        para["column_idx"] = idx
        para["total_columns"] = total
        node = DecideDataTypeSingle(para=para, id_para ="column_name")
        node.inherit(self)
        return node
    
    def extract(self, item):
        table_pipeline = self.para["table_pipeline"]
        con = self.item["con"]
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        
        self.elements = columns
        self.nodes = {col: self.construct_node(col, idx, len(columns)) for idx, col in enumerate(columns)}


    def display_after_finish_workflow(self, callback, document):
        clear_output(wait=True)
        
        con = self.item["con"]
        database_name = get_database_name(con)
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]
        
        
        data = {
            'Column': [],
            'Current Type': [],
            'Target Type': [],
            'Matched?':[]
        }
        
        alert = 0
        
        if "Decide Data Type" in document:
            for column_name, details in document["Decide Data Type"].items():
                data['Column'].append(column_name)
                data['Current Type'].append(details['current_type'])
                data['Target Type'].append(details['target_type'])
                data['Matched?'].append("✔️ Yes"  if details['current_type'] == details['target_type'] else "❌ No")
                if details['current_type'] != details['target_type']:
                    alert += 1
                
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        all_data_types = list(data_types_database[database_name].keys())
        editable_columns = [False, False, True, False]
        lists = {'Target Type': all_data_types}
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset, lists=lists)
        
        next_button = widgets.Button(
            description='Accept Cast',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  
        
        reject_button = widgets.Button(
            description='Reject Cast',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
        
        def on_button_clicked2(b):
            with self.output_context():
                clear_output(wait=True)
                df = grid_to_updated_dataframe(grid, reset=reset)
                df['Target Type'] = df['Current Type']
                callback(df.to_json(orient="split"))

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                df = grid_to_updated_dataframe(grid, reset=reset)
                callback(df.to_json(orient="split"))
            
        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
        create_progress_bar_with_numbers(1, doc_steps)
        create_explore_button(query_widget, table_pipeline)
        display(HTML(f"😎 We have analyzed Data Types:"))
        if alert > 0:
            display(HTML(f"⚠️ There are {alert} columns with mismatched data types."))
        else:
            display(HTML(f"✅ All columns have the correct data types."))
        display(grid)
        display(HBox([reject_button, next_button]))

class DecideDataTypeSingle(Node):
    default_name = 'Decide Data Type'
    default_description = 'This node allows users to decide the data type for a single column.'

    def extract(self, item):
        clear_output(wait=True)
        
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        
        column_name = self.para["column_name"]
        idx = self.para["column_idx"]
        total = self.para["total_columns"]
        
        table_object = self.para["table_object"]
        column_desc = table_object.column_desc.get(column_name, "")
        
        display(HTML(f"🔍 Checking data types for <i>{table_name}[{column_name}]</i>..."))
        create_progress_bar_with_numbers(1, doc_steps)
        show_progress(max_value=total, value=idx)
        
        sample_size = 50
        
        database_name = get_database_name(con)
        all_data_types = list(data_types_database[database_name].keys())
        
        schema = table_pipeline.get_schema(con)
        column_type = get_reverse_type(schema[column_name], database_name)
        
        query = create_sample_distinct_query(con, table_pipeline, column_name, sample_size)
        with_context = table_pipeline.get_codes(mode="WITH")
        query = with_context + "\n" + query
        sample_values = run_sql_return_df(con,query)
        
        return column_name, column_type, all_data_types, sample_values, column_desc

    def run(self, extract_output, use_cache=True):
        column_name, column_type, all_data_types, sample_values, column_desc = extract_output
        self.messages = []

        sample_values_list = sample_values[column_name].values.tolist()
        
        template = f"""'{column_name}' is for: {column_desc}
It has the following distinct values:  {sample_values_list}

Task: Decide what is the most suitable column type (potentially after some extraction).
The column type should be one of the following: {all_data_types}
Choose BOOLEAN only if values mean true/false (So, e.g., Male/Female shall not be BOOLEAN)

Return in the following format:
```json
{{
    "reasoning": "The column means... It is most suitable to be...",
    "target_type": "INT"
}}
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        json_code = extract_json_code(response['choices'][0]['message']["content"])
        summary = json.loads(json_code)
        
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: all(key in jc for key in ["reasoning", "target_type"]), "The JSON code does not contain the correct keys."),
            (lambda jc: jc["target_type"] in all_data_types, "The target type is not a valid data type."),
        ]

        for check, error_message in checks:
            if not check(summary):
                raise ValueError(f"Validation failed: {error_message}")
            
        summary["current_type"] = column_type
        
        return summary
        
    def run_but_fail(self, extract_output, use_cache=True):
        column_name, column_type, _, _, database_name, _, _ = extract_output
        return {"reasoning": "Fail to decide", 
                "target_type": column_type,
                "current_type": column_type}
        
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
        
def build_column_viz(column_name, is_numeric=True, con=None, table_name=None):
    html_output = ""
    javascript_output = ""
    if is_numeric:
        html_output += f"<div id=\"hist_viz_{column_name}\"></div>"
        counts, bin_width, bin_centers = build_histogram_inputs(con, column_name, table_name)
        javascript_output += f"""data = {[
        {"x": center, "y": count} for center, count in zip(bin_centers, counts)
    ]};
    binWidth = {bin_width};
    drawHistogram("hist_viz_{column_name}", data, binWidth);
"""
    
    else:
        html_output += f"<div id=\"bar_viz_{column_name}\"></div>"
        data_dict = build_barchat_input(con, column_name, table_name)
        total_value = sum(data_dict.values())
        data = [{"label": (str(label)[:15] + "...") if len(str(label)) > 15 else str(label), "value": (value / total_value) * 100} for label, value in data_dict.items()]
        javascript_output += f"""data = {data};
    drawBarChart("bar_viz_{column_name}", data);
"""  
    
    return html_output, javascript_output





class DBTProjectConfig(Node):
    default_name = 'DBT Project Configuration'
    default_description = 'This step allows you to configure the DBT project.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        dbt_name_input = widgets.Text(
            description='Name:',
            value=self.para.get("dbt_name", "My Project"),
            style={'description_width': 'initial'}
        )

        dbt_directory_input = widgets.Text(
            description='Directory:',
            value=self.para.get("dbt_directory", "./dbt_project"),
            style={'description_width': 'initial'}
        )

        next_button = widgets.Button(description="Next", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                dbt_name = dbt_name_input.value
                dbt_directory = dbt_directory_input.value

                self.para["dbt_name"] = dbt_name
                self.para["dbt_directory"] = os.path.join(dbt_directory, "models")

                callback({
                    "dbt_name": dbt_name,
                    "dbt_directory": dbt_directory
                })

        next_button.on_click(on_button_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "dbt_name" in self.para and "dbt_directory" in self.para:
                on_button_click(next_button)
                return
        
        create_progress_bar_with_numbers(0, model_steps)
        display(HTML("<div style='line-height: 1.2;'><h2>🛠️ Configure the project</h2>😎 <em>You will find the results in the directory as a dbt project</em></div>"))
        display(VBox([dbt_name_input, dbt_directory_input]), next_button)
            
class WriteSCDForAll(MultipleNode):
    default_name = 'Write SCD For All'
    default_description = 'This writes SCD queries for the given tables'
    
    def construct_node(self, element_name, idx=0, total=0, order_by_clause=None, id_columns=None, version_columns=None):
        para = self.para.copy()
        para["element_name"] = element_name
        para["idx"] = idx
        para["total"] = total
        para["order_by_clause"] = order_by_clause
        para["id_columns"] = id_columns
        para["version_columns"] = version_columns
        
        node = WriteSCDTable(para=para)
        node.inherit(self)
        return node
    
    def extract(self, item):
        document = self.get_sibling_document("Perform SCD For All").get('Perform SCD Table', {})
        print(document)
        self.elements = []
        self.nodes = {}
        
        for idx, (table, table_data) in enumerate(document.items()):
            if isinstance(table_data, dict) and "order_by_clause" in table_data:
                self.elements.append(table)
                self.nodes[table] = self.construct_node(
                    element_name=table,
                    idx=idx,
                    total=len(document),
                    order_by_clause=table_data["order_by_clause"],
                    id_columns=table_data.get("id_columns", []),
                    version_columns=table_data.get("version_columns", [])
                )
        
        self.item = item
        
        
class WriteSCDTable(Node):
    default_name = 'Write SCD Table'
    default_description = 'This writes SCD for the given table'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        
        table_name = self.para["element_name"]
        
        display(HTML(f'{running_spinner_html} Writing Slowly Changing Dimensions for <i>{table_name}</i> ...'))

        idx = self.para["idx"]
        total = self.para["total"]
        con = self.item["con"]
        self.progress = show_progress(max_value=total, value=idx)
        
        order_by_clause = self.para["order_by_clause"]
        id_columns = self.para["id_columns"]
        version_columns = self.para["version_columns"]
        
        data_project = self.para["data_project"]
        table_pipeline = data_project.table_pipelines[table_name]
        
        sample_size = 5
        sample_df = run_sql_return_df(con, f'SELECT * FROM {table_pipeline} LIMIT {sample_size}')
        sample_df = sample_df.applymap(truncate_cell)
        
        table_object = data_project.table_object[table_name]
        table_summary = table_object.table_summary

        return table_name, order_by_clause, id_columns, version_columns, sample_df, table_summary, table_object
    
    def run(self, extract_output, use_cache=True):
        table_name, order_by_clause, id_columns, version_columns, sample_df, table_summary, table_object = extract_output
        
        template = f"""You have table '{table_name}' with the following sample rows:
{sample_df.to_csv(index=False, quoting=1)}

Its current summary:
{table_summary}

This is a slowly changing dimension table.
The columns that uniquely identify objects are: {id_columns}
The ORDER BY clause for identifying the latest version is: {order_by_clause}

We are going to create a snapshot table from this table, where 
(1) only the latest version of each object is kept
(2) The version columns are removed

Please update the summary of the new table, in short simple SVO sentences and < 500 chars
Respond with the following format:
```yml
new_summary: The table is about ... It tracks the most recent version of ...
```"""
        messages = [{"role": "user", "content": template}]
        
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(summary, dict):
            raise TypeError("summary must be a dictionary")
        if 'new_summary' not in summary:
            raise KeyError("summary is missing the 'new_summary' key")
        if not isinstance(summary['new_summary'], str):
            raise TypeError("summary['new_summary'] must be a string")
        
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        table_name, order_by_clause, id_columns, version_columns, sample_df, table_summary, table_object = extract_output
        return {"new_summary": table_summary}

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        table_name, order_by_clause, id_columns, version_columns, sample_df, table_summary, table_object = extract_output
        
        labels = []
        file_names = []
        contents = []
        con = self.item["con"]
        
        new_table_name = "snapshot_" + table_name.replace("stg_", "")
        data_project = self.para["data_project"]
        columns = table_object.columns
        
        id_column_string = ", ".join([enclose_table_name(col, con=con) for col in id_columns])
        columns_except_version_string = ",\n".join([enclose_table_name(col, con=con) for col in columns if col not in version_columns])
        
        comment = f"""-- Slowly Changing Dimension: Dimension keys are {id_column_string}
-- Version columns are {', '.join(version_columns)}
-- We will create Type 1 SCD (latest snapshot)
"""
        sql_query = f"""
SELECT 
{indent_paragraph(columns_except_version_string)}
FROM {enclose_table_name(table_name, con)}
QUALIFY ROW_NUMBER() OVER ( 
    PARTITION BY {id_column_string}
    ORDER BY
{indent_paragraph(order_by_clause, spaces=8)}
) = 1"""
        sql_query = comment + sql_query

        labels.append("SQL")
        file_names.append(f"{new_table_name}.sql")
        contents.append(sql_query)
        
        table_pipeline = data_project.table_pipelines[table_name]
        sql_query = f"""
SELECT 
{indent_paragraph(columns_except_version_string)}
FROM {enclose_table_name(table_pipeline, con)}
QUALIFY ROW_NUMBER() OVER ( 
    PARTITION BY {id_column_string}
    ORDER BY
{indent_paragraph(order_by_clause, spaces=8)}
) = 1"""

        sql_step = SQLStep(table_name=table_name, con=con)
        new_table_pipeline = TransformationSQLPipeline(steps = [sql_step], edges=[])

        sql_query = comment + sql_query
        
        database = self.para.get("database", None)
        schema = self.para.get("schema", None)

        step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con, database=database, schema=schema)
        new_table_pipeline.add_step_to_final(step)
        new_table_pipeline.run_codes(con=con, mode="WITH_TABLE")
        
        data_project.table_pipelines[new_table_name] = new_table_pipeline
        
        schema = new_table_pipeline.get_schema(con)
        columns = list(schema.keys())
        
        table_object.columns = columns
        table_object.table_name = new_table_name
        table_object.table_summary = summary["new_summary"]
        
        if len(id_columns) == 1:
            id_column = id_columns[0]
            table_object.uniqueness[id_column] = {}
            table_object.uniqueness[id_column]["current_unique"] = True
            table_object.uniqueness[id_column]["unique_reason"] = "Unique dimension key, derived from the slowly changing dimension"
        
        yml_dict = table_object.create_dbt_schema_dict(table_cocoon_meta={"scd_base_table": table_name, "scd_columns": version_columns})
        yml_content = yaml.dump(yml_dict, default_flow_style=False)
        labels.append("YML")
        file_names.append(f"{new_table_name}.yml")
        contents.append(yml_content)

        if "dbt_directory" in self.para:
            if not file_exists(os.path.join(self.para["dbt_directory"], "snapshot")):
                create_directory(os.path.join(self.para["dbt_directory"], "snapshot"))
            file_names = [os.path.join(self.para["dbt_directory"], "snapshot", file_name) for file_name in file_names]
       
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
        overwrite_checkbox.value = True
        save_files_click(save_button)  
        
        data_project.change_table_name(table_name, new_table_name)
        callback({"sql": sql_query, "yml": yml_content})
        
class PerformSCDForAll(MultipleNode):
    default_name = 'Perform SCD For All'
    default_description = 'This perform SCD for the given tables'
    
    def construct_node(self, element_name, idx=0, total=0, version_columns = None, id_columns = None):
        
        para = self.para.copy()
        para["element_name"] = element_name
        para["idx"] = idx
        para["total"] = total
        para["version_columns"] = version_columns
        para["id_columns"] = id_columns
        
        node = PerformSCDTable(para=para)
        node.inherit(self)
        return node
    
    def extract(self, item):
        df = pd.read_json(self.get_sibling_document('Decide SCD For All'), orient="split")
        
        scd_tables = df[df['Is SCD'] == True]['Table'].tolist()
         
        self.elements = scd_tables
        self.nodes = {element: self.construct_node(element, idx, len(self.elements), 
                                                   df[df["Table"] == element]["Version Columns"].iloc[0],
                                                    df[df["Table"] == element]["ID Columns"].iloc[0])
                      for idx, element in enumerate(self.elements)}
        self.item = item


class PerformSCDTable(Node):
    default_name = 'Perform SCD Table'
    default_description = 'This perform SCD for the given table'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        
        table_name = self.para["element_name"]
        
        display(HTML(f'{running_spinner_html} Performing Slowly Changing Dimensions for <i>{table_name}</i> ...'))

        idx = self.para["idx"]
        total = self.para["total"]
        con = self.item["con"]
        self.progress = show_progress(max_value=total, value=idx)
        
        version_columns = self.para["version_columns"]
        id_columns = self.para["id_columns"]
        data_project = self.para["data_project"]
        table_pipeline = data_project.table_pipelines[table_name]
        table_object = data_project.table_object[table_name]
        column_desc = table_object.print_column_desc(columns = version_columns, show_category=True, show_pattern=True, show_type=True)

        return con, table_pipeline, version_columns, id_columns, column_desc
    
    def run(self, extract_output, use_cache=True):
        con, table_pipeline, version_columns, id_columns, column_desc = extract_output
        table_name = self.para["element_name"]
        database_name = get_database_name(con)
        max_iterations = 10
        
        initial_template = f"""Table '{table_name}' has the following version columns:
{column_desc}

Task: Create the ORDER BY clause for an SCD (Slowly Changing Dimension) query for this table.
Version columns: {version_columns}

The ORDER BY clause should be in the following format:
SELECT * 
FROM table
QUALIFY ROW_NUMBER() OVER ( 
    PARTITION BY id_col 
    ORDER BY 
        {{order_by_clause}}
) = 1

Note that we use {database_name} syntax.

Return the result in yml
```yml
reasoning: >
    To create the ORDER BY clause for the SCD query, we need to ...

order_by_clause: |
    "update time" DESC,
    CASE WHEN "is_deleted" = true THEN 0 ELSE 1 END,
    "valid_until" IS NULL DESC
```"""
        messages = [{"role": "user", "content": initial_template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        reasoning = summary["reasoning"]
        
        for i in range(max_iterations):
            try:
                test_query = f"""
                SELECT 1
                FROM {table_pipeline}
                ORDER BY
                {summary['order_by_clause']}
                LIMIT 1
                """
                df = run_sql_return_df(con, test_query)
                break
            except Exception:
                detailed_error_info = get_detailed_error_info()
                debug_template = f"""Table '{table_name}' has the following version columns:
{column_desc}

You have the following order_by_clause:
{summary['order_by_clause']}

The order_by_clause is used in the following query:
SELECT * FROM "TABLE" ORDER BY {{order_by_clause}}

It has an error: {detailed_error_info}

Please correct the ORDER BY clause, but don't change the logic.
Note that we use {database_name} syntax.
Return the result in yml
```yml
reasoning: >
    The error is caused by ...

order_by_clause: |
    "col_name" DESC
```"""
                messages = [{"role": "user", "content": debug_template}]
                response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=(i == 0))
                messages.append(response['choices'][0]['message'])
                self.messages.append(messages)

                yml_code = extract_yml_code(response['choices'][0]['message']["content"])
                summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
                
        if i == max_iterations - 1:
            return self.run_but_fail(extract_output, use_cache)
        
        summary["reasoning"] = reasoning
        summary["id_columns"] = id_columns
        summary["version_columns"] = version_columns
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        con, table_pipeline, version_columns, id_columns, column_desc = extract_output
        return {"reasoning": "Failed to create ORDER BY clause", "order_by_clause": ",\n".join(version_columns),
                "id_columns": id_columns, "version_columns": version_columns}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)
    
    
class DecideSCDTable(Node):
    default_name = 'Decide SCD Table'
    default_description = 'This decide if the given table contains SCD'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        
        table_name = self.para["element_name"]
        
        display(HTML(f'{running_spinner_html} Identifying Slowly Changing Dimensions for <i>{table_name}</i> ...'))

        idx = self.para["idx"]
        total = self.para["total"]
        con = self.item["con"]
        self.progress = show_progress(max_value=total, value=idx)

        data_project = self.para["data_project"]
        table_pipeline = data_project.table_pipelines[table_name]
        table_object = data_project.table_object[table_name]
        table_desc = table_object.table_summary
        
        if self.para.get("cocoon_catalog_options", {}).get("schema_only", False):
            table_info = table_object.print_column_desc()
        else:
            sample_size = 5
            sample_df = run_sql_return_df(con, f'SELECT * FROM {table_pipeline} LIMIT {sample_size}')
            sample_df = sample_df.applymap(truncate_cell)
            table_info = sample_df.to_csv(index=False, quoting=1)
        
        return table_name, table_desc, table_info
        
    def run(self, extract_output, use_cache=True):
        table_name, table_desc, table_info = extract_output

        template =  f"""You have table '{table_name}': {table_desc}
        
Here is the table information:
{table_info}

Is the table a slowly changing dimension?
SCD have different versions of rows to track dimension element changes over time
It has to be a dimension, not a fact table that tracks aggregates over time.

If so, what are the columns that
(1) uniquely identify each dimension element
(2) for each element, identify columns that track its update timestamp/version
Respond with the following format:
```yml
reasoning: >-
    The table is about ... It is a dimension/fact ...
    It tracks the versions of ... The columns that track the update timestamp/version are ...
scd: true/false
# if scd is true, specify the following
id_cols: ["col1", "col2"...]
version_cols: ["col1", "col2"...] # columns related to the update timestamp/version
```"""
        messages = [{"role": "user", "content": template}]

        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(summary, dict):
            raise TypeError("summary must be a dictionary")

        required_keys = ['reasoning', 'scd']
        for key in required_keys:
            if key not in summary:
                raise KeyError(f"summary is missing required key: {key}")

        if not isinstance(summary['reasoning'], str):
            raise TypeError("summary['reasoning'] must be a string")
        if not isinstance(summary['scd'], bool):
            raise TypeError("summary['scd'] must be a boolean")

        if summary['scd']:
            for key in ['id_cols', 'version_cols']:
                if key not in summary:
                    raise KeyError(f"summary is missing required key for SCD: {key}")
                if not isinstance(summary[key], list):
                    raise TypeError(f"summary['{key}'] must be a list")
                if not all(isinstance(col, str) for col in summary[key]):
                    raise TypeError(f"All elements in summary['{key}'] must be strings")

            all_cols = set(summary['id_cols'] + summary['version_cols'])
            missing_cols = all_cols - set(sample_df.columns)
            if missing_cols:
                raise ValueError(f"The following columns are missing from the DataFrame: {missing_cols}")
            
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {
            "reasoning": "Fail to decide",
            "scd": False
        }
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
        
        
    
class DecideSCDForAll(MultipleNode):
    default_name = 'Decide SCD For All'
    default_description = 'This decide if the given tables contain SCD'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["element_name"] = element_name
        para["idx"] = idx
        para["total"] = total
        node = DecideSCDTable(para=para)
        node.inherit(self)
        return node

    def extract(self, item):
        cocoon_catalog_options = self.para.get('cocoon_catalog_options', {})
        
        if cocoon_catalog_options.get("schema_only", False) or cocoon_catalog_options.get("scd", True) is False:
            self.elements = []
            self.nodes = {}
            return

        data_project = self.para["data_project"]
        
        candidate_elements = set(data_project.tables.keys())
        candidate_elements -= set(data_project.partition_mapping.keys())
        
        self.elements = list(candidate_elements)
        
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item

    def display_after_finish_workflow(self, callback, document):

        data_project = self.para["data_project"]
        query_widget = self.item["query_widget"]
        
        dropdown = create_explore_button(query_widget, 
                                    table_name=list(data_project.table_pipelines.keys()),
                                    logical_to_physical=data_project.table_pipelines)
        

        data = {
            'Table': [],
            'Is SCD': [],
            'Version Columns': [],
            'ID Columns': [],
        }
        
        if "Decide SCD Table" in document:
            for table in document["Decide SCD Table"]:
                summary = document["Decide SCD Table"][table]
                data['Table'].append(table)
                data['Is SCD'].append(summary["scd"])
                if summary["scd"]:
                    data['Version Columns'].append(summary["version_cols"])
                    data['ID Columns'].append(summary["id_cols"])
                else:
                    data['Version Columns'].append([])
                    data['ID Columns'].append([])
                    
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True, True]
        reset = True
        editable_list = {
            'Version Columns': {},
            'ID Columns':{}
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset,  editable_list=editable_list)
        
        
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                document = df.to_json(orient="split")
                
                callback(document)

        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(HTML("🤓 We have identified slowly changing dimensions. <br> 😎 Slowly changing dimensions are hard to model, so we will get the latest snapshot"))  
        display(grid)
        display(next_button)
        
class DecideKeysForAll(MultipleNode):
    default_name = 'Decide Keys For All'
    default_description = 'This decide keys for the given tables'
    
    def construct_node(self, element_name, idx=0, total=0):
        
        para = self.para.copy()
        para["element_name"] = element_name
        para["idx"] = idx
        para["total"] = total
        
        node = DecideKeysTable(para=para)
        node.inherit(self)
        return node
    
    def extract(self, item):
        data_project = self.para["data_project"]
         
        self.elements = list(data_project.tables.keys())
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        self.item = item
        
    def display_after_finish_workflow(self, callback, document):
        
        query_widget = self.item["query_widget"]
        data_project = self.para["data_project"]
        dropdown = create_explore_button(query_widget, 
                                         table_name=self.elements,
                                         logical_to_physical=data_project.table_pipelines)
        
        data = {
            'Table': [],
            'Primary Key': [],
            'Keys': [],
        }
        
        if "Decide Keys Table" in document:
            for table in document["Decide Keys Table"]:
                summary = document["Decide Keys Table"][table]
                data['Table'].append(table)
                data['Keys'].append(summary["keys"])
                if summary["pk"] is None:
                    data['Primary Key'].append("")
                else:
                    data['Primary Key'].append(summary["pk"])
                    
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True]
        reset = True
        editable_list = {
            'Keys': {}
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset,  editable_list=editable_list)
        
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                document = df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(HTML("🤓 We have identified keys for the tables"))
        display(grid)
        display(next_button)

        
class DecideKeysTable(Node):
    default_name = 'Decide Keys Table'
    default_description = 'This decide keys for the given table'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        
        table_name = self.para["element_name"]
        
        display(HTML(f'{running_spinner_html} Identifying Keys for <i>{table_name}</i> ...'))

        idx = self.para["idx"]
        total = self.para["total"]
        con = self.item["con"]
        self.progress = show_progress(max_value=total, value=idx)
        
        data_project = self.para["data_project"]
        table_pipeline = data_project.table_pipelines[table_name]
        table_object = data_project.table_object[table_name]
        table_summary = table_object.table_summary
        column_desc = table_object.print_column_desc(show_category=True, show_unique=True, show_pattern=True)

        if self.para.get("cocoon_catalog_options", {}).get("schema_only", False):
            sample_df = ""
        else:
            sample_size = 5
            sample_df = run_sql_return_df(con, f'SELECT * FROM {table_pipeline} LIMIT {sample_size}')
            sample_df = sample_df.applymap(truncate_cell)
            sample_df = sample_df.to_csv(index=False, quoting=1)
        
        return table_name, sample_df, table_summary, column_desc

    def run(self, extract_output, use_cache=True):
        table_name, sample_df, table_summary, column_desc = extract_output
        
        template =  f"""You have table '{table_name}': {table_summary}
{"It has the following sample rows:" + sample_df  if sample_df else ""}
Below are the summary of the columns:
{column_desc}

Task: First, identify the columns that represent keys or ids, potentially for primary or foreign keys.
These columns could be integers, or strings with some patterns. Please exclude "Date" or "Location" columns.
Then, identify if there is a primary key for this table, which is a key and unique for each row.

Respond with the following format:
```yml
reasoning: >
    The table is about ...
keys: 
    - col1
    - col2
pk: col1 # Leave empty if no single-column primary key
```"""

        messages = [{"role": "user", "content": template}]
        
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(summary, dict):
            raise TypeError("summary must be a dictionary")
        
        required_keys = ['reasoning', 'keys']
        for key in required_keys:
            if key not in summary:
                raise KeyError(f"summary is missing the '{key}' key")
        
        if not isinstance(summary['reasoning'], str):
            raise TypeError("summary['reasoning'] must be a string")
        
        if summary['keys'] is None:
            summary['keys'] = []
            
        if not isinstance(summary['keys'], list):
            raise TypeError("summary['keys'] must be a list")
        if 'pk' in summary and not isinstance(summary['pk'], str) and summary['pk'] is not None:
            raise TypeError("summary['pk'] must be a string or None")

        summary["keys"] = [key for key in summary["keys"] if key]

        table_columns = list(self.para["data_project"].table_object[table_name].column_desc.keys())

        for key in summary['keys']:
            if key not in table_columns:
                raise ValueError(f"Key '{key}' is not a column in the table")

        if 'pk' in summary and summary['pk'] and summary['pk'] not in table_columns:
            raise ValueError(f"Primary key '{summary['pk']}' is not a column in the table")
        
        if 'pk' in summary and summary['pk'] and summary['pk'] not in summary['keys']:
            summary['keys'].append(summary['pk'])
        
        return summary
    
    
    def run_but_fail(self, extract_output, use_cache=True):
        table_name, sample_df, table_summary, column_desc = extract_output
        
        return {"reasoning": "fail to identify keys",
                "keys": [],
                "pk": None }
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        callback(summary) 
        
        
class ConnectPKFKTable(ListNode):
    default_name = 'Connect PK FK Table'
    default_description = 'This connects PK FK for the given table'

    def extract(self, item):
        clear_output(wait=True)
        display(HTML(f'{running_spinner_html} Connecting PK FK ...'))
        
        data_project = self.para["data_project"]
        key_df = pd.read_json(self.get_sibling_document('Refine Primary Key'), orient="split")
        
        tables_to_process = []
        database_desc = ""
        
        pk_to_column = {}
        
        key_df = key_df.sort_values(by='Table')
        
        for idx, row in key_df.iterrows():
            table_name = row["Table"]
            pk = row["Primary Key"]
            
            if pk and pk != "":
                table_object = data_project.table_object[table_name]
                table_desc = f"'{table_name}':\n"
                column_desc = f"pk:\n" 
                column_desc += indent_paragraph(table_object.print_column_desc(columns=[pk],
                                                             show_category=True, 
                                                             show_unique=False, 
                                                             show_pattern=True))
                table_desc += indent_paragraph(column_desc) + "\n"
                database_desc += table_desc + "\n"
                
                pk_to_column[table_name] = pk

        if database_desc == "":
            return tables_to_process
        
        for idx, row in key_df.iterrows():
            table_name = row["Table"]
            keys = row["Keys"]
            pk = row["Primary Key"]
            
            keys = [key for key in keys if key != pk]
            keys.sort()
            
            if len(keys) == 0:
                continue
            
            table_object = data_project.table_object[table_name]
            
            current_table_desc = f"\nCurrent table {table_name}:\n"
            keys_desc = "fks:\n"
            keys_desc += indent_paragraph(table_object.print_column_desc(columns=keys,
                                                         show_category=True, 
                                                         show_unique=True, 
                                                         show_pattern=True))
            current_table_desc += indent_paragraph(keys_desc) + "\n"
            
            full_desc = database_desc + current_table_desc
            
            tables_to_process.append((table_name, keys, pk, full_desc, pk_to_column))
        
        self.progress = show_progress(max_value=len(tables_to_process), value=0)
        return tables_to_process

    def run(self, extract_output, use_cache=True):
        table_name, keys, pk, database_desc, pk_to_column = extract_output
        
        template = f"""You have a database, with the following tables and their keys:
{database_desc}

For the table '{table_name}', we need to match, for each foreign key, the corresponding primary key.
The primary key should be in another table and the column name should be semantically similar. 
The patterns should also be similar (but not necessarily the same).

Now, respond with the following format:
```yml
reasoning: >-
    The foreign keys in '{table_name}' are about ... The potential primary keys are ...
fks:
    - key: key_name
      pk_table: primary_key_table # Leave empty if no match
      pk_column: primary_key_column # Leave empty if no match
```"""

        messages = [{"role": "user", "content": template}]
        
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(summary, dict):
            raise TypeError("summary must be a dictionary")
        
        required_keys = ['reasoning', 'fks']
        for key in required_keys:
            if key not in summary:
                raise KeyError(f"summary is missing the '{key}' key")
        
        if not isinstance(summary['reasoning'], str):
            raise TypeError("summary['reasoning'] must be a string")
        
        for fk in summary['fks']:
            if not isinstance(fk, dict):
                raise TypeError(f"Each foreign key should be a dictionary: {fk}")
            required_fk_keys = ['key', 'pk_table', 'pk_column']
            for key in required_fk_keys:
                if key not in fk:
                    raise KeyError(f"Foreign key is missing the '{key}' key: {fk}")
            
            if not isinstance(fk['key'], str) or fk['key'] == '':
                raise ValueError(f"'key' must be a non-empty string: {fk}")
            
            if fk['key'] not in keys:
                raise ValueError(f"Foreign key '{fk['key']}' is not in the keys list for table '{table_name}'")
                       
            for pk_key in ['pk_table', 'pk_column']:
                if fk[pk_key] == '':
                    fk[pk_key] = None
                elif not isinstance(fk[pk_key], (str, type(None))):
                    raise TypeError(f"'{pk_key}' must be a string or None: {fk}")
            
            if fk['pk_table'] is not None:
                if fk['pk_table'] not in pk_to_column:
                    raise ValueError(f"Primary key table '{fk['pk_table']}' is not in pk_to_column dictionary")
                
                if fk['pk_column'] != pk_to_column[fk['pk_table']]:
                    fk['pk_column'] = pk_to_column[fk['pk_table']]
            
        self.progress.value += 1
        
        return {table_name: summary}

    def run_but_fail(self, extract_output, use_cache=True):
        table_name, _, _, _, _ = extract_output
        return {table_name: {"reasoning": "Failed to run", "fks": []}}

    def merge_run_output(self, run_outputs):
        return {k: v for d in run_outputs for k, v in d.items()}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        data_project = self.para["data_project"]
        
        query_widget = self.item["query_widget"]
        
        dropdown = create_explore_button(query_widget, 
                                    table_name=list(data_project.table_pipelines.keys()),
                                    logical_to_physical=data_project.table_pipelines)
        
        data = {
            'Table[PK]': [],
            'Table[FK]': [],
        }
        
        pk_to_fk = {}
        
        for table, summary in run_output.items():
            for fk in summary['fks']:
                pk_table = fk['pk_table']
                pk_column = fk['pk_column']
                fk_table = table
                fk_column = fk['key']
                
                if pk_table is None or pk_column is None:
                    continue
                
                pk_key = f"{pk_table}[{pk_column}]"
                fk_value = f"{fk_table}[{fk_column}]"
                
                if pk_key not in pk_to_fk:
                    pk_to_fk[pk_key] = []
                pk_to_fk[pk_key].append(fk_value)
        
        for pk, fk_list in pk_to_fk.items():
            data['Table[PK]'].append(pk)
            data['Table[FK]'].append(fk_list)
        
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True]
        reset = True
        editable_list = {
            'Table[FK]': {},
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset, editable_list=editable_list)
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                document = df.to_json(orient="split")
                
                for idx, row in df.iterrows():
                    if row['Table[PK]'] == "":
                        continue
                    
                    pk_table, pk_column = row['Table[PK]'].strip('[]').split('[')
                    
                    for fk in row['Table[FK]']:
                        print(fk)
                        fk_table, fk_column = fk.strip('[]').split('[')
                        data_project.add_foreign_key_to_primary_key(fk_table, fk_column, pk_table, pk_column)
                
                callback(document)
        
        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(grid)
        display(next_button)
        

def get_table_to_entities_mapping(pk_fk_map, primary_table_to_entity):
    def get_entity_name(table):
        return primary_table_to_entity.get(table, table)

    table_to_entities = {table: [entity] for table, entity in primary_table_to_entity.items()}

    for (pk_table, _), fk_list in pk_fk_map.items():
        pk_entity = get_entity_name(pk_table)
        
        for (fk_table, _) in fk_list:
            if fk_table not in table_to_entities:
                table_to_entities[fk_table] = []
            
            if pk_entity not in table_to_entities[fk_table]:
                table_to_entities[fk_table].append(pk_entity)

    return table_to_entities




class RelationUnderstanding(Node):
    default_name = 'Relation Understand'
    default_description = 'This understands the relation of the table'

    def extract(self, item):
        clear_output(wait=True)
        
        idx = self.para["table_idx"]
        total = self.para["table_total"]
        table_name = self.para["table_name"]
        entities = self.para["entities"]
        con = self.item["con"]

        display(HTML(f'{running_spinner_html} Reading relation in {table_name} ...'))
        create_progress_bar_with_numbers(3, doc_steps)

        self.input_item = item

        show_progress(max_value=total, value=idx)

        data_project = self.para["data_project"]
       
        table_object = data_project.table_object[table_name]
        table_summary = table_object.table_summary
        table_pipeline = data_project.table_pipelines[table_name]
        
        schema_only = self.para.get("cocoon_catalog_options", {}).get("schema_only", False)
        
        if schema_only:
            column_desc = table_object.print_column_desc(show_category=True, show_unique=True, show_pattern=True)
            return table_name, entities, column_desc, table_summary, schema_only
        else:
            sample_size = 5
            sample_df = run_sql_return_df(con, f'SELECT * FROM {table_pipeline} LIMIT {sample_size}')
            sample_df = sample_df.applymap(truncate_cell)
            return table_name, entities, sample_df, table_summary, schema_only
    
    def run(self, extract_output, use_cache=True):
        table_name, entities, data, table_summary, schema_only = extract_output
        
        more_than_one_entity = len(entities) > 1

        if schema_only:
            template = f"""You have a table '{table_name}': {table_summary}
It has the following column descriptions:
{data}

It is about {len(entities)} entities: '{', '.join(entities)}'
"""
        else:
            template = f"""You have a table '{table_name}': {table_summary}
It has the following sample rows:
{data.to_csv(index=False, quoting=1)}

It is about {len(entities)} entities: '{', '.join(entities)}'
"""

        template += """
First, describe the relation between the entities:
(1). Include all entities
(2). Simple sentence about how they are related (<20 words)
(3). Be descriptive and specific (not just 'relates', 'has', ...)
"""
        if more_than_one_entity:
            template += "Then, pick a name for the relation."

        template += """

Return in the following format:
```yml
relation_desc: >-
    This stores the Items that are ordered by Customers
"""
        if more_than_one_entity:
            template += "relation_name: CustomerOrderItems"
        template += "```"

        messages = [{"role": "user", "content": template}]
        response =  call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        summary['entities'] = entities
        
        if not isinstance(summary, dict):
            raise TypeError("summary must be a dictionary")

        if 'relation_desc' not in summary:
            raise KeyError("summary is missing the 'relation_desc' key")
        if not isinstance(summary['relation_desc'], str):
            raise TypeError("summary['relation_desc'] must be a string")

        if more_than_one_entity:
            if 'relation_name' not in summary:
                raise KeyError("summary is missing the 'relation_name' key for multiple entities")
            if not isinstance(summary['relation_name'], str):
                raise TypeError("summary['relation_name'] must be a string")

        summary['entities'] = entities
        
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        table_name, entities, sample_df, table_summary = extract_output
        return {"relation_desc": table_summary,
                "relation_name": "".join(entities)}

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
        

def build_story_yml_dict(relation_df, entity_df, story_summary, group_df):
    yml_dict = OrderedDict()

    yml_dict['groups'] = [
        OrderedDict([
            ('group_name', row['Group Name']),
            ('group_summary', row['Group Summary']),
            ('tables', row['Tables']),
            *([('join_info', row['Join Info'])] if pd.notna(row['Join Info']) and row['Join Info'] != '' else [])
        ])
        for _, row in group_df.iterrows()
    ]

    yml_dict['entities'] = [
        OrderedDict([
            ('entity_name', row['Entity Name']),
            ('entity_description', row['Entity Description']),
            ('table_name', row['Table']),
            ('primary_key', row['Primary Key']),
        ])
        for _, row in entity_df.iterrows()
    ]

    yml_dict['relations'] = [
        OrderedDict([
            *([('relation_name', row['Relation Name'])] if pd.notna(row['Relation Name']) and row['Relation Name'] != '' else []),
            ('relation_description', row['Relation Description']),
            ('table_name', row['Table']),
            ('entities', row['Entities']),
        ])
        for _, row in relation_df.iterrows()
    ]

    yml_dict['story'] = story_summary

    return yml_dict



class DocumentProject(Node):
    default_name = 'Document Project'
    default_description = 'This documents the project'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_catalog_options'), dict) and 
            self.para['cocoon_catalog_options'].get('catalog_html') is False):
            callback({})
            return
        
        clear_output(wait=True)
        
        display(HTML(f"🥳 We have generated the report!"))
        
        dbt_directory = self.para["dbt_directory"]
        dbt_project_name = self.para["dbt_name"]
        data_project = self.para["data_project"]
        con = self.item["con"]
        
        model_html = get_project_html(model_directory=dbt_directory, con=con, project_name=data_project, sample_size=5)
        
        display(HTML(wrap_in_iframe(model_html, width=1200, height=1200)))
        labels = ["HTML"]
        file_names = [os.path.join(dbt_directory, "model.html")]
        contents = [model_html]
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
        save_files_click(save_button) 
        

        
def read_source_file(sources_file_path):
    if not file_exists(sources_file_path):
        return None, None, None
    
    try:
        yml_content = yaml.safe_load(read_from(sources_file_path))
        
        if not yml_content or 'sources' not in yml_content:
            return None, None, None
        
        source = yml_content['sources'][0]
        database = source.get('database')
        schema = source.get('schema')
        tables = source.get('tables', [])
        
        
        if len(tables) > 0 and isinstance(tables[0], dict):
            table_names = [table['name'] for table in tables]
        else:
            table_names = tables
        
        return database, schema, table_names
    
    except Exception as e:
        return None, None, None

def read_data_project_from_dir(directory, con=None, database=None, schema=None, source_only=False, create=True):

    data_project = DataProject(directory)
    dbt_directory = os.path.join(directory, "models")

    data_project.name = os.path.basename(os.path.normpath(directory))

    mode = "WITH_VIEW"
    
    seeds_directory = os.path.join(directory, "seeds") 
    
    if file_exists(seeds_directory):
        files = list_files_in(seeds_directory)
        tables = []

        for file in files:
            if file.endswith(".csv"):
                file_path = f"{seeds_directory}/{file}"
                table_name = file.split(".")[0]
                table_name = clean_table_name(table_name)
                df = pd.read_csv(file_path)
                df.columns = [clean_column_name(col) for col in df.columns]
                if create and con:
                    create_table_from_df(df, con, table_name=table_name, database=database, schema=schema)
                tables.append(table_name)
    
    sources_file_path = os.path.join(dbt_directory, "sources.yml")
    source_database, source_schema, source_tables = read_source_file(sources_file_path)
 
    if source_only:
        for table_name in source_tables:
            data_project.add_table(table_name, get_table_schema(con, table_name, database=source_database, schema=source_schema))

        return data_project
    
    
    partitions = []
    if file_exists(os.path.join(dbt_directory, "partition")):
        for file_name in list_files_in(os.path.join(dbt_directory, "partition")):
            if file_name.endswith(".yml"):
                partitions.append(file_name[:-4])
                
    for partition in partitions:
        file_path = os.path.join(dbt_directory, "partition", f"{partition}.yml")
        yml_data = yaml.safe_load(read_from(file_path))
        table_object = PartitionTable()
        table_object.read_attributes_from_dbt_schema_yml(yml_data)
        table_name = table_object.table_name
        data_project.add_table_project(table_object)
        data_project.add_table(table_name, table_object.columns)
        updated_partitions = [rename_for_stg(table_name) for table_name in table_object.partitions]
        data_project.partition_mapping[table_name] = updated_partitions

    stage_tables = []
    if file_exists(os.path.join(dbt_directory, "stage")):
        for file_name in list_files_in(os.path.join(dbt_directory, "stage")):
            if file_name.endswith(".yml"):
                stage_tables.append(file_name[:-4])
                
    for stage_table in stage_tables:
        new_table_name = stage_table
        
        table_pipeline = TransformationSQLPipeline(steps=[], edges=[])
        
        sql_file_path = os.path.join(dbt_directory, "stage", f"{stage_table}.sql")
        if file_exists(sql_file_path):
            sql_query = read_from(sql_file_path)
            sql_step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con, database=database, schema=schema)
            table_pipeline.steps.append(sql_step)
            
            if create and con:
                table_pipeline.materialize(con=con, database=database, schema=schema, mode=mode)
        
        data_project.table_pipelines[new_table_name] = table_pipeline
        
        file_path = os.path.join(dbt_directory, "stage", f"{stage_table}.yml")
        yml_data = yaml.safe_load(read_from(file_path))
        table_object = Table()
        table_object.read_attributes_from_dbt_schema_yml(yml_data)
        data_project.add_table_project(table_object)
        
        skip_table = False
        for partition, tables in data_project.partition_mapping.items():
            if new_table_name in tables:
                skip_table = True
                break
        
        if not skip_table:
            data_project.add_table(new_table_name, table_object.columns)
            
    snapshot_tables = []
    if file_exists(os.path.join(dbt_directory, "snapshot")):
        for file_name in list_files_in(os.path.join(dbt_directory, "snapshot")):
            if file_name.endswith(".yml"):
                snapshot_tables.append(file_name[:-4])
                
    for snapshot_table in snapshot_tables:
        new_table_name = snapshot_table
        file_path = os.path.join(dbt_directory, "snapshot", f"{snapshot_table}.sql")
        sql_query = read_from(file_path)
        sql_step = SQLStep(table_name=new_table_name, sql_code=sql_query, con=con, database=database, schema=schema)
        table_pipeline = TransformationSQLPipeline(steps = [sql_step], edges=[])
        
        if create and con:
            table_pipeline.materialize(con=con, database=database, schema=schema, mode=mode)
        data_project.table_pipelines[new_table_name] = table_pipeline
           
        yml_data = yaml.safe_load(read_from(os.path.join(dbt_directory, "snapshot", f"{snapshot_table}.yml")))
        
        table_object = Table()
        table_object.read_attributes_from_dbt_schema_yml(yml_data)
        data_project.add_table_project(table_object)
        
        old_table_name = yml_data.get("cocoon_meta", {}).get("scd_base_table", None)
        data_project.history_table[new_table_name] = old_table_name
        scd_columns = yml_data.get("cocoon_meta", {}).get("scd_columns", [])
        data_project.scd_columns[old_table_name] = scd_columns
        
        if old_table_name:
            del data_project.tables[old_table_name]
        
        skip_table = False
        for partition, tables in data_project.partition_mapping.items():
            if new_table_name in tables:
                skip_table = True
                break
        
        if not skip_table:
            data_project.add_table(new_table_name, table_object.columns)

    join_yml_content = ""
    if file_exists(os.path.join(dbt_directory, "join", "cocoon_join.yml")):
        join_yml_path = os.path.join(dbt_directory, "join", "cocoon_join.yml")
        join_yml_content = read_from(join_yml_path)
            
    if join_yml_content:
        data_project.build_foreign_key_from_join_graph_yml(join_yml_content)
        
        join_graph_data = yaml.safe_load(join_yml_content)
        
        if 'join_graph' in join_graph_data:
            for item in join_graph_data['join_graph']:
                table_name = item.get('table_name')
                primary_key = item.get('primary_key')
                foreign_keys = item.get('foreign_keys')
                time_keys = item.get('time_keys')
                
                if table_name:
                    data_project.key[table_name] = {
                        "primary_key": primary_key,
                        "foreign_keys": foreign_keys,
                        "time_keys": time_keys
                    }
                
    er_yml_content = ""
    if file_exists(os.path.join(dbt_directory, "er", "cocoon_er.yml")):
        er_file_path = os.path.join(dbt_directory, "er", "cocoon_er.yml")
        er_yml_content = read_from(er_file_path)
            
    if er_yml_content:
        data_project.build_er_story_from_yml(er_yml_content)

    data_project.generate_text_join()

    return data_project


class DecideVariantTypes(Node):
    default_name = 'Decide Variant Types'
    default_description = 'This allows users to analyze and decide how to handle VARIANT data types in Snowflake.'
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        display(HTML(f'{running_spinner_html} Analyzing VARIANT data types ...'))
        
        create_progress_bar_with_numbers(0, doc_steps)
        self.progress = show_progress(1)
        con = self.item["con"]
        table_pipeline = self.para["table_pipeline"]
        database_name = get_database_name(con)
        
        if database_name != "Snowflake":
            callback({})
            return
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('parse_variant_types') is False):
            callback({})
            return
        
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        variant_columns = []
        
        for col in columns:
            if get_reverse_type(schema[col], database_name) == 'VARIANT':
                variant_columns.append(col)
                
        if not variant_columns:
            callback({})
            return
            
        document = {}
        with_context = table_pipeline.get_codes(mode="WITH")
        
        for col in variant_columns:
            document[col] = OrderedDict()
            
            query = f"""
            SELECT 
                TYPEOF(t."{col}") as DATA_TYPE,
                COUNT(*) as TYPE_COUNT
            FROM 
                {table_pipeline} t
            GROUP BY 
                TYPEOF(t."{col}")
            ORDER BY 
                TYPE_COUNT DESC"""
            
            query = with_context + query
            result_df = run_sql_return_df(con, query)
            document[col]["variant_types"] = result_df['DATA_TYPE'].tolist()
            
            if 'OBJECT' in document[col]["variant_types"]:
                unique_keys_query = f"""
                SELECT COUNT(DISTINCT f.VALUE) AS UNIQUE_KEY_COUNT
                FROM {table_pipeline} t,
                LATERAL FLATTEN(OBJECT_KEYS(t."{col}")) f
                WHERE IS_OBJECT(t."{col}")
                """
                unique_keys_query = with_context + unique_keys_query
                unique_keys_result = run_sql_return_df(con, unique_keys_query)
                unique_key_count = unique_keys_result['UNIQUE_KEY_COUNT'].iloc[0]
                
                if unique_key_count <= 100:
                    keys_query = f"""
                    SELECT DISTINCT f.VALUE::VARCHAR as KEY
                    FROM {table_pipeline} t,
                    LATERAL FLATTEN(OBJECT_KEYS(t."{col}")) f
                    WHERE IS_OBJECT(t."{col}")
                    """
                    keys_query = with_context + keys_query
                    keys_result = run_sql_return_df(con, keys_query)
                    
                    document[col]["object_keys"] = keys_result['KEY'].tolist()
        
        table_object = self.para["table_object"]
        table_object.variant = document
        callback(document)
        
def rename_for_stg(old_table_name):
    return "stg_" + old_table_name.replace("src_", "")


class RefinePK(Node):
    default_name = 'Refine Primary Key'
    default_description = 'This node refines the primary key for the given table and clusters related tables'
    
    def extract(self, item):
        clear_output(wait=True)

        create_progress_bar_with_numbers(2, model_steps)
        display(HTML(f'{running_spinner_html} Clustering Keys ...'))
        
        key_df = pd.read_json(self.get_sibling_document('Decide Keys For All'), orient="split")
        
        pk_df = key_df[key_df['Primary Key'] != ""]
        pk_df = pk_df.sort_values(by=['Primary Key', 'Table'])

        data_project = self.para["data_project"]
        
        pk_table_description = ""
        for idx, row in pk_df.iterrows():
            table = row['Table']
            pk = row['Primary Key']
            keys = row['Keys']
            fks = [key for key in keys if key != pk]
            table_object = data_project.table_object[table]
            table_summary = table_object.table_summary
            
            pk_table_description += f"{table}: {table_summary}\n   Primary key: {pk}\n  Foreign keys: {','.join(fks)}\n"
        
        return pk_df, pk_table_description
    
    def run(self, extract_output, use_cache=True):
        pk_df, pk_table_description = extract_output
        
        if pk_df.empty:
            return self.run_but_fail(extract_output)
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_catalog_options'), dict) and 
            self.para['cocoon_catalog_options'].get('cluster_pk') is False):
            return self.run_but_fail(extract_output)
        
        template = f"""You have the following tables with primary keys:
{pk_table_description}

Your task is to cluster tables that share the same primary key (pk):
Check if these tables are joinable on the primary key (NOT on any of their foreign key).

For example, 'UserInformation' has pk 'userid' and 'UserAddress' has pk 'UserIdentifier'. 
They are joinable on 'userid' and 'UserIdentifier'. After join, they show a comprehensive user profile.

However, 'Order'  has pk 'orderid' and fk 'userid'. It doesn't share pk because it joins on fk.

Now, identify clusters of tables that are joinable on the their primary key.
Return your analysis in the following YAML format:

```yml
reasoning: >-
    Explanation of your clustering process and decisions...

# only for clusters with > 1 table
clusters:
  - cluster_name: name_of_cluster
    explanation: They are joinable on the primary key, and together they form a comprehensive view of ...
    tables: 
      - table1
      - table2
  - cluster_name: another_cluster
    ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.safe_load(yml_code)
        
        if 'clusters' not in summary or not isinstance(summary['clusters'], list):
            summary['clusters'] = []
        
        all_tables = set(pk_df['Table'])
        clustered_tables = set()
        
        for cluster in summary['clusters']:
            if 'tables' not in cluster:
                raise ValueError(f"Cluster {cluster.get('cluster_name', 'unnamed')} is missing 'tables' key")
            
            for table in cluster['tables']:
                if table not in all_tables:
                    raise ValueError(f"Table '{table}' in cluster {cluster.get('cluster_name', 'unnamed')} does not exist in the original data")
                
                if table in clustered_tables:
                    raise ValueError(f"Table '{table}' appears in multiple clusters")
                
                clustered_tables.add(table)
            
        return summary
    
    def run_but_fail(self, extract_output):
        return {
            "reasoning": "Failed to cluster tables.",
            "clusters": []
        }

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output
        
        original_df = pd.read_json(self.get_sibling_document('Decide Keys For All'), orient="split")
        
        cluster_data = []
        for cluster in summary.get("clusters", []):
            cluster_name = cluster['cluster_name']
            tables = cluster['tables']
            explanation = cluster['explanation']
            
            cluster_data.append({
                'Cluster Name': cluster_name,
                'Tables': tables,
                'Explanation': explanation,
                'Endorsed': True
            })
        
        df = pd.DataFrame(cluster_data)
        
        if len(df) == 0:
            callback(original_df.to_json(orient="split"))
            return
        
        editable_columns = [True, True, True, True]
        reset = True
        editable_list = {
            'Tables': {}
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset, editable_list=editable_list)
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                updated_df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                
                updated_df = updated_df[updated_df['Endorsed']]
                
                table_positions = {}
                for _, row in updated_df.iterrows():
                    tables = row['Tables']
                    for i, table in enumerate(tables):
                        table_positions[table] = i
                
                for idx, row in original_df.iterrows():
                    table_name = row['Table']
                    if table_name in table_positions:
                        if table_positions[table_name] > 0:
                            original_df.at[idx, 'Primary Key'] = ""
                
                document = original_df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(HTML("🤓 We have clustered tables based on shared primary keys. Please review and refine the clusters:"))
        display(grid)
        display(next_button)
        
def truncate_cell(value, max_length=200):
    if pd.api.types.is_numeric_dtype(value):
        return value
    
    str_value = str(value)
    if len(str_value) > max_length:
        return str_value[:max_length] + '...'
    return value
    

def truncate_value(value, max_length=200):
    str_value = str(value)
    if len(str_value) > max_length:
        truncated = str_value[:max_length] + '...'
        return f"{repr(truncated)} # truncated"
    else:
        return repr(value)
    
class SelectSchema(Node):
    default_name = 'Select Schema'
    default_description = 'This step allows you to select the database schema.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        if "con" in self.input_item and self.input_item["con"]:
            con = self.input_item["con"]
        else:
            callback({})
            return

        try:
            database, _ = get_default_database_and_schema(con)
        except:
            database, schema = "", ""
        
        schema = "cocoon_schema"

        database_input = widgets.Text(value=database, description="Database:")
        schema_input = widgets.Text(value=schema, description="Schema:")
        test_button = widgets.Button(description="Test Access", button_style='info', icon='play')
        output = widgets.HTML()

        def on_test_button_click(b):
            with self.output_context():
                database = database_input.value
                schema = schema_input.value
                if not database or not schema:
                    output.value = "<div style='color: red;'>Both database and schema must be specified.</div>"
                    return
                else:
                    success, result = create_schema_and_objects(con, database, schema)
                    output.value = result
                    if success:
                        self.para["database"] = database
                        self.para["schema"] = schema
                        callback({"database": database, "schema": schema})

        test_button.on_click(on_test_button_click)


        def on_next_button_click(b):
            with self.output_context():
                database = database_input.value
                schema = schema_input.value
                
                if not database or not schema:
                    output.value = "<div style='color: red;'>Both database and schema must be specified before proceeding.</div>"
                    return

                self.para["database"] = database
                self.para["schema"] = schema
                
                callback({"database": database, "schema": schema})


        if "database" in self.para and "schema" in self.para and self.para["database"] and self.para["schema"]:
            database_input.value = self.para["database"]
            schema_input.value = self.para["schema"]
            on_test_button_click(test_button)
            return

        create_progress_bar_with_numbers(0, model_steps)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_test_button_click(test_button)
            return 
        
        display(HTML("""
        <div style='line-height: 1.2;'>             
            <h2>🛡️ Access Control</h2>
            🫡 <em>Cocoon <b>only</b> writes under the database and schema you specify:</em><br>
        </div>
        """))
        
        display(VBox([database_input, schema_input]), VBox([test_button, output]))
        

class RefineRelations(ListNode):
    default_name = 'Refine Relations'
    default_description = 'This refines relations by identifying and renaming duplicate relation names'

    def extract(self, item):
        clear_output(wait=True)
        display(HTML(f'{running_spinner_html} Refining Relations ...'))

        relation_df = pd.read_json(self.get_sibling_document('Relation Understand For All'), orient="split")
        
        data_project = self.para["data_project"]
        
        grouped_relations = relation_df.groupby('Relation Name')
        
        relations_to_refine = []
        for name, group in grouped_relations:
            
            if not name:
                continue
            
            if len(group) > 1:
                group_info = []
                for _, row in group.iterrows():
                    table_name = row['Table']
                    table_object = data_project.table_object[table_name]
                    table_summary = table_object.table_summary
                    group_info.append({
                        'Table': table_name,
                        'Entities': row['Entities'],
                        'Relation Description': row['Relation Description'],
                        'Table Summary': table_summary
                    })
                relations_to_refine.append((name, group_info))
        
        self.progress = show_progress(max_value=len(relations_to_refine), value=0)
        return relations_to_refine

    def run(self, extract_output, use_cache=True):
        relation_name, group_info = extract_output
        
        tables_info = "\n".join([f"Table: {info['Table']}\nEntities: {', '.join(info['Entities'])}\nDescription: {info['Relation Description']}\nTable Summary: {info['Table Summary']}" 
                                 for info in group_info])
        
        template = f"""You have multiple relations with the same name '{relation_name}':

{tables_info}

Please suggest new names for these relations to highlight their differences. 
The new names should be descriptive and specific, capturing the unique aspects of each relation.
Also, provide an updated relation description for each table.

Return your response in the following format:
```yml
reasoning: >-
    The relations differ in... We can rename them to highlight...

renamed_relations:
    - reasoning: brief explanation for the new name and description
      original_table: table_name
      new_relation_name: new_name
      new_relation_description: updated description
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        self.progress.value += 1
        
        return summary

    def run_but_fail(self, extract_output, use_cache=True):
        relation_name, group_info = extract_output
        return {
            "reasoning": "Failed to refine relations",
            "renamed_relations": [
                {
                    "original_table": info['Table'],
                    "new_relation_name": relation_name,
                    "new_relation_description": info['Relation Description'],
                    "reasoning": "Failed to generate new name and description"
                } for info in group_info
            ]
        }

    def merge_run_output(self, run_outputs):
        return run_outputs

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        self.progress.value += 1
        
        original_df = pd.read_json(self.get_sibling_document('Relation Understand For All'), orient="split")
        
        if len(original_df) == 0:
            callback(original_df.to_json(orient="split"))
            return
        
        for refinement in run_output:
            for relation in refinement['renamed_relations']:
                original_df.loc[original_df['Table'] == relation['original_table'], 'Relation Name'] = relation['new_relation_name']
                original_df.loc[original_df['Table'] == relation['original_table'], 'Relation Description'] = relation['new_relation_description']
        
        editable_columns = [False, True, True, True]
        reset = True
        lists = ['Entities']
        
        grid = create_dataframe_grid(original_df, editable_columns, reset=reset, lists=lists)
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                relation_df = grid_to_updated_dataframe(grid, reset=reset, lists=lists)
                
                document = relation_df.to_json(orient="split")
                
                relation_df = relation_df[relation_df['Relation Name'] != ""]
                
                duplicate_names = relation_df['Relation Name'][relation_df['Relation Name'].duplicated()].tolist()
                
                if duplicate_names:
                    display(HTML("⚠️ Error: Duplicate relation names: " + ", ".join(duplicate_names) + "<br> 😊 Please rename the relations to highlight the difference"))
                    return
                
                callback(document)
        
        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(HTML("🤓 We have described the relations. Please review and make any necessary adjustments:"))
        display(grid)
        display(next_button)
        
def get_tags(input_string):
    tag_map = {
        "-- Projection": "Projection",
        "-- Deduplication": "Deduplication",
        "-- NULL Imputation": "Cleaning",
        "-- Clean unusual": "Cleaning",
        "-- Handling missing": "Missing",
        "-- Column Type Casting": "Casting",
        "-- Trim Leading": "Cleaning",
        "-- Rename": "Renaming",
        "-- FD": "Cleaning"
    }
    
    return set(tag for key, tag in tag_map.items() if key in input_string)

class DBTProjectConfigAndRead(Node):
    default_name = 'DBT Project Configuration and Read'
    default_description = 'This step allows you to specify an existing DBT project directory, preview, and read the project.'
    create = True

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        dbt_directory_input = widgets.Text(
            description='Project Directory:',
            value=self.para.get("dbt_directory", ""),
            placeholder="/path/to/your/dbt/project",
            style={'description_width': 'initial'},
            layout={'width': '50%'}
        )

        preview_button = widgets.Button(description="Preview Project", button_style='info', icon='eye')
        submit_button = widgets.Button(description="Submit", button_style='success', icon='check')
        submit_button.layout.display = 'none'

        output = widgets.Output()

        create = self.class_para.get("create", self.create)
        
        def read_project(directory):
            with self.output_context():
                if not directory:
                    print("Please specify a directory.")
                    return None
                
                display(HTML(f"{running_spinner_html} Creating temp views ..."))
                
                try:
                    con = self.item["con"]
                    database = self.para.get("database", None)
                    schema = self.para.get("schema", None)
                    
                    data_project = read_data_project_from_dir(directory, con, database=database, schema=schema, create=create)
                    
                    self.para["data_project"] = data_project
                    self.para["dbt_directory"] = directory
                    
                    return data_project
                except Exception as e:
                    print(f"Error reading project: {str(e)}")
                    return None

        def on_preview_click(b):
            with self.output_context():
                directory = dbt_directory_input.value
                data_project = read_project(directory)
                if data_project:
                    data_project.display_story_multiple_page()
                    submit_button.layout.display = 'block'

        def on_submit_click(b):
            with self.output_context():
                directory = dbt_directory_input.value
                callback({"dbt_directory": directory})

        preview_button.on_click(on_preview_click)
        submit_button.on_click(on_submit_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "dbt_directory" in self.para:
                dbt_directory_input.value = self.para["dbt_directory"]
                on_preview_click(preview_button)
                
                if submit_button.layout.display != 'none':
                    on_submit_click(submit_button)
                return

        create_progress_bar_with_numbers(0, model_steps)
        display(HTML("<h3>📂 Specify Catalog Directory</h3><em>Built by Cocoon</em>"))
        display(dbt_directory_input)
        display(preview_button)
        display(submit_button)
        display(output)
        
        
class SimpleBusinessQuestionNode(Node):
    default_name = 'Business Question Input'
    default_description = 'This step allows you to explore the data and input a business question.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        data_project = self.para["data_project"]

        question_input = widgets.Textarea(
            placeholder='Enter your business question here...',
            value=self.para.get("business_question", ""),
            layout={'width': '96%', 'height': '100px'}
        )
        question_input.add_class('modern-textarea')
        
        submit_button = widgets.Button(description="Submit", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                business_question = question_input.value.strip()

                if not business_question:
                    print("Please enter a business question before submitting.")
                    return
                
                if "chatui" in self.para:
                    escaped = escape_html(business_question)
                    self.para["chatui"].add_message("You", "🙂 " + escaped)

                self.para["business_question"] = business_question

                callback({"business_question": business_question})

        submit_button.on_click(on_button_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "business_question" in self.para:
                on_button_click(submit_button)
                return
        
        create_progress_bar_with_numbers(1, model_steps)

        display(HTML("<h4>🗃️ Available Tables:</h4>"))
        query_widget = self.item["query_widget"]
        dropdown = create_explore_button(query_widget, 
                                         table_name=list(data_project.table_pipelines.keys()),
                                         logical_to_physical=data_project.table_pipelines)
        

        display(HTML("<h4>📖 Story Behind Tables:</h4>"))
        data_project.display_story_multiple_page()

        display(HTML("<h4>❓ Now, enter Your Business Question:</h4>"))
        display(VBox([question_input]))
        display(submit_button)
        

    
class BusinessQuestionEntityRelationNode(Node):
    default_name = 'Business Question Entity and Relation Analysis'
    default_description = 'This node analyzes the entities and relations related to the business questions.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"🔍 Analyzing the business question..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]
        business_question = self.para["business_question"]
        story_list = data_project.get_story_list()
        
        return data_project, business_question, story_list

    def run(self, extract_output, use_cache=True):
        data_project, business_question, story_list = extract_output
        
        story_desc = "\n".join([f"{i+1}. '{item.get('Name') or item.get('name')}': {item.get('Description') or item.get('description')}" 
                        for i, item in enumerate(story_list)])

        template = f"""Given the following question:
'{business_question}'

And the following descriptions of steps in a story-telling fashion:
{story_desc}

First reason about how the question fits into the story
Then provide the step names that contains information to answer the question

Return your analysis in the following format:
```yml
reasoning: >-
    The question means... In the story, it is asking for ...
related_steps:
    - name: xx
      explanation: This is related because ...
    - name: xx
      ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'related_steps' not in analysis or not isinstance(analysis['related_steps'], list):
            raise ValueError("Analysis should contain a 'related_steps' key with a list value")
        
        for relation in analysis['related_steps']:
            if not isinstance(relation, dict) or 'name' not in relation or 'explanation' not in relation:
                raise ValueError("Each related relation should be a dictionary with 'name' and 'explanation' keys")
        
        return analysis

    def run_but_fail(self, extract_output, use_cache=True):
        data_project, business_question, story_list = extract_output
        
        default_analysis = {
            'reasoning': 'Unable to analyze the business question in relation to the story.',
            'related_steps': []
        }
        
        return default_analysis

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output
        data_project, business_question, story_list = extract_output
        
        data = {
            'Name': [],
            'Related': [],
            'Explanation': [],
        }
        
        related_steps = {relation['name'] for relation in analysis['related_steps']}
        for item in story_list:
            name = item.get('Name') or item.get('name')
            is_related = name in related_steps
            explanation = next((relation['explanation'] for relation in analysis['related_steps'] if relation['name'] == name), '')
            
            data['Name'].append(name)
            data['Related'].append(is_related)
            data['Explanation'].append(explanation if is_related else '')
            
        df = pd.DataFrame(data)
        
        editable_columns = [False, True, True]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset)
        
        relation_to_entities = {}
        for relation_info in data_project.relations.values():
            relation_name = relation_info['relation_name']
            if relation_name is not None:
                relation_to_entities[relation_name] = relation_info['entities']
        
        highlight_indices = [i for i, item in enumerate(story_list) if (item.get('Name') or item.get('name')) in related_steps]
        
        html_content = generate_er_diagram_html(relation_to_entities, story_list, highlight_indices)
        
        if "chatui" in self.para:
            
            yml_content = yaml.dump(story_list, default_flow_style=False)
            highlighted_yml_content = highlight_yml_only(yml_content)
            message = f"😎 <b>RAG from Cocoon</b>: Putting the question in context using Cocoon's ER story... {highlighted_yml_content}"
            
            if df['Related'].any():
                related_entities = ", ".join(df[df['Related']]['Name'].tolist())
                message += f"🤓 <b>We've found the related relations:</b> {related_entities}"
                message += html_content
                message += f"<b>Reasoning</b>: {analysis['reasoning']}<br>"
                
                self.para["chatui"].add_message("GenAI", message)
            else:
                message += f"🤔 <b>We can't find any related relation</b>.<br><b>Reasoning</b>:{analysis['reasoning']}"
                self.para["chatui"].add_message("GenAI", message)
                
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset)
                
                if not df['Related'].any():
                    print("Error: At least one relation must be marked as related to the business question.")
                
                document = {
                    'reasoning': analysis['reasoning'],                    
                    'related_steps': df.to_json(orient="split")
                }
                callback(document)
                
        next_button.on_click(on_button_click)
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return

        display(HTML(f"🙂 <b>Analysis:</b> {analysis['reasoning']}<br>"+\
                    html_content + f"<b>😎 Your feedback</b>"))
        display(grid)
        display(next_button)
        
class BusinessQuestionSQLFeasibility(Node):
    default_name = 'Business Question SQL Feasibility Check'
    default_description = 'This node evaluates whether a business question can be answered with SQL or if it is too ambiguous/abstract.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"🤔 Evaluating the feasibility of answering the business question with SQL..."))
        self.progress = show_progress(1)

        business_question = self.para["business_question"]
        
        return business_question

    def run(self, extract_output, use_cache=True):
        business_question = extract_output

        template = f"""Evaluate the following business question for its feasibility to be answered using SQL:

'{business_question}'

Consider the following guidelines:
If it's not a question, then it's not suitable for DQL SQL.
Questions with terms like "best" or "worst" without clear metrics are often ambiguous.
Questions about strategies or abstract concepts are typically not suitable for SQL queries.
Questions that needs to create table/set permission go beyond DQL.

Provide your analysis in the following format:
```yml
explanation: >-
    [Your detailed explanation here, discussing why the question is or isn't feasible for SQL]
sql_feasible: true/false
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'explanation' not in analysis or not isinstance(analysis['explanation'], str):
            raise ValueError("Analysis should contain an 'explanation' key with a string value")
        
        if 'sql_feasible' not in analysis or not isinstance(analysis['sql_feasible'], bool):
            raise ValueError("Analysis should contain an 'sql_feasible' key with a boolean value")
        
        return analysis

    def run_but_fail(self, extract_output, use_cache=True):
        default_analysis = {
            'explanation': 'Unable to evaluate the SQL feasibility of the business question.',
            'sql_feasible': False
        }
        
        return default_analysis

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output
        business_question = extract_output
        
        if not analysis['sql_feasible'] and "chatui" in self.para:
            message = f"🤔 <b>We can't run the query</b><br><b>Reason</b>: {analysis['explanation']}"
            self.para["chatui"].add_message("GenAI", message)
            
        display(HTML(f"❓ <b>Business Question:</b> {business_question}"))
        display(HTML(f"📊 <b>SQL Feasibility:</b> {'Yes' if analysis['sql_feasible'] else 'No'}"))
        display(HTML(f"🧠 <b>Explanation:</b> {analysis['explanation']}"))
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                document = {
                    'business_question': business_question,
                    'sql_feasible': analysis['sql_feasible'],
                    'explanation': analysis['explanation']
                }
                callback(document)
        
        next_button.on_click(on_button_click)
        
        if analysis['sql_feasible']:
            display(next_button)
            
            if viewer or ("viewer" in self.para and self.para["viewer"]):
                on_button_click(next_button)
                return
            
            
 
def generate_er_diagram_html(relation_map, relation_details, highlight_indices):

    graph_html = generate_workflow_graph(relation_map, relation_details, highlight_indices)

    descriptions_html = '<p>Story behind the relationships <small class="text-muted">(oval for entity, box for relation, octagon for table group))</small></p><ol class="small">'
    
    for index, detail in enumerate(relation_details):
        relation_name = (detail.get('Name') or detail.get('name'))
        relation_desc = (detail.get('Description') or detail.get('description'))
        if index in highlight_indices:
            descriptions_html += f"<li><b>[{relation_name}]: {relation_desc}</b></li>"
        else:
            descriptions_html += f"<li>[{relation_name}]: {relation_desc}</li>"
    descriptions_html += "</ol>"

    html_content = f"""
    {descriptions_html}
    {graph_html}
    """

    return html_content   

def extract_related_tables(data_project, related_steps):
    story = data_project.get_story_list()
    related_relations = []
    related_groups = []
    related_entities = []
    
    for element in story:
        element_name = next((element[key] for key in element if key.lower() == 'name'), '')
        if element_name in  related_steps:
            element_type = next((element[key] for key in element if key.lower() == 'type'), '').lower()
            if element_type == 'relation':
                related_relations.append(element_name)
            elif element_type == 'group':
                related_groups.append(element_name)
            elif element_type == 'entity':
                related_entities.append(element_name)
    
    tables = set()

    for relation_name in related_relations:
        for table, relation_data in data_project.relations.items():
            if relation_data['relation_name'] == relation_name:
                tables.add(table)
                for entity in relation_data['entities']:
                    for entity_table, entity_data in data_project.entities.items():
                        if entity_data['entity_name'] == entity:
                            tables.add(entity_table)
                    if entity not in related_entities:
                        related_entities.append(entity)
             
    for entity in related_entities:
        for entity_table, entity_data in data_project.entities.items():
            if entity_data['entity_name'] == entity:
                tables.add(entity_table)
                
        for table, relation_data in data_project.relations.items():
            if relation_data['relation_name'] is None and len(relation_data['entities']) == 1:
                if relation_data['entities'][0] == entity:
                    tables.add(table)

    for group_name in related_groups:
        group_data = data_project.groups[data_project.groups['Group Name'] == group_name]
        for table in group_data['Tables'].iloc[0]:
            tables.add(table)

    for table in list(tables):
        if table in data_project.history_table:
            tables.add(data_project.history_table[table])

    new_tables = set()
    for table in tables:
        if table in data_project.partition_mapping:
            new_tables.update(data_project.partition_mapping[table])
        else:
            new_tables.add(table)

    return list(new_tables)    
 
class BusinessQuestionDataSufficiency(Node):
    default_name = 'Business Question Data Sufficiency Check'
    default_description = 'This node evaluates whether the current tables have enough information to answer the business question.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"🔍 Evaluating data sufficiency for the business question..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]
        business_question = self.para["business_question"]

        df = pd.read_json(self.get_sibling_document('Business Question Entity and Relation Analysis')['related_steps'], orient="split")
        related_steps = df[df['Related'] == True]['Name'].tolist()

        tables = extract_related_tables(data_project, related_steps)

        schema_description = yaml.dump(data_project.describe_project_yml(tables=tables, limit=9999))

        return business_question, schema_description, tables, data_project

    def run(self, extract_output, use_cache=True):
        business_question, schema_description, all_related_tables, data_project = extract_output


        template = f"""Given the following question:
'{business_question}'

And the following schema description of relevant tables:
{schema_description}

Please analyze if the available data is sufficient to answer the business question using SQL. 
If it is sufficient, provide a high-level explanation of what selections, aggregations, and joins might be needed, and over which columns and tables.

Return your analysis in the following format:
```yml
explanation: >-
    [Your detailed explanation here, discussing data sufficiency and potential SQL approach]
sufficient: true/false
sql_approach: >-
    If sufficient, provide a detailed step-by-step instruction of the SQL approach.
    Dictates which tables, columns, and operations to use.
    However, don't provide codes.
# from sql_approach list all related tables, even if insufficient
related_tables: ['table1', 'table2', ...]
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'sufficient' not in analysis or not isinstance(analysis['sufficient'], bool):
            raise ValueError("Analysis should contain a 'sufficient' key with a boolean value")
        
        if 'explanation' not in analysis or not isinstance(analysis['explanation'], str):
            raise ValueError("Analysis should contain an 'explanation' key with a string value")
        
        if analysis['sufficient']:
            if 'sql_approach' not in analysis or not isinstance(analysis['sql_approach'], str):
                raise ValueError("When data is sufficient, analysis should contain a 'sql_approach' key with a string value")
            
            if 'related_tables' not in analysis or not isinstance(analysis['related_tables'], list):
                raise ValueError("When data is sufficient, analysis should contain a 'related_tables' key with a list value")
            
            invalid_tables = [table for table in analysis['related_tables'] if table not in all_related_tables]
            if invalid_tables:
                raise ValueError(f"The following tables were not identified as related during extraction: {', '.join(invalid_tables)}")

        return analysis
    
    def run_but_fail(self, extract_output, use_cache=True):
        default_analysis = {
            'explanation': 'Unable to generate an explanation for the given business question and schema.',
            'sufficient': False,
            'sql_approach': 'Unable to generate an SQL approach due to analysis failure.',
            'related_tables': []
        }
        
        return default_analysis
        
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output
        business_question, schema_description, all_related_tables, data_project = extract_output
        
       
        
        related_tables = analysis.get('related_tables', [])
        
        if not analysis['sufficient']:
            display(HTML(f"⚠️ <b>Caveat:</b> {analysis['explanation']}"))
        
        
        display(HTML(f"📚 <b>Related Tables:</b>"))
        html_content = data_project.generate_html_graph_static(highlight_nodes=related_tables)
        display(HTML(html_content))
        
        
        text_area = None
        display(HTML(f"🧐 <b>SQL Instruction:</b>"))
        text_area, char_count_label = create_text_area_with_char_count(analysis['sql_approach'], max_chars=1200)
        layout = Layout(display='flex', justify_content='space-between', width='100%')
        
        display(HBox([text_area, char_count_label], layout=layout))
        display(HTML(f"🔗 <b>Related Tables:</b>"))
            
        multi_select = create_column_selector(all_related_tables, default=False, except_columns = related_tables)
        
        if "chatui" in self.para:
            yml_dict = data_project.describe_project_yml(tables=all_related_tables, limit=9999)
            highlighted_yml_content = highlight_yml_only(schema_description)
            message = f"😎 <b>RAG from Cocoon</b>: Checking out all the related tables Cocoon set up... {highlighted_yml_content}"

            if related_tables and analysis['sufficient']:

                message += f"🤓 <b>We've identified the related tables</b>: {', '.join(related_tables)}"
                message += html_content
                message += f"🧠 <b>Explanation</b>: {analysis['explanation']}<br>"
                message += f"💡 <b>Instruction to write SQL</b>: {analysis['sql_approach']}<br>"
                
                self.para["chatui"].add_message("GenAI", message)
            else:
                message += "🤔 <b>We couldn't find any related tables</b>.<br>"
                message += f"🧠 <b>Explanation:</b> {analysis['explanation']}"
                self.para["chatui"].add_message("GenAI", message)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                if not (related_tables and analysis['sufficient']):
                    print("Error: Cannot proceed because current data is insufficient, or no related table.")
                
                selected_indices = multi_select.value
                selected_tables = [all_related_tables[i] for i in selected_indices]
                document = {
                    'business_question': business_question,
                    'data_sufficient': analysis['sufficient'],
                    'explanation': analysis['explanation'],
                    'sql_approach': text_area.value if text_area else analysis.get('sql_approach', 'N/A'),
                    'related_tables': selected_tables
                }
                callback(document)
        
        next_button.on_click(on_button_click)
        display(next_button)
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
class JoinGraphBuilder(Node):
    default_name = 'Join Graph Builder'
    default_description = 'This node builds a join graph based on the SQL approach and related tables.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"🔗 Connecting the source tables based on join..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]
        previous_node_output = self.get_sibling_document('Business Question Data Sufficiency Check')
        
        sql_approach = previous_node_output['sql_approach']
        related_tables = previous_node_output['related_tables']
        
        key_info = data_project.get_key_info(related_tables)
        
        return sql_approach, related_tables, key_info

    def run(self, extract_output, use_cache=True):
        sql_approach, related_tables, key_info = extract_output

        key_info_str = ""
        for table, keys in key_info.items():
            key_info_str += f"'{table}': {keys}"
            
        template = f"""Given the following SQL approach to execute:
'{sql_approach}'

And the following related tables with primary key and foreign key information:
{key_info_str}

Please provide a natural language description of how these tables should be joined to execute the SQL. Inlude:
1. The main table(s) that form the base of the query.
2. How each table should be joined to the others, specifying the join keys.
3. Any potential issues or considerations with the joins (e.g., outer join... if you want to keep values)

Your response should be in the following format:
```yml
reasoning: >-
    The join keys are ... Based on the SQL approach, the tables should be joined in the following way...
join_summary: >- # Clear and concise
    To execute the SQL, (no) joins are needed. 
    For each needed join, the join keys are ... it is a (inner/outer) join.
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'join_summary' not in analysis or not isinstance(analysis['join_summary'], str):
            raise ValueError("Analysis should contain a 'join_summary' key with a string value")
        
        return analysis

    def run_but_fail(self, extract_output, use_cache=True):
        default_analysis = {
            'reasoning': 'Unable to generate reasoning for the join graph.',
            'join_summary': 'Unable to generate a join description for the given tables and keys.'
        }
        
        return default_analysis

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output
        sql_approach, related_tables, key_info = extract_output
        
        display(HTML(f"💡 <b>SQL Approach:</b> {sql_approach}"))
        display(HTML(f"📚 <b>Related Tables:</b> {', '.join(related_tables)}"))
        display(HTML(f"🤔 <b>Reasoning:</b> {analysis['reasoning']}"))
        display(HTML(f"🔗 <b>Join Description:</b> {analysis['join_summary']}"))
        
        if "chatui" in self.para:
            yml_content = yaml.dump(key_info, default_flow_style=False)
            highlighted_yml_content = highlight_yml_only(yml_content)
            message = f"😎 <b>RAG from Cocoon</b>: Looking at how tables connect, thanks to Cocoon's key info... {highlighted_yml_content}"
            message += f"🤓 <b>We've planned the join:</b> {analysis['join_summary']}"
            self.para["chatui"].add_message("GenAI", message)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                document = {
                    'sql_approach': sql_approach,
                    'related_tables': related_tables,
                    'join_reasoning': analysis['reasoning'],
                    'join_summary': analysis['join_summary']
                }
                callback(document)
        
        next_button.on_click(on_button_click)
        display(next_button)
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        

class ColumnSelector(Node):
    default_name = 'Column Selector'
    default_description = 'This node identifies the necessary columns for each table in the SQL query.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"📊 Selecting columns for the SQL query..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]
        previous_node_output = self.get_sibling_document('Business Question Data Sufficiency Check')
        join_graph_output = self.get_sibling_document('Join Graph Builder')
        
        sql_approach = previous_node_output['sql_approach']
        related_tables = previous_node_output['related_tables']
        join_summary = join_graph_output['join_summary']
        
        table_columns = {}
        for table in related_tables:
            if table in data_project.table_object:
                table_columns[table] = list(data_project.table_object[table].columns)
        
        return sql_approach, join_summary, table_columns

    def run(self, extract_output, use_cache=True):
        sql_approach, join_summary, table_columns = extract_output

        column_info_str = ""
        for table, columns in table_columns.items():
            column_info_str += f"'{table}': {columns}\n"
            
        template = f"""Given the following SQL approach:
'{sql_approach}'

And the following join description:
'{join_summary}'

With the following tables and their available columns:
{column_info_str}

Please identify the necessary columns for each table to fulfill the SQL approach.
Consider the following:
1. Columns needed for joins
2. Columns required for filtering or conditions
3. Columns needed for the final output or any calculations

Your response should be in the following format:
```yml
reasoning: >-
    [Explain why these columns are necessary based on the SQL approach and join description]
selected_columns:
    table1:
        - column1
        - column2
    table2:
        - column1
        - column2
    ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'selected_columns' not in analysis or not isinstance(analysis['selected_columns'], dict):
            raise ValueError("Analysis should contain a 'selected_columns' key with a dictionary value")
        
        for table, columns in analysis['selected_columns'].items():
            if not isinstance(columns, list):
                raise ValueError(f"Columns for table '{table}' should be a list")
            analysis['selected_columns'][table] = list(set(columns) & set(table_columns[table]))
        
        return analysis

    def run_but_fail(self, extract_output, use_cache=True):
        sql_approach, join_summary, table_columns = extract_output
        default_analysis = {
            'reasoning': 'Unable to determine the necessary columns for the query.',
            'selected_columns': {table: [] for table in table_columns.keys()}
        }
        
        return default_analysis

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output
        sql_approach, join_summary, table_columns = extract_output
        
        display(HTML(f"💡 <b>SQL Approach:</b> {sql_approach}"))
        display(HTML(f"🔗 <b>Join Description:</b> {join_summary}"))
        display(HTML(f"🤔 <b>Reasoning for Column Selection:</b> {analysis['reasoning']}"))
        display(HTML("<b>Selected Columns:</b>"))
        for table, columns in analysis['selected_columns'].items():
            display(HTML(f"• <b>{table}:</b> {', '.join(columns)}"))
        
        
        if "chatui" in self.para:    
            
            data_project = self.para["data_project"] 
                
            column_dict = {}
            for table, columns in analysis['selected_columns'].items():
                table_object = data_project.table_object[table]
                column_dict[table] = table_object.get_column_desc_yml(columns=columns, show_category=True, show_pattern=True)
            
            yml_content = yaml.dump(column_dict, default_flow_style=False)
            highlighted_yml_content = highlight_yml_only(yml_content)
            message = f"😎 <b>RAG from Cocoon</b>: Diving into column details Cocoon prepared... getting a bit technical here! {highlighted_yml_content}"
            message += f"🤔 <b>Reasoning for column selection</b>: {analysis['reasoning']}"
            self.para["chatui"].add_message("GenAI", message)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                document = {
                    'sql_approach': sql_approach,
                    'join_summary': join_summary,
                    'column_selection_reasoning': analysis['reasoning'],
                    'selected_columns': analysis['selected_columns']
                }
                callback(document)
        
        next_button.on_click(on_button_click)
        display(next_button)
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return


class SQLQueryConstructor(Node):
    default_name = 'SQL Query Constructor'
    default_description = 'This node constructs the final SQL query based on previous analyses.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"🛠️ Constructing the SQL query..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]
        business_question = self.para["business_question"]
        previous_node_output = self.get_sibling_document('Business Question Data Sufficiency Check')
        join_graph_output = self.get_sibling_document('Join Graph Builder')
        column_selector_output = self.get_sibling_document('Column Selector')
        
        con = self.item["con"]
        
        database_hint = ""
        if con:
            database_name = get_database_name(con)
            database_hint = f"Note that we use {database_name} syntax. {database_general_hint.get(database_name, '')}"
        
        sql_approach = previous_node_output['sql_approach']
        join_summary = join_graph_output['join_summary']
        selected_columns = column_selector_output['selected_columns']
        
        column_descriptions = {}
        for table, columns in selected_columns.items():
            table_object = data_project.table_object[table]
            column_descriptions[table] = table_object.print_column_desc(columns=columns, show_category=True, show_pattern=True)
        
        return business_question, sql_approach, join_summary, selected_columns, column_descriptions, database_hint

    def run(self, extract_output, use_cache=True):
        business_question, sql_approach, join_summary, selected_columns, column_descriptions, database_hint = extract_output

        column_info_str = ""
        for table, columns in selected_columns.items():
            column_info_str += f"Table: {table}\n{column_descriptions[table]}\n"
            
        template = f"""Given the following business question:
'{business_question}'

With the following selected columns and their descriptions:
{column_info_str}

And the SQL approach to answer this question:
'{sql_approach}'

And the following join summary:
'{join_summary}'

Please construct a complete SQL query. 
{database_hint}
Your response should be in the following format:
```yml
reasoning: >-
    [Explain the structure of the SQL query and any important considerations]
sql_query: |
    SELECT "col1", "col2"
    FROM "table"
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'sql_query' not in analysis or not isinstance(analysis['sql_query'], str):
            raise ValueError("Analysis should contain a 'sql_query' key with a string value")
        
        return analysis

    def run_but_fail(self, extract_output, use_cache=True):
        default_analysis = {
            'reasoning': 'Unable to construct the SQL query based on the provided information.',
            'sql_query': 'SELECT 1;  -- Placeholder query'
        }
        
        return default_analysis

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output
        business_question, sql_approach, join_summary, selected_columns, _, _ = extract_output
        

        
        next_button = widgets.Button(description="Finalize Query", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():                
                if "chatui" in self.para:
                    message = "😊 <b>We have written the SQL:</b>"
                    message += highlight_sql_only(analysis['sql_query'])

                    self.para["chatui"].add_message("GenAI", message)
                    
                document = {
                    'sql_approach': sql_approach,
                    'join_summary': join_summary,
                    'selected_columns': selected_columns,
                    'query_construction_reasoning': analysis['reasoning'],
                    'sql_query': analysis['sql_query']
                }
                callback(document)
        
        next_button.on_click(on_button_click)
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(HTML(f"💡 <b>SQL Approach:</b> {sql_approach}"))
        display(HTML(f"🔗 <b>Join Summary:</b> {join_summary}"))
        display(HTML(f"🤔 <b>Query Construction Reasoning:</b> {analysis['reasoning']}"))
        display(HTML("<b>Constructed SQL Query:</b>"))
        display(HTML(f"<pre><code>{analysis['sql_query']}</code></pre>"))
        display(next_button)
        
        
class DebugSQLQuery(Node):
    default_name = 'Debug SQL'
    default_description = 'Debug the SQL'

    def extract(self, input_item):
        clear_output(wait=True)
        display(HTML(f"{running_spinner_html} Verifying the codes..."))
        self.progress = show_progress(1)
        
        con = self.item["con"]
        sql_query = self.get_sibling_document('SQL Query Constructor')['sql_query']
        
        return con, sql_query
    
    def run(self, extract_output, use_cache=True):
        con, sql_query = extract_output
        max_iterations = 10
        self.messages = []
        
        
        if not con:
            return {
            "sql_query": sql_query,
            "error_message": "Database connection not provided"
        }
            
        if not sql_query:
            return self.run_but_fail(extract_output=extract_output)
        
        database_hint = ""
        if con:
            database_name = get_database_name(con)
            database_hint = f"Note that we use {database_name} syntax. {database_general_hint.get(database_name, '')}"
        
        code_clauses = {"sql_query": sql_query}
        
        for i in range(max_iterations):
            try:
                sql = code_clauses["sql_query"]
                df = run_sql_return_df(con, sql)
                code_clauses["output_columns"] = list(df.columns)
                code_clauses["sample_output"] = df.head(5).to_dict(orient='records')
                break
            except Exception as e:
                detailed_error_info = str(e)
                
                template = f"""You have the following SQL:
{sql}
It has an error: {detailed_error_info}
Please correct the SQL, but don't change the logic. {database_hint}
Respond in the following format:
```yml
reasoning: >-
    [Explain the error and the correction made]
new_sql_query: |
    [The corrected SQL query]
```
"""
                messages = [{"role": "user", "content": template}]
                response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
                self.messages.append(messages + [response['choices'][0]['message']])

                yml_code = extract_yml_code(response['choices'][0]['message']["content"])
                correction = yaml.load(yml_code, Loader=yaml.SafeLoader)
                
                if 'reasoning' not in correction or 'new_sql_query' not in correction:
                    raise ValueError("LLM response should contain 'reasoning' and 'new_sql_query' keys")
                
                code_clauses["sql_query"] = correction['new_sql_query']
                code_clauses["debug_reasoning"] = correction['reasoning']
                
                if "chatui" in self.para:
                    message = f"🤔 <b>The sql has bug:</b> <div class=\"highlight\">{detailed_error_info}</div>"
                    message += f"🔧 We have updated the SQL:"
                    message += highlight_sql_only(code_clauses['sql_query'])

                    self.para["chatui"].add_message("GenAI", message)
        
        if i == max_iterations - 1:
            if "chatui" in self.para:
                message = "❌ <b>SQL fails to run</b>"
                self.para["chatui"].add_message("GenAI", message)
            raise ValueError(f"Failed to debug SQL after {max_iterations} attempts")
                    
        return code_clauses

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        code_clauses = run_output
        
        if not code_clauses["sql_query"]:
            callback(code_clauses)
            return
        
        display(HTML(f"🎉 <b>The codes are ready!</b>"))
   
        
        sql_query = code_clauses['sql_query']
        highlighted_html = highlight_sql(sql_query)
        display(HTML(highlighted_html))
        
        labels = ["SQL"]
        file_names = ["cocoon_result.sql"]
        contents = [sql_query]
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
        
        if "sample_output" in code_clauses:
            display(HTML("<b>🎊 Sample Output:</b>"))
            display(pd.DataFrame(code_clauses["sample_output"]))
            
        if "chatui" in self.para:
            if "sample_output" in code_clauses:
                if len(code_clauses["sample_output"]) == 0:
                    message = "🤨 <b>SQL runs successful!</b> The result is ... empty??"
                else:
                    message = "✅ <b>SQL runs successful!</b> Here are the samples (first 5 rows):"
                message += pd.DataFrame(code_clauses["sample_output"]).head(5).to_html()
                self.para["chatui"].add_message("GenAI", message)
        
        next_button = widgets.Button(description="Next", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                callback(code_clauses)
        
        next_button.on_click(on_button_click)
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(next_button)

    def run_but_fail(self, extract_output, use_cache=True):
        con, sql_query = extract_output
        return {
            "sql_query": sql_query,
            "debug_reasoning": "Unable to debug the SQL query due to persistent errors.",
            "error_message": "SQL debugging failed after multiple attempts."
        }
        
        
        
def create_cocoon_genai_workflow(con=None, query_widget=None, viewer=False, para=None, output=None):
    if para is None:
        para = {}
        
    if query_widget is None:
        query_widget = QueryWidget(con)

    
    item = {
        "con": con,
        "query_widget": query_widget
    }
    
    workflow_para = {"viewer": viewer}
    
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("GenAI Workflow", 
                            item = item, 
                            description="A workflow query data model with GenAI",
                            para = workflow_para,
                            output=output)
    
    main_workflow.add_to_leaf(SelectSchema(output=output))
    main_workflow.add_to_leaf(DBTProjectConfigAndRead(output=output))
    main_workflow.add_to_leaf(SimpleBusinessQuestionNode(output=output))
    main_workflow.add_to_leaf(BusinessQuestionEntityRelationNode(output=output))
    main_workflow.add_to_leaf(BusinessQuestionDataSufficiency(output=output))
    main_workflow.add_to_leaf(JoinGraphBuilder(output=output))
    main_workflow.add_to_leaf(ColumnSelector(output=output))
    main_workflow.add_to_leaf(SQLQueryConstructor(output=output))
    main_workflow.add_to_leaf(DebugSQLQuery(output=output))
    main_workflow.add_to_leaf(TableAttributeExtractor(output=output))
    main_workflow.add_to_leaf(SQLWarningsGenerator(output=output))
    
    return query_widget, main_workflow

def create_dummy_genai_workflow(con, query_widget=None, viewer=False, para=None, output=None):
    if para is None:
        para = {}
        
    if query_widget is None:
        query_widget = QueryWidget(con)

    
    item = {
        "con": con,
        "query_widget": query_widget
    }
    
    workflow_para = {"viewer": viewer}
    
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("GenAI Workflow", 
                            item = item, 
                            description="A workflow query data model with GenAI",
                            para = workflow_para,
                            output=output)
    
    main_workflow.add_to_leaf(DBTProjectConfigAndReadSourceOnly(output=output))
    main_workflow.add_to_leaf(SimpleBusinessQuestionNode(output=output))
    main_workflow.add_to_leaf(SQLQueryConstructorDummy(output=output))
    main_workflow.add_to_leaf(DebugSQLQuery(output=output))
    return query_widget, main_workflow

class SQLQueryConstructorDummy(Node):
    default_name = 'SQL Query Constructor'
    default_description = 'This node constructs an SQL query directly from the business question and database schema.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"🛠️ Constructing the SQL query directly..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]
        business_question = self.para["business_question"]
        schema_description = data_project.describe_project(limit=999)
        con = self.item["con"]
        database_name = get_database_name(con)

        if "chatui" in self.para:
            message = "😵 <b>Current RAG</b>: Reading all the source data schema from databases... phew, it's a lot!"
            sql_query = data_project.create_all_tables_ddl()
            message += highlight_sql_only(sql_query)
            self.para["chatui"].add_message("GenAI", message)
        
        return business_question, schema_description, database_name

    def run(self, extract_output, use_cache=True):
        business_question, schema_description, database_name = extract_output

        template = f"""Given the following business question:
'{business_question}'

And the schema of the database:
{schema_description}

Please construct a complete SQL query that answers the business question based on the given schema.
Consider the following:
1. Ensure the query directly addresses the business question
2. Use appropriate joins if multiple tables are needed
3. Include all necessary columns in the SELECT statement
4. Add appropriate WHERE clauses to filter the data as required
5. Include any required GROUP BY, HAVING, or ORDER BY clauses
6. Use table aliases if needed for clarity

Note that we use {database_name} syntax.
Your response should be in the following format:
```yml
sql_query: |
    [The complete SQL query]
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        
        if 'sql_query' not in analysis or not isinstance(analysis['sql_query'], str):
            raise ValueError("Analysis should contain a 'sql_query' key with a string value")
        
        return analysis

    def run_but_fail(self, extract_output, use_cache=True):
        default_analysis = {
            'reasoning': 'Unable to construct the SQL query based on the provided business question and schema.',
            'sql_query': 'SELECT 1;  -- Placeholder query'
        }
        
        return default_analysis

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output
        business_question, _, _ = extract_output
        
        next_button = widgets.Button(description="Finalize Direct Query", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                
                if "chatui" in self.para:
                    message = "🤨 <b>I've done my best with the SQL... fingers crossed it's right!</b>"
                    message += highlight_sql_only(analysis['sql_query'])
                    self.para["chatui"].add_message("GenAI", message)
                    
                document = {
                    'business_question': business_question,
                    'sql_query': analysis['sql_query']
                }
                callback(document)
        
        next_button.on_click(on_button_click)
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return

        display(HTML(f"❓ <b>Business Question:</b> {business_question}"))
        display(HTML("<b>Constructed SQL Query:</b>"))
        display(HTML(f"<pre><code>{analysis['sql_query']}</code></pre>"))
        display(next_button)
        
        
class DBTProjectConfigAndReadSourceOnly(Node):
    default_name = 'DBT Project Configuration and Read'
    default_description = 'This step allows you to specify an existing DBT project directory and read the project.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        dbt_directory_input = widgets.Text(
            description='Project Directory:',
            value=self.para.get("dbt_directory", ""),
            placeholder="/path/to/your/dbt/project",
            style={'description_width': 'initial'},
            layout={'width': '50%'}
        )

        submit_button = widgets.Button(description="Read Project", button_style='success')

        output = widgets.Output()

        def on_button_click(b):
            with self.output_context():
                directory = dbt_directory_input.value
                
                if not directory:
                    print("Please specify a directory.")
                    return

                con = self.item["con"]
                database = self.para.get("database", None)
                schema = self.para.get("schema", None)
                
                data_project = read_data_project_from_dir(directory, con, source_only=True, database=database, schema=schema)
                
                self.para["data_project"] = data_project
                self.para["dbt_directory"] = directory
                
                print(f"Successfully read project from {directory}")
                
                callback({"dbt_directory": directory})
                

        submit_button.on_click(on_button_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "dbt_directory" in self.para:
                dbt_directory_input.value = self.para["dbt_directory"]
                on_button_click(submit_button)
                return

        create_progress_bar_with_numbers(1, model_steps)
        display(HTML("<h3>📂 Specify DBT Directory</h3>Preferably built by Cocoon"))
        display(dbt_directory_input)
        display(submit_button)
        display(output)
            

class DetectReferentialIntegrity(Node):
    default_name = 'Detect Referential Integrity'
    default_description = 'This node detects referential integrity violations between primary and foreign keys'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        print("🔍 Detecting referential integrity violations...")

        data_project = self.para["data_project"]
        con = self.item["con"]
        query_widget = self.item["query_widget"]

        if self.para.get("cocoon_catalog_options", {}).get("schema_only", False):
            empty_df = pd.DataFrame(columns=['PK', 'FK', 'Orphan', 'Explanation'])
            callback(empty_df.to_json(orient="split"))
            return

        pk_fk_document = self.get_sibling_document('Connect PK FK Table')
        pk_fk_df = pd.read_json(pk_fk_document, orient="split")

        integrity_data = []

        for _, row in pk_fk_df.iterrows():
            pk_table, pk_column = row['Table[PK]'].strip('[]').split('[')
            pk_table_pipeline = data_project.table_pipelines[pk_table]

            for fk in row['Table[FK]']:
                fk_table, fk_column = fk.strip('[]').split('[')
                fk_table_pipeline = data_project.table_pipelines[fk_table]

                try:
                    only1, only2, overlap = generate_queries_for_overlap(pk_table_pipeline, [pk_column], fk_table_pipeline, [fk_column], con)

                    if only2 > 0:
                        total_fk = only2 + overlap
                        orphan_percentage = (only2 / total_fk) * 100
                        integrity_data.append({
                            'PK': f'{pk_table}[{pk_column}]',
                            'FK': f'{fk_table}[{fk_column}]',
                            'Orphan': f'{orphan_percentage:.2f}%',
                            'Explanation': ''
                        })
                except Exception as e:
                    write_log(f"""
The node is {self.name}
The error is: {e}
pl_table: {pk_table}, pl_column: {pk_column}, fk_table: {fk_table}, fk_column: {fk_column}
""")

        integrity_df = pd.DataFrame(integrity_data)

        if integrity_df.empty:
            callback(integrity_df.to_json(orient="split"))
            return

        dropdown = create_explore_button(query_widget, 
                            table_name=list(data_project.table_pipelines.keys()),
                            logical_to_physical=data_project.table_pipelines)
        
        editable_columns = [False, False, False, True]
        grid = create_dataframe_grid(integrity_df, editable_columns, reset=True)

        next_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        def on_button_clicked(b):
            with self.output_context():
                updated_df = grid_to_updated_dataframe(grid)
                document = updated_df.to_json(orient="split")
                
                updated_df['pk_table_name'] = updated_df['PK'].apply(lambda x: x.split('[')[0])
                updated_df['pk'] = updated_df['PK'].apply(lambda x: x.split('[')[1].rstrip(']'))
                updated_df['fk_table_name'] = updated_df['FK'].apply(lambda x: x.split('[')[0])
                updated_df['fk'] = updated_df['FK'].apply(lambda x: x.split('[')[1].rstrip(']'))
                
                data_project.referential_integrity = updated_df[["pk_table_name", "pk", "fk_table_name", "fk", "Orphan", "Explanation"]]
                
                callback(document)

        next_button.on_click(on_button_clicked)

        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(next_button)
            return
        
        display(HTML("⚠️ Referential Integrity Violations Detected, Please Explain:"))
        display(grid)
        display(next_button)
        
class ChatUI:
    def __init__(self):
        self.messages = []
        self.output = widgets.HTML()
        self.text_input = widgets.Text(placeholder='Type your message here...', layout=widgets.Layout(width='80%'))
        
        
        
        self.update_chat()
        
    def display(self):
        display(self.output)
        
        
    
    def add_message(self, sender, message):
        self.messages.append((sender, message))
        self.update_chat()
    
    def update_chat(self):
        chat_html = self.get_chat_html()
        self.output.value = wrap_in_iframe(chat_html, height="500px", width="1000px")
    
    def get_chat_html(self):
        chat_html = """
<style>
    body, html {
        margin: 0;
        padding: 0;
        font-family: Arial, sans-serif;
        font-size: 12px;
    }
    .container {
        display: flex;
        height: 100%;
    }
        
    .chat-container {
        border: 1px solid #e2e2e2;
        border-radius: 12px;
        padding: 20px;
        overflow-y: auto;
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Helvetica', 'Arial', sans-serif;
    }
    
    .chat-side {
        width: 100%;
        padding: 20px;
        box-sizing: border-box;
        overflow-y: auto;
        position: relative;
    }
    
    .message {
        margin-bottom: 16px;
        padding: 15px;
        border-radius: 15px;
        box-shadow: 0 5px 15px rgba(0, 0, 0, 0.1);
        word-wrap: break-word;
    }
    .user-message {
        background-color: #449dfc;
        color: #ffffff;
        margin-left: 15%;
    }
    .bot-message {
        background-color: white;
        color: #000000;
    }
    .sender {
        font-weight: 600;
        margin-bottom: 5px;
        font-size: 0.85em;
    }
    .user-sender {
        text-align: right;
        color: #ffffff;
    }
    .bot-sender {
        color: #8E8E93;
    }
    
table {
    width: 100%;
    color: #000000;
    border-collapse: collapse;
    margin-top: 20px;
    font-size: 0.9em;
    border-radius: 4px;
    overflow: hidden;
}

th,
td {
    padding: 3px 4px;
    text-align: left;
    border-bottom: 1px solid #dddddd;
    border-right: 1px solid #dddddd;
}

thead tr {
    background-color: #464646;
    /* Changed color for header */
    color: #ffffff;
    /* Changed text color for better contrast */
    text-align: left;
    font-weight: bold;
    font-size: 12px;
}

th {
    top: 0;
    background-color: #464646;
    color: #ffffff;
}

tbody tr {
    background-color: #f9f9f9;
    /* Lighter color for content rows */
    font-size: 10px
}


pre { line-height: 125%; }
.highlight {
    background: #272822;
    color: #f8f8f2;
    border-radius: 4px;
    padding: 1em;
    margin: 1em 0;
    overflow-x: auto;
    font-size: 12px;
}

td.linenos .normal { color: #aaaaaa; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: #aaaaaa; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
.highlight .hll { background-color: #404040 }
.highlight { background: #202020; color: #d0d0d0 }
.highlight .c { color: #ababab; font-style: italic } /* Comment */
.highlight .err { color: #a61717; background-color: #e3d2d2 } /* Error */
.highlight .esc { color: #d0d0d0 } /* Escape */
.highlight .g { color: #d0d0d0 } /* Generic */
.highlight .k { color: #6ebf26; font-weight: bold } /* Keyword */
.highlight .l { color: #d0d0d0 } /* Literal */
.highlight .n { color: #d0d0d0 } /* Name */
.highlight .o { color: #d0d0d0 } /* Operator */
.highlight .x { color: #d0d0d0 } /* Other */
.highlight .p { color: #d0d0d0 } /* Punctuation */
.highlight .ch { color: #ababab; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #ababab; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #ff3a3a; font-weight: bold } /* Comment.Preproc */
.highlight .cpf { color: #ababab; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #ababab; font-style: italic } /* Comment.Single */
.highlight .cs { color: #e50808; font-weight: bold; background-color: #520000 } /* Comment.Special */
.highlight .gd { color: #ff3a3a } /* Generic.Deleted */
.highlight .ge { color: #d0d0d0; font-style: italic } /* Generic.Emph */
.highlight .ges { color: #d0d0d0; font-weight: bold; font-style: italic } /* Generic.EmphStrong */
.highlight .gr { color: #ff3a3a } /* Generic.Error */
.highlight .gh { color: #ffffff; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #589819 } /* Generic.Inserted */
.highlight .go { color: #cccccc } /* Generic.Output */
.highlight .gp { color: #aaaaaa } /* Generic.Prompt */
.highlight .gs { color: #d0d0d0; font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #ffffff; text-decoration: underline } /* Generic.Subheading */
.highlight .gt { color: #ff3a3a } /* Generic.Traceback */
.highlight .kc { color: #6ebf26; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #6ebf26; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #6ebf26; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #6ebf26 } /* Keyword.Pseudo */
.highlight .kr { color: #6ebf26; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #6ebf26; font-weight: bold } /* Keyword.Type */
.highlight .ld { color: #d0d0d0 } /* Literal.Date */
.highlight .m { color: #51b2fd } /* Literal.Number */
.highlight .s { color: #ed9d13 } /* Literal.String */
.highlight .na { color: #bbbbbb } /* Name.Attribute */
.highlight .nb { color: #2fbccd } /* Name.Builtin */
.highlight .nc { color: #71adff; text-decoration: underline } /* Name.Class */
.highlight .no { color: #40ffff } /* Name.Constant */
.highlight .nd { color: #ffa500 } /* Name.Decorator */
.highlight .ni { color: #d0d0d0 } /* Name.Entity */
.highlight .ne { color: #bbbbbb } /* Name.Exception */
.highlight .nf { color: #71adff } /* Name.Function */
.highlight .nl { color: #d0d0d0 } /* Name.Label */
.highlight .nn { color: #71adff; text-decoration: underline } /* Name.Namespace */
.highlight .nx { color: #d0d0d0 } /* Name.Other */
.highlight .py { color: #d0d0d0 } /* Name.Property */
.highlight .nt { color: #6ebf26; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #40ffff } /* Name.Variable */
.highlight .ow { color: #6ebf26; font-weight: bold } /* Operator.Word */
.highlight .pm { color: #d0d0d0 } /* Punctuation.Marker */
.highlight .w { color: #666666 } /* Text.Whitespace */
.highlight .mb { color: #51b2fd } /* Literal.Number.Bin */
.highlight .mf { color: #51b2fd } /* Literal.Number.Float */
.highlight .mh { color: #51b2fd } /* Literal.Number.Hex */
.highlight .mi { color: #51b2fd } /* Literal.Number.Integer */
.highlight .mo { color: #51b2fd } /* Literal.Number.Oct */
.highlight .sa { color: #ed9d13 } /* Literal.String.Affix */
.highlight .sb { color: #ed9d13 } /* Literal.String.Backtick */
.highlight .sc { color: #ed9d13 } /* Literal.String.Char */
.highlight .dl { color: #ed9d13 } /* Literal.String.Delimiter */
.highlight .sd { color: #ed9d13 } /* Literal.String.Doc */
.highlight .s2 { color: #ed9d13 } /* Literal.String.Double */
.highlight .se { color: #ed9d13 } /* Literal.String.Escape */
.highlight .sh { color: #ed9d13 } /* Literal.String.Heredoc */
.highlight .si { color: #ed9d13 } /* Literal.String.Interpol */
.highlight .sx { color: #ffa500 } /* Literal.String.Other */
.highlight .sr { color: #ed9d13 } /* Literal.String.Regex */
.highlight .s1 { color: #ed9d13 } /* Literal.String.Single */
.highlight .ss { color: #ed9d13 } /* Literal.String.Symbol */
.highlight .bp { color: #2fbccd } /* Name.Builtin.Pseudo */
.highlight .fm { color: #71adff } /* Name.Function.Magic */
.highlight .vc { color: #40ffff } /* Name.Variable.Class */
.highlight .vg { color: #40ffff } /* Name.Variable.Global */
.highlight .vi { color: #40ffff } /* Name.Variable.Instance */
.highlight .vm { color: #40ffff } /* Name.Variable.Magic */
.highlight .il { color: #51b2fd } /* Literal.Number.Integer.Long */
        </style>
        """
        
        chat_html += self.get_chat_container_html()
        return chat_html
        
    def get_chat_container_html(self):
        chat_container_html = "<div class=\"chat-container\">"
        for sender, message in self.messages:
            message_class = "user-message" if sender == "You" else "bot-message"
            sender_class = "user-sender" if sender == "You" else "bot-sender"
            chat_container_html += f'''
                <div class="message {message_class}">
                    <div class="sender {sender_class}">{sender}</div>
                    {message}
                </div>
            '''
        chat_container_html += "</div>"
        return chat_container_html



class DummyChatUI:
    def __init__(self, processing_func):
        self.processing_func = processing_func

    def add_message(self, sender, message):
        self.processing_func(sender, message)










class TableAttributeExtractor(Node):
    default_name = 'Table Attribute Extractor'
    default_description = 'Extracts table names and their referenced attributes from the SQL query using LLM'

    def extract(self, input_item):
        clear_output(wait=True)
        
        debug_sql_query_output = self.get_sibling_document('Debug SQL')
        sql_query = debug_sql_query_output['sql_query']
        return sql_query

    def run(self, extract_output, use_cache=True):
        sql_query = extract_output
        
        template = f"""Given the following SQL query:
{sql_query}

Please extract a dictionary where the keys are table names (remove database/schema names) and the values are lists of their attributes referenced in the query.

Your response should be in the following format:
```yml
table_attributes:
    table1:
        - attribute1
        - attribute2
    table2:
        - attribute3
        - attribute4
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(summary, dict) or 'table_attributes' not in summary:
            raise ValueError("Invalid response format from LLM")
        
        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        table_attributes = run_output['table_attributes']
        
        display(HTML("<b>Extracted Table Attributes:</b>"))
        for table, attributes in table_attributes.items():
            display(HTML(f"<b>{table}:</b> {', '.join(attributes)}"))

        next_button = widgets.Button(description="Proceed", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                callback(run_output)
        
        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(next_button)

    def run_but_fail(self, extract_output, use_cache=True):
        return {
            'reasoning': 'Unable to extract table attributes from the provided SQL query.',
            'table_attributes': {},
        }
        
        
class SQLWarningsGenerator(Node):
    default_name = 'SQL Warnings Generator'
    default_description = 'Generates warnings based on table attributes, referential integrity, and column-specific alerts'

    def extract(self, input_item):
        table_attribute_extractor_output = self.get_sibling_document('Table Attribute Extractor')
        return table_attribute_extractor_output['table_attributes']

    def run(self, extract_output, use_cache=True):
        return extract_output

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        table_attributes = run_output
        data_project = self.para["data_project"]
        
        clear_output(wait=True)
        
        ref_integrity_warnings = data_project.get_referential_integrity_warning(table_attributes)
        
        column_warnings = {}
        for table, columns in table_attributes.items():
            if table in data_project.table_object:
                table_warnings = {}
                for column in columns:
                    alerts = data_project.table_object[table].get_alerts(column)
                    if alerts:
                        table_warnings[column] = alerts
                if table_warnings:
                    column_warnings[table] = table_warnings
        
        all_warnings = {
            'referential_integrity': ref_integrity_warnings,
            'column_specific': column_warnings
        }
        
        html_content = "<h3>SQL Query Warnings</h3>"
        chat_warning_content = ""
        
        if ref_integrity_warnings:
            html_content += "<h4>Referential Integrity Warnings:</h4>"
            html_content += "<ul>"
            for warning in ref_integrity_warnings:
                html_content += f"<li>{warning}</li>"
                chat_warning_content += f"<li><b>{warning[0]}</b>: {warning[1]}</li>"
            html_content += "</ul>"
        else:
            html_content += "<p>No referential integrity warnings found.</p>"
        
        if column_warnings:
            html_content += "<h4>Column-specific Warnings:</h4>"
            for table, warnings in column_warnings.items():
                html_content += f"<h5>Table: {table}</h5>"
                html_content += "<ul>"
                for column, alerts in warnings.items():
                    html_content += f"<li><strong>{column}:</strong>"
                    html_content += "<ul>"
                    for alert in alerts:
                        html_content += f"<li>{alert}</li>"
                        chat_warning_content += f"<li><b>{alert[0]}</b>: {alert[1]}</li>"
                    html_content += "</ul></li>"
                html_content += "</ul>"
        else:
            html_content += "<p>No column-specific warnings found.</p>"
        
        display(HTML(html_content))
        
        if "chatui" in self.para:
            if chat_warning_content:
                message = "🤓 <b>RAG from Cocoon:</b> Beware! The following are unaddressed data quality issues that may compromise the results:"
                message += f"<ul>{chat_warning_content}</ul>"
                self.para["chatui"].add_message("GenAI", message)
        
        next_button = widgets.Button(description="Acknowledge Warnings", button_style='warning', icon='exclamation-triangle')
        
        def on_button_click(b):
            with self.output_context():
                callback(all_warnings)
        
        next_button.on_click(on_button_click)
        display(next_button)

class DescribePartitionColumnsList(ListNode):
    default_name = 'Describe Partition Columns'
    default_description = 'This node allows users to describe the columns across multiple partition tables.'

    def extract(self, item):
        clear_output(wait=True)

        con = self.item["con"]
        partition_tables = self.para["tables"]
        partition_name = self.para["partition_name"]
        display(HTML(f"{running_spinner_html} Checking columns for partition <i>{partition_name}</i>..."))
        
        create_progress_bar_with_numbers(0, doc_steps)
        self.progress = show_progress(1)

        self.input_item = item

        table_pipeline = self.para["data_project"].table_pipelines[partition_tables[0]]
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        sample_size = 5
        
        partition_summary = self.get_sibling_document("Create Partition Summary")
        
        outputs = []
        
        columns = sorted(columns)
        
        for i in range(0, len(columns), 30):
            chunk_columns = columns[i:i + 30]
            sample_df = table_pipeline.get_samples(con, columns=chunk_columns, sample_size=sample_size)
            sample_df = sample_df.applymap(truncate_cell)
            table_desc = sample_df.to_csv(index=False, quoting=1)
            outputs.append((table_desc, partition_summary, chunk_columns))
        
        return outputs
    
    def run(self, extract_output, use_cache=True):
        table_desc, partition_summary, column_names = extract_output

        template = f"""You have the following sample from a table partition:
{table_desc}
Partition summary: {partition_summary['partition_summary']}

Tasks: 
(1) Describe the columns in the partition tables.
(2) If the original column name is not descriptive, provide a new name. Otherwise, keep the original name.
Make sure there are no duplicated new column names

Return in the following format:
```json
{{
    "{column_names[0]}": ["Short description in < 10 words", "new_column_name"],
    ...
}}
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        processed_string = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
    
        checks = [
            (lambda jc: isinstance(jc, dict), "The returned JSON code is not a dictionary."),
            (lambda jc: all(key in column_names for key in jc.keys()), "The dictionary contains keys that are not in the column_names list."),
            (lambda jc: all(isinstance(value, list) and len(value) == 2 for value in jc.values()), "Not all values in the dictionary are lists with exactly 2 elements."),
            (lambda jc: all(isinstance(value[0], str) for value in jc.values()), "Not all column descriptions are strings."),
            (lambda jc: all(isinstance(value[1], str) for value in jc.values()), "Not all new column names are strings."),
            (lambda jc: len(set(value[1] for value in jc.values())) == len(jc), "New column names are not unique."),
        ]

        for check, error_message in checks:
            if not check(json_code):
                raise ValueError(f"Validation failed: {error_message}")
        
        return json_code

    def run_but_fail(self, extract_output, use_cache=True):
        default_response = {column: [column, column] for column in extract_output[2]}
        return default_response
    
    def merge_run_output(self, run_outputs):
        merged_output = {}
        for run_output in run_outputs:
            merged_output.update(run_output)
        return merged_output
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        json_code = run_output
        self.progress.value += 1

        partition_tables = self.para["tables"]
        partition_name = self.para["partition_name"]
        query_widget = self.item["query_widget"]

        create_explore_button(query_widget, self.para["data_project"].table_pipelines[partition_tables[0]])
        con = self.item["con"]
        schema = self.para["data_project"].table_pipelines[partition_tables[0]].get_schema(con)

        rows_list = []

        for col in schema:
            rows_list.append({
                "Column": col,
                "Summary": json_code[col][0],
                "New Column Name": json_code[col][1],
                "Renamed?": "✔️ Yes" if json_code[col][1] != col else "❌ No"
            })

        df = pd.DataFrame(rows_list)
        
        editable_columns = [False, True, True, False]
        grid = create_dataframe_grid(df, editable_columns, reset=True)

        next_button = widgets.Button(
            description='Accept Rename',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )  
        
        reject_button = widgets.Button(
            description='Reject Rename',
            disabled=False,
            button_style='danger',
            tooltip='Click to submit',
            icon='close'
        )
        
        def on_button_clicked2(b):
            with self.output_context():
                new_df = grid_to_updated_dataframe(grid)
                new_df["New Column Name"] = new_df["Column"]
                document = new_df.to_json(orient="split")
                get_column_desc_from_df(new_df)
                callback(document)
                
        def get_column_desc_from_df(df):
            table_object = self.para["table_object"]
            for index, row in df.iterrows():
                table_object.column_desc[row["New Column Name"]] = row["Summary"]
        
        def on_button_clicked(b):
            with self.output_context():
                new_df = grid_to_updated_dataframe(grid)
                new_df["New Column Name"] = new_df["New Column Name"].apply(clean_column_name)
                document = new_df.to_json(orient="split")
                get_column_desc_from_df(new_df)
                callback(document)

        next_button.on_click(on_button_clicked)
        reject_button.on_click(on_button_clicked2)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            if (hasattr(self, 'para') and 
                isinstance(self.para, dict) and 
                isinstance(self.para.get('cocoon_stage_options'), dict) and 
                self.para['cocoon_stage_options'].get('rename') is False):
                on_button_clicked2(reject_button)
                return
            on_button_clicked(next_button)
            return
        
        display(HTML(f"😎 We have described the columns for partition <i>{partition_name}</i>:"))
        display(grid)
        display(HBox([reject_button, next_button]))
        

class CreatePartitionSummary(Node):
    default_name = 'Create Partition Summary'
    default_description = 'This node creates a summary of the partition based on a sample table.'

    def extract(self, item):
        clear_output(wait=True)
        self.input_item = item
        
        con = self.item["con"]
        partition_tables = self.para["tables"]
        partition_name = self.para["partition_name"]
        
        display(HTML(f"{running_spinner_html} Generating partition summary for <i>{partition_name}</i>..."))
        create_progress_bar_with_numbers(0, doc_steps)
        self.progress = show_progress(1)
        
        sample_size = 5

        sample_table_name = partition_tables[0]
        table_pipeline = self.para["data_project"].table_pipelines[sample_table_name]
        schema = table_pipeline.get_schema(con)
        columns = list(schema.keys())
        sample_df = table_pipeline.get_samples(con, columns=columns, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        table_desc = sample_df.to_csv(index=False, quoting=1)

        return partition_name, sample_table_name, table_desc, partition_tables

    def run(self, extract_output, use_cache=True):
        partition_name, sample_table_name, table_desc, partition_tables = extract_output

        template = f"""You have the following partition names: {', '.join(partition_tables)}

A sample from the '{sample_table_name}' partition:
{table_desc}

Summarize the table based on this sample and the partition names. Explain how the partitions might be related and what they could represent collectively.

Now, provide your summary in short simple SVO sentences and < 500 chars.
Also, provide a new name for the partition that summarizes all tables.
Return in the following format:
```yml
reasoning: The table contains multiple related paritions for different years
partition_summary: >-
    The tables are about the order processing system, for years 2019-2021...
new_partition_name: OrderProcessingSystem
```
"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)

        summary = response['choices'][0]['message']['content']
        assistant_message = response['choices'][0]['message']
        messages.append(assistant_message)
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']['content'])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)

        if not isinstance(summary, dict):
            raise ValueError("Summary is not a dictionary")

        required_keys = ['partition_summary', 'new_partition_name']
        for key in required_keys:
            if key not in summary:
                raise KeyError(f"'{key}' is missing from the summary")

        return summary

    def run_but_fail(self, extract_output, use_cache=True):
        partition_name, sample_table_name, table_desc, partition_tables = extract_output
        return {
            'partition_summary': 'No summary available',
            'new_partition_name': 'DefaultPartitionName'
        }

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        summary = run_output 
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        self.progress.value += 1

        partition_name, _, _, partition_tables = extract_output
        query_widget = self.item["query_widget"]
        data_project = self.para["data_project"]
        
        create_explore_button(query_widget, 
                        table_name=partition_tables,
                        logical_to_physical=data_project.table_pipelines)

        display(HTML(f"📝 Here is the partition summary. Please review and edit if necessary:"))
        
        text_area, char_count_label = create_text_area_with_char_count(summary['partition_summary'], max_chars=500)
        layout = Layout(display='flex', justify_content='space-between', width='100%')
        display(HBox([text_area, char_count_label], layout=layout))

        display(HTML(f"🏷️ Suggested new partition name: {summary['new_partition_name']}"))
        new_name_input = widgets.Text(value=summary['new_partition_name'])
        display(new_name_input)

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                print("Submission received.")
                
                final_summary = {
                    'partition_summary': text_area.value,
                    'new_partition_name': new_name_input.value
                }
                table_object = self.para["table_object"]
                table_object.table_summary = final_summary['partition_summary']
                table_object.table_name = final_summary['new_partition_name']
                table_object.partitions = partition_tables
                callback(final_summary)

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)

        display(submit_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_clicked(submit_button)
            return


class WritePartitionYMLCode(Node):
    default_name = 'Write Partition Code'
    default_description = 'This node allows users to write the code for the partition.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        border_style = """
<style>
.border-class {
    border: 1px solid black;
    padding: 10px;
    margin: 10px;
}
</style>
"""

        tab_data = []
        labels = []
        file_names = []
        contents = []
  
        table_object = self.para["table_object"]
        table_object.columns = list(table_object.column_desc.keys())
        table_name = table_object.table_name
        
        data_project = self.para["data_project"]
        data_project.table_object[table_name] = table_object
        data_project.partition_mapping[table_name] = table_object.partitions
        
        yml_dict = table_object.create_dbt_schema_dict()
        yml_content = yaml.dump(yml_dict, default_flow_style=False)
        
        formatter = HtmlFormatter(style='default')
        css_style = f"<style>{formatter.get_style_defs('.highlight')}</style>"

        highlighted_yml = highlight(yml_content, YamlLexer(), formatter)

        bordered_content = f'<div class="border-class">{highlighted_yml}</div>'

        combined_html = css_style + border_style + bordered_content

        tab_data.append(("yml", combined_html))
        labels.append("YML")
        file_names.append(f"{table_name}.yml")
        contents.append(yml_content)

        tabs = create_dropdown_with_content(tab_data) 
        
        if "dbt_directory" in self.para:
            if not file_exists(os.path.join(self.para["dbt_directory"], "partition")):
                create_directory(os.path.join(self.para["dbt_directory"], "partition"))
            file_names = [os.path.join(self.para["dbt_directory"], "partition", file_name) for file_name in file_names]
       
        create_progress_bar_with_numbers(4, doc_steps)
        display(HTML(f"🎉 Congratulations! Below is the stage codes."))
        display(tabs)
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)
        save_files_click(save_button)  
        
        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                document = {}
                
                for i in range(len(labels)):
                    label = labels[i]
                    content = contents[i]
                    document[label] = content

                callback(document)

        submit_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            overwrite_checkbox.value = True
            save_files_click(save_button)  
            on_button_clicked(submit_button)
            return
        
        
        display(submit_button)
        
def read_partition_ymls_and_update_project(data_project, dbt_directory):
    partition_dir = os.path.join(dbt_directory, "partition")
    
    if not file_exists(partition_dir):
        print(f"Partition directory not found: {partition_dir}")
        return

    for filename in list_files_in(partition_dir):
        if filename.endswith(".yml"):
            file_path = os.path.join(partition_dir, filename)
            
            file_content = read_from(file_path)
            yml_content = yaml.safe_load(file_content)

            table_object = PartitionTable()
            table_object.read_attributes_from_dbt_schema_yml(yml_content)
            table_name = table_object.table_name

            data_project.table_object[table_name] = table_object
            
            if hasattr(table_object, 'partitions'):
                data_project.partition_mapping[table_name] = table_object.partitions
            else:
                print(f"Warning: 'partitions' not found for table {table_name}")

    print(f"Processed {len(data_project.table_object)} tables from YML files.")
    print(f"Updated partition mapping for {len(data_project.partition_mapping)} tables.")

    return data_project


class DecidePartition(Node):
    default_name = 'Decide Partition'
    default_description = 'This allows users to view and endorse or reject table partitions based on schema similarity.'

    def extract(self, item):
        data_project = self.para["data_project"]
        all_tables = data_project.tables
        
        partitions = {}
        for table_name, attributes in all_tables.items():
            attr_set = frozenset(attributes.keys())
            if attr_set not in partitions:
                partitions[attr_set] = []
            partitions[attr_set].append(table_name)
        
        partitions_with_multiple_tables = {attr_set: tables for attr_set, tables in partitions.items() if len(tables) > 1}
        
        return partitions_with_multiple_tables

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        query_widget = self.item["query_widget"]
        data_project = self.para["data_project"]
        dbt_directory = self.para.get("dbt_directory")
        
        create_explore_button(query_widget, 
                              table_name=list(data_project.table_pipelines.keys()),
                              logical_to_physical=data_project.table_pipelines)
        

        
        if extract_output is None or not extract_output:
            print("No partitions with multiple tables found.")
            callback([])
            return

        partition_data = []
        for idx, (attr_set, tables) in enumerate(extract_output.items()):
            partition_data.append({
                'Partition': f'Partition{idx}',
                'Tables': ', '.join(tables),
                'Attributes': ', '.join(attr_set)
            })

        df = pd.DataFrame(partition_data)

        html_content = """
        🤓 We've found partitions with the same schema. Here are the partitions:
        """
        html_content += df.to_html(index=False, classes='table table-striped', border=0)

        display(HTML(html_content))

        endorse_button = widgets.Button(description="Endorse All Partitions", button_style='success', icon='check')
        reject_button = widgets.Button(description="Reject All Partitions", button_style='danger', icon='times')

        def on_endorse(b):
            endorsed_partitions = list(extract_output.values())
            clear_output(wait=True)
            callback(endorsed_partitions)

        def on_reject(b):
            clear_output(wait=True)
            callback([])

        endorse_button.on_click(on_endorse)
        reject_button.on_click(on_reject)

        buttons = [reject_button, endorse_button]
        
        def on_express(b):
            clear_output(wait=True)
            updated_data_project = read_partition_ymls_and_update_project(data_project, dbt_directory)
            self.para["data_project"] = updated_data_project
            print("✅ Express load completed. Partitions have been updated.")
            callback([])
        
        if dbt_directory and file_exists(os.path.join(dbt_directory, "partition")):
            print("🚀 Found existing partitions. You can express load them.")
            express_button = widgets.Button(description="Express Load", button_style='info', icon='fast-forward')
            express_button.on_click(on_express)
            buttons.append(express_button)
            
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if len(buttons) == 2:
                on_endorse(None)
            elif len(buttons) == 3:
                on_express(None)
            return
            
        button_box = widgets.HBox(buttons)
        display(button_box)
        
class PrintPartition(Node):
    default_name = 'Print Partition'
    default_description = 'This is a dummy node to print partition information.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        print(f"Dummy Print Partition for: {self.para['partition_name']}")
        print(f"Tables: {self.para['tables']}")
        callback({})

def create_cocoon_describe_partition_workflow(con, query_widget=None, partition_name=None, tables=None, para=None, output=None):
    if para is None:
        para = {}
        
    if query_widget is None:
        query_widget = QueryWidget(con)
    
    item = {
        "con": con,
        "query_widget": query_widget
    }
    
    workflow_para = {
        "partition_name": partition_name,
        "tables": tables,
        "table_object": PartitionTable()
    }
    
    for key, value in para.items():
        workflow_para[key] = value

    describe_workflow = Workflow("Describe Partition Workflow", 
                                 item=item, 
                                 description="A workflow to describe partition",
                                 para=workflow_para,
                                 output=output)
    
    describe_workflow.add_to_leaf(CreatePartitionSummary(output=output))
    describe_workflow.add_to_leaf(DescribePartitionColumnsList(output=output))
    describe_workflow.add_to_leaf(WritePartitionYMLCode(output=output))
    
    
    return query_widget, describe_workflow


class DescribePartitionForAll(MultipleNode):
    default_name = 'Describe Partition For All'
    default_description = 'This describes all partitions'

    def construct_node(self, partition_name, tables=None, all_partitions=None, idx=0, total=0):
        if all_partitions is None:
            all_partitions = []
            
        if self.item is not None:
            con = self.item.get("con", None)
            query_widget = self.item.get("query_widget", None)
        else:
            con = None
            query_widget = None
        
        para = {
            "partition_name": partition_name, 
            "partition_idx": idx,
            "all_partitions": all_partitions,
            "partition_total": total
        }
        
        for key in self.para:
            para[key] = self.para[key]
        
        _, workflow = create_cocoon_describe_partition_workflow(con=con, query_widget=query_widget, 
                                                                partition_name=partition_name, tables=tables,
                                                                para=para, output=self.output)
        
        return workflow

    def extract(self, item):
        decide_partition_result = self.get_sibling_document('Decide Partition')
        
        if not decide_partition_result:
            self.elements = []
            self.nodes = {}
            return
        
        self.elements = [f"Partition_{idx}" for idx in range(len(decide_partition_result))]
        self.nodes = {}
        
        for idx, (partition_name, tables) in enumerate(zip(self.elements, decide_partition_result)):
            self.nodes[partition_name] = self.construct_node(partition_name, tables, self.elements, idx, len(self.elements))


class GroupSimilarTables(Node):
    default_name = 'Group Similar Tables'
    default_description = 'This node groups similar tables without primary/foreign keys'

    def extract(self, item):
        clear_output(wait=True)
        display(HTML(f"🤓 Futher grouping tables ..."))
        self.progress = show_progress(1)

        data_project = self.para["data_project"]
        join_graph_dict = data_project.join_graph_dict(add_time_keys=False)
        join_graph = join_graph_dict['join_graph']

        tables_without_keys = []
        for table_info in join_graph:
            table_name = table_info['table_name']
            primary_key = table_info.get('primary_key')
            foreign_keys = table_info.get('foreign_keys', [])
            
            if not primary_key and not foreign_keys:
                tables_without_keys.append(table_name)

        yml_dict = data_project.describe_project_yml(tables=tables_without_keys, limit=9999)
        
        return tables_without_keys, yml_dict

    def run(self, extract_output, use_cache=True):
        tables_without_keys, yml_dict = extract_output
        
        if not tables_without_keys:
            return {"groups": []}

        template = f"""You are tasked with grouping similar tables. 
Ideally, for each group of tables, they have common attributes to join.
Here are the tables and their descriptions:

{yaml.dump(yml_dict)}

Please group these tables based on their similarities and potential join attributes. For each group, provide:
1. A group name
2. A brief short summary of group (and its focus that makes it different from other groups)
3. The table names in the group (each table should belong to exactly one group)
4. Potential attributes that could be used to join these tables (if any and >1 table)

Return the result in the following YAML format:
```yml
groups:
  - name: Group1Name
    summary: The group is about ... 
    tables:
      - Table1Name
      - Table2Name
    join_info: The tables can be joined on ...
...
```
"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        result = yaml.safe_load(yml_code)
        
        return result
    
    def run_but_fail(self, extract_output, use_cache=True):
        tables_without_keys, yml_dict = extract_output
        
        if not tables_without_keys:
            return {"groups": []}

        dummy_group = {
            "name": "All Other Tables",
            "summary": "This group contains all other tables.",
            "tables": tables_without_keys,
        }

        result = {"groups": [dummy_group]}
        return result

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        result = run_output

        data = []
        for group in result['groups']:
            data.append({
                'Group Name': group['name'],
                'Group Summary': group['summary'],
                'Tables': group['tables'],
                'Join Info': group.get('join_info', '')
                
            })
        
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True, True]
        reset = True
        editable_list = {
            'Tables': {},
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset, editable_list=editable_list)
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                document = df.to_json(orient="split")
                data_project = self.para["data_project"]
                data_project.groups = df
                callback(document)
            
        next_button.on_click(on_button_click)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        display(grid)
        display(next_button)
        


class ReorderRelationToStory(Node):
    default_name = 'Reorder Relation To Story'
    default_description = 'This reorders the relation to story'

    def extract(self, item):
        clear_output(wait=True)
        
        path = self.para["path"]
        display(HTML(f"📖 Understanding the process for <i>{'>'.join(path)}</i>..."))
        create_progress_bar_with_numbers(3, model_steps)
        
        relation_df = self.para["relation_df"]
        group_df = self.para["group_df"] 
        single_entity_result = self.para["single_entity_result"]

        desc_list = self.para["content"]
        group_desc = self.para["group_desc"]
        
        combined_desc_list = []

        for i, name in enumerate(desc_list, start=1):
            if name in group_desc:
                description = group_desc[name]["Description"]
                combined_desc_list.append(f"{i}. '{name}': {description}")
            else:
                combined_desc_list.append(f"{i}. '{name}': No description available")
        
        combined_desc = "\n".join(combined_desc_list)
        
        return relation_df, group_df, single_entity_result, combined_desc, desc_list

    def run(self, extract_output, use_cache=True):
        relation_df, group_df, single_entity_result, combined_desc, all_names = extract_output
        
        if len(all_names) == 1:
            return self.run_but_fail(extract_output, use_cache) 

        template = f"""For a database, you have the following 'Name': 'Description'
{combined_desc}
Now, tell a story using these elements. 
First, reorder them based on the sequence of events or logical flow. E.g., "Customer opens accounts" should come before "Customer deposits money".
Then, modify the description for each element:
(1) Make the story coherent. 
(2) If involving entities, be clear about how they are related.
(3) AVOID general verbs like 'relate', 'have', 'do'
(4) DONT make up character. E.g., "Customer *Tom* opens account" has a made up Tom.

Return in the following format:
```yml
reasoning: >
    The elements are ... The sequence should be ...
story:
    -   name: Element Name
        description: short and simple in < 10 words
    -   ...
```"""
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)

        if not isinstance(summary, dict):
            raise TypeError("summary must be a dictionary")

        required_keys = ['reasoning', 'story']
        for key in required_keys:
            if key not in summary:
                raise KeyError(f"summary is missing the '{key}' key")

        if not isinstance(summary['reasoning'], str):
            raise TypeError("summary['reasoning'] must be a string")
        if not isinstance(summary['story'], list):
            raise TypeError("summary['story'] must be a list")

        relation_names = set(relation_df['Relation Name'].tolist())
        group_names = set(group_df['Group Name'].tolist())
        single_entity_names = set(single_entity_result['Entity Name'].tolist())
        
        for i, item in enumerate(summary['story']):
            if not isinstance(item, dict):
                raise TypeError(f"Item {i} in summary['story'] must be a dictionary")
            if 'name' not in item or 'description' not in item:
                raise KeyError(f"Item {i} in summary['story'] is missing 'name' or 'description'")
            if not isinstance(item['name'], str) or not isinstance(item['description'], str):
                raise TypeError(f"'name' and 'description' in item {i} of summary['story'] must be strings")
            
            if item['name'] in relation_names:
                item['type'] = 'relation'
            elif item['name'] in group_names:
                item['type'] = 'group'
            elif item['name'] in single_entity_names:
                item['type'] = 'entity'
            else:
                raise ValueError(f"Name '{item['name']}' in story does not exist in the original relation, group, or single entity DataFrames")

        story_names = [item['name'] for item in summary['story']]
        all_names_set = set(all_names)
        
        if len(story_names) != len(set(story_names)):
            raise ValueError("Some names appear more than once in the story")
        
        if set(story_names) != all_names_set:
            missing_names = all_names_set - set(story_names)
            raise ValueError(f"The following names are missing from the story: {missing_names}")
        
        summary['story'] = [OrderedDict([('name', item['name']), ('description', item['description']), ('type', item['type'])]) for item in summary['story']]
        
        return summary
    
    def run_but_fail(self, extract_output, use_cache=True):
        relation_df, group_df, single_entity_result, _, all_names = extract_output
        story_list = []
        
        for name in all_names:
            if name in relation_df['Relation Name'].values:
                row = relation_df[relation_df['Relation Name'] == name].iloc[0]
                story_list.append(OrderedDict([
                    ("name", name),
                    ("description", row['Relation Description']),
                    ("type", "relation")
                ]))
            elif name in group_df['Group Name'].values:
                row = group_df[group_df['Group Name'] == name].iloc[0]
                story_list.append(OrderedDict([
                    ("name", name),
                    ("description", row['Group Summary']),
                    ("type", "group")
                ]))
            elif name in single_entity_result['Entity Name'].values:
                row = single_entity_result[single_entity_result['Entity Name'] == name].iloc[0]
                story_list.append(OrderedDict([
                    ("name", name),
                    ("description", row['Entity Description']),
                    ("type", "entity")
                ]))
            else:
                story_list.append(OrderedDict([
                    ("name", name),
                    ("description", "No description available"),
                    ("type", "unknown")
                ]))
        
        return {"reasoning": "Fail to run", "story": story_list}

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        relation_df, group_df, single_entity_result, _, _ = extract_output
        story_summary = run_output["story"]
        
        
        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check' 
        )
        
        def on_next_button_clicked(b):
            with self.output_context():
                callback(story_summary)
        
        next_button.on_click(on_next_button_clicked)

        viewer = self.para.get("viewer", False)
        
        if viewer:
            on_next_button_clicked(next_button)
            return 
        
        relation_to_entities = {}
        for idx, row in relation_df.iterrows():
            relation_to_entities[row['Relation Name']] = row['Entities']
            
        display_pages(total_page=len(story_summary), 
                      create_html_content=partial(create_html_content_er_story, relation_to_entities, story_summary, None))
        
        display(next_button)
        
        











        




class DescribeSQLList(ListNode):
    default_name = 'Describe SQL'
    default_description = 'This node describes the SQL queries associated with each model in the dbt lineage.'

    def extract(self, item):
        clear_output(wait=True)

        dbt_lineage = self.para['dbt_lineage']
        
        display(HTML(f"{running_spinner_html} Understanding SQL queries for dbt lineage..."))

        sql_query = """
        SELECT model_name, raw_code
        FROM model
        WHERE raw_code IS NOT NULL
        """

        df = dbt_lineage.conn.execute(sql_query).df()
        
        total = len(df)
        
        outputs = [(idx, total, row.model_name, row.raw_code) for idx, row in enumerate(df.itertuples(index=False))]
        
        self.progress = show_progress(len(outputs))

        return outputs
    
    def run(self, extract_output, use_cache=True):
        idx, total, model_name, sql_query = extract_output
        
        clear_output(wait=True)
        display(HTML(f"({idx+1}/{total}) Understanding the SQL for '{model_name}'..."))
        
        template = f"""SQL query for model '{model_name}':
{sql_query}

Summarize the SQL query's purpose and tag it:
1. filtering: selects/semi-joins
2. cleaning: e.g., trims, standardizes, formats columns
3. deduplication: e.g., select distinct, window functions/qualifying
4. featurization: e.g., extracting weekend/weekday from a date
5. integration: joins/unions tables (excluding semi-joins)
6. aggregation: groups data (sum, count, average, etc.)
7. other: significant tasks beyond the above

Respond with the following format:
```yml
summary: >-
    Brief summary of what the SQL query does.
tags:
    filtering: >-
        only active users `status = True` # reference the related codes
    other_applicable_tag: >-
        ...
```"""

        messages = [{"role": "user", "content": template}]
        
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        result = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(result, dict):
            raise TypeError("result must be a dictionary")
        
        required_keys = ['summary', 'tags']
        for key in required_keys:
            if key not in result:
                raise KeyError(f"result is missing the '{key}' key")
        
        if not isinstance(result['summary'], str):
            raise TypeError("result['summary'] must be a string")
        
        if not isinstance(result['tags'], dict):
            raise TypeError("result['tags'] must be a dictionary")
        
        for tag_key, tag_value in result['tags'].items():
            if not isinstance(tag_key, str) or not isinstance(tag_value, str):
                raise TypeError(f"Tag key and value must be strings: {tag_key}: {tag_value}")
        self.progress.value += 1
        
        return {model_name: result}

    def run_but_fail(self, extract_output, use_cache=True):
        _, _, model_name, _ = extract_output
        return {model_name: {"summary": "Failed to run", "tags": []}}

    def merge_run_output(self, run_outputs):
        return {k: v for d in run_outputs for k, v in d.items()}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        dbt_lineage = self.para['dbt_lineage']
        
        display(HTML("🎉 We have documented all the models..."))

        data = {
            'model_name': [],
            'summary': [],
            'tags': []
        }
        
        model_htmls = []
        
        for model_name, result in run_output.items():
            data['model_name'].append(model_name)
            data['summary'].append(result['summary'])
            data['tags'].append(result['tags'])
            
            raw_code_query = "SELECT raw_code FROM model WHERE model_name = ?"
            raw_code_df = dbt_lineage.conn.execute(raw_code_query, [model_name]).df()
            raw_code = raw_code_df['raw_code'][0] if not raw_code_df.empty else ""
            
            tag_html = "<ul>" + "".join(["<li>" + generate_tag_html(tag, description) + "</li>" for tag, description in result['tags'].items()]) + "</ul>"
            
            model_dict = OrderedDict([
                ("Description", result['summary']),
                ("Tags", tag_html),
                ("Model Code", highlight_sql(raw_code)),
            ])
            
            model_html = ""
            for key, value in model_dict.items():
                model_html += wrap_in_card(key, value)

            formatter = HtmlFormatter(style='monokai')
            css_style = formatter.get_style_defs('.highlight')
            html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <style>
        pre {{ line-height: 125%; }}
        .highlight {{
            background: #272822;
            color: #f8f8f2;
            border-radius: 4px;
            padding: 1em;
            margin: 1em 0;
            overflow-x: auto;
            font-size: 12px;
        }}
        {css_style}
        {tag_css}
        {table_css}
    </style>
</head>
<body>
    {model_html}
</body>
</html>
"""
            
            model_htmls.append([model_name, html])
        
        tabs = create_dropdown_with_content(model_htmls)
        display(tabs)


        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True]
        reset = True
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset)
        
        display(grid)
        
        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')
        
        def on_button_click(b):
            with self.output_context():
                updated_df = grid_to_updated_dataframe(grid, reset=reset)
                
                for _, row in updated_df.iterrows():
                    model_name = row['model_name']
                    summary = row['summary']
                    tags = ast.literal_eval(row['tags']) if isinstance(row['tags'], str) else row['tags']

                    dbt_lineage.conn.execute("""
                        UPDATE model
                        SET cocoon_description = ?
                        WHERE model_name = ?
                    """, (summary, model_name))

                    dbt_lineage.conn.execute("""
                        DELETE FROM model_cocoon_tag
                        WHERE model_name = ?
                    """, (model_name,))

                    for tag, description in tags.items():
                        dbt_lineage.conn.execute("""
                            INSERT INTO model_cocoon_tag (model_name, cocoon_tag, cocoon_description)
                            VALUES (?, ?, ?)
                            ON CONFLICT(model_name, cocoon_tag) DO UPDATE SET
                            cocoon_description = EXCLUDED.cocoon_description
                        """, (model_name, tag, description))
                
                document = df.to_json(orient="split")
                callback(document)
        
        next_button.on_click(on_button_click)
        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
        
        
class BuildColumnLineageForAll(MultipleNode):
    default_name = 'Build Column Lineage For All'
    default_description = 'This node builds column lineage from input to output columns for each model in the dbt lineage.'

    def extract(self, item):
        clear_output(wait=True)

        dbt_lineage = self.para['dbt_lineage']
        
        display(HTML(f"{running_spinner_html} Identifying models for column lineage..."))
        
        sql_query = """
WITH model_with_sql AS (
    SELECT DISTINCT model_name
    FROM model
    WHERE raw_code IS NOT NULL
),
model_with_columns AS (
    SELECT model_name
    FROM model_column
    GROUP BY model_name
    HAVING COUNT(*) > 0
),
model_with_parents AS (
    SELECT DISTINCT target_model_name AS child_model
    FROM model_lineage
),
parent_models_with_columns AS (
    SELECT DISTINCT ml.target_model_name AS child_model
    FROM model_lineage ml
    JOIN model_column mc ON ml.source_model_name = mc.model_name
    GROUP BY ml.target_model_name
    HAVING COUNT(DISTINCT mc.model_name) > 0
)
SELECT m.model_name
FROM model m
JOIN model_with_sql mws ON m.model_name = mws.model_name
JOIN model_with_columns mwc ON m.model_name = mwc.model_name
JOIN model_with_parents mwp ON m.model_name = mwp.child_model
JOIN parent_models_with_columns pmwc ON m.model_name = pmwc.child_model
        """

        df = dbt_lineage.conn.execute(sql_query).df()
        eligible_models = df['model_name'].tolist()

        self.elements = eligible_models
        total = len(eligible_models)

        self.progress = show_progress(total)

        self.nodes = {model_name: self.construct_node(model_name, idx, total) 
                      for idx, model_name in enumerate(eligible_models)}

        return self.nodes

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["model_name"] = element_name
        para["model_idx"] = idx
        para["total_models"] = total

        node = ColumnLineageForModel(para=para, id_para="model_name")
        node.inherit(self)
        return node
    
class ColumnLineageForModel(ListNode):
    default_name = 'Column Lineage For Model'
    default_description = 'This node builds column lineage for a specific model, processing columns in chunks.'

    def extract(self, item):
        clear_output(wait=True)

        model_name = self.para['model_name']
        dbt_lineage = self.para['dbt_lineage']
        model_idx = self.para["model_idx"]
        total_models = self.para["total_models"]
        
        def get_model_info(conn, model_name):
            sql_query = conn.execute("""
                SELECT raw_code
                FROM model
                WHERE model_name = ?
            """, [model_name]).fetchone()
            sql_query = sql_query[0] if sql_query else ''


            sql_info = conn.execute("""
                SELECT cocoon_tag, cocoon_description
                FROM model_cocoon_tag
                WHERE model_name = ?
            """, [model_name]).fetchone()
            sql_info = dict(zip(['summary', 'tags'], sql_info)) if sql_info else None

            columns = conn.execute("""
                SELECT column_name, description
                FROM model_column
                WHERE model_name = ?
            """, [model_name]).df().set_index('column_name').to_dict('index')

            parent_nodes = conn.execute("""
                SELECT source_model_name as parent_model
                FROM model_lineage
                WHERE target_model_name = ?
            """, [model_name]).df()['parent_model'].tolist()

            parent_columns = {}
            for parent in parent_nodes:
                parent_cols = conn.execute("""
                    SELECT column_name, description
                    FROM model_column
                    WHERE model_name = ?
                """, [parent]).df().set_index('column_name').to_dict('index')
                parent_columns[parent] = parent_cols

            return sql_query, sql_info, columns, parent_nodes, parent_columns

        display(HTML(f"{running_spinner_html} ({model_idx+1}/{total_models}) Building column lineage for <i>{model_name}</i>..."))

        sql_query, sql_info, columns, parent_nodes, parent_columns = get_model_info(dbt_lineage.conn, model_name)
        
        chunk_size = 20
        outputs = []
        
        for parent, parent_cols in parent_columns.items():
            parent_col_items = list(parent_cols.items())
            for i in range(0, len(parent_col_items), chunk_size):
                chunk_columns = dict(parent_col_items[i:i + chunk_size])
                outputs.append({
                    'model_name': model_name,
                    'sql_query': sql_query,
                    'sql_info': sql_info,
                    'columns': columns,
                    'parent_table': parent,
                    'parent_columns': {parent: chunk_columns}
                })
        
        self.progress = show_progress(len(outputs))
        
        return outputs

    def run(self, extract_output, use_cache=True):
        model_name = extract_output['model_name']
        sql_query = extract_output['sql_query']
        sql_info = extract_output['sql_info']
        columns = extract_output['columns']
        parent_table = extract_output['parent_table']
        parent_columns = extract_output['parent_columns'][parent_table]


        
        parent_columns_str = '\n'.join(
            f"{i+1}. '{col}': {parent_columns[col]['description'][:97] + '...' if parent_columns[col]['description'] and len(parent_columns[col]['description']) > 100 else parent_columns[col]['description']}" 
            if parent_columns[col]['description'] 
            else f"{i+1}. '{col}'" 
            for i, col in enumerate(parent_columns)
        )

        columns = OrderedDict(sorted(columns.items()))

        output_columns_str = '\n'.join(
            f"{i+1}. '{col}': {columns[col]['description'][:97] + '...' if columns[col]['description'] and len(columns[col]['description']) > 100 else columns[col]['description']}" 
            if columns[col]['description'] 
            else f"{i+1}. '{col}'" 
            for i, col in enumerate(columns)
        )
        
        template = f"""SQL:
{sql_query}

Description of the SQL: {sql_info['summary']}

We have the output columns: {output_columns_str}

Focus on these input columns for parent table '{parent_table}':
{parent_columns_str}

Describe how each column is used with tag:
1. direct: Copied without changes
2. filtering: Used in WHERE or HAVING clauses
3. cleaning: Standardized, formatted, or cast
4. deduplication: Used as ID (e.g., window by), criteria (qualify...) or DISTINCT
5. featurization: New features derived from it
6. integration: Join or union key
7. aggregation: Used in GROUP BY or aggregate functions
8. other: Significant tasks not covered above

Respond with the following format for each input column:
```yml
column_name:
    is_used: true/false
    # if is_used, fill out below...
    how_used:
        # for each applicable tag, reference related codes
        cleaning: >-
            cast to integer `CAST(number AS INT)`
    # is this column mapped to another column in the output, for column lineage
    # e.g., it is directly copied to, aggregated, or transformed 
    # if not, says null
    outputs:
        'output_column_name': how it is mapped
```"""

        messages = [{"role": "user", "content": template}]
        
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        result = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(result, dict):
            raise TypeError("result must be a dictionary")
        
        columns_to_remove = []
        
        for column_name, column_info in result.items():
            if not isinstance(column_info, dict):
                raise TypeError(f"Information for column '{column_name}' must be a dictionary")
            
            if 'is_used' not in column_info:
                raise KeyError(f"Column '{column_name}' is missing the 'is_used' key")
            
            if not isinstance(column_info['is_used'], bool):
                raise TypeError(f"'is_used' for column '{column_name}' must be a boolean")
            
            if column_info['is_used']:
                required_keys = ['how_used', 'outputs']
                for key in required_keys:
                    if key not in column_info:
                        raise KeyError(f"Column '{column_name}' is missing the '{key}' key")
                
                if not isinstance(column_info['how_used'], dict):
                    raise TypeError(f"'how_used' for column '{column_name}' must be a dictionary")
                
                if column_info['outputs'] and not isinstance(column_info['outputs'], dict):
                    raise TypeError(f"'outputs' for column '{column_name}' must be a dictionary, but currently of type {type(column_info['outputs'])}: {column_info['outputs']}")
            
                if column_info['outputs']:
                    valid_outputs = {}
                    for output_col, mapping in column_info['outputs'].items():
                        if output_col in columns:
                            valid_outputs[output_col] = mapping
                    
                    if valid_outputs:
                        column_info['outputs'] = valid_outputs
                    else:
                        columns_to_remove.append(column_name)
                else:
                    columns_to_remove.append(column_name)
        
            
        self.progress.value += 1
        
        return {model_name: {parent_table: result}}

    def run_but_fail(self, extract_output, use_cache=True):
        return {}

    def merge_run_output(self, run_outputs):
        merged = {}
        for output in run_outputs:
            for model_name, model_data in output.items():
                if model_name not in merged:
                    merged[model_name] = {}
                
                for parent_table, table_data in model_data.items():
                    if parent_table not in merged[model_name]:
                        merged[model_name][parent_table] = {}
                    
                    merged[model_name][parent_table].update(table_data)
        
        return merged
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        dbt_lineage = self.para['dbt_lineage']

        display(HTML("🎉 We have documented the column lineage..."))

        data = {
            'input_model': [],
            'input_column': [],
            'tags': [],
            'output_model': [],
            'output_columns': []
        }

        nodes = {}
        edges = []

        for model_name, model_data in run_output.items():
            for parent_table, table_data in model_data.items():
                if parent_table not in nodes:
                    nodes[parent_table] = set()
                if model_name not in nodes:
                    nodes[model_name] = set()

                for column_name, column_info in table_data.items():
                    if column_info['is_used']:
                        data['input_model'].append(parent_table)
                        data['input_column'].append(column_name)
                        
                        tags = ""
                        if column_info['how_used']:
                            tags = str(column_info['how_used'])
                        data['tags'].append(tags)
                        
                        data['output_model'].append(model_name)
                        output_cols = ""
                        if column_info['outputs']:
                            output_cols = str(column_info['outputs'])
                        data['output_columns'].append(output_cols)

                        nodes[parent_table].add(column_name)
                        
                        if column_info['outputs']:
                            for output_col, description in column_info['outputs'].items():
                                nodes[model_name].add(output_col)
                                edges.append((parent_table, column_name, model_name, output_col))
                        else:
                            edges.append((parent_table, column_name, model_name, None))

        nodes = {k: list(v) for k, v in nodes.items()}

        html_output = generate_schema_graph_graphviz(nodes, edges)
        display(HTML(html_output))

        def generate_model_html(model_name, model_data):
            model_html_list = []
            for parent_table, table_data in model_data.items():

                html = ""
                html += "<table class='table table-striped'>"
                html += "<thead><tr><th>Input Column</th><th>Usage</th><th>Output Columns</th></tr></thead>"
                html += "<tbody>"
                
                for column_name, column_info in table_data.items():
                    if column_info['is_used']:
                        usage_html = "".join([generate_tag_html(tag, description) + "<br>" for tag, description in column_info['how_used'].items()])

                        output_columns = column_info.get('outputs', {})
                        output_html = ""
                        if output_columns:
                            for output_col, description in output_columns.items():
                                output_html += f"<b>{output_col}</b>: {description}<br>"
                        
                        html += f"<tr><td>{column_name}</td><td>{usage_html}</td><td>{output_html}</td></tr>"
                
                html += "</tbody></table>"
                full_html = wrap_in_model_html(wrap_in_card(parent_table, html))
                model_html_list.append([parent_table, full_html])

            return model_html_list

        def wrap_in_model_html(content):
            formatter = HtmlFormatter(style='monokai')
            css_style = formatter.get_style_defs('.highlight')
            return f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <style>
        pre {{ line-height: 125%; }}
        .highlight {{
            background: #272822;
            color: #f8f8f2;
            border-radius: 4px;
            padding: 1em;
            margin: 1em 0;
            overflow-x: auto;
            font-size: 12px;
        }}
        {css_style}
        {tag_css}
        {table_css}
    </style>
</head>
<body>
    <div class="container">
        {content}
    </div>
</body>
</html>"""

        model_htmls = []
        for model_name, model_data in run_output.items():
            model_html_list = generate_model_html(model_name, model_data)
            model_htmls += model_html_list

        model_code_html = highlight_sql(extract_output[0]['sql_query'])
        model_htmls.append(["Model Code", model_code_html])

        tabs = create_dropdown_with_content(model_htmls)
        display(tabs)



        df = pd.DataFrame(data)

        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, False, True, False, True]
        reset = True
        editable_list = {}

        grid = create_dataframe_grid(df, editable_columns, reset=reset, editable_list=editable_list)

        display(grid)

        next_button = widgets.Button(description="Next Step", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                updated_df = grid_to_updated_dataframe(grid, reset=reset, editable_list=editable_list)
                
                for _, row in updated_df.iterrows():
                    tags = eval(row['tags']) if row['tags'] else {}
                    output_columns = eval(row['output_columns']) if row['output_columns'] else {}
                    
                    for tag, description in tags.items():
                        dbt_lineage.conn.execute("""
                            INSERT INTO model_column_cocoon_tag 
                            (source_model_name, source_column_name, target_model_name, cocoon_tag, cocoon_description)
                            VALUES (?, ?, ?, ?, ?)
                            ON CONFLICT(source_model_name, source_column_name, target_model_name, cocoon_tag) 
                            DO UPDATE SET cocoon_description = EXCLUDED.cocoon_description
                        """, (row['input_model'], row['input_column'], row['output_model'], tag, description))

                    for output_column, description in output_columns.items():
                        dbt_lineage.conn.execute("""
                            INSERT INTO column_lineage 
                            (source_model_name, source_column_name, target_model_name, target_column_name, cocoon_description)
                            VALUES (?, ?, ?, ?, ?)
                            ON CONFLICT(source_model_name, source_column_name, target_model_name, target_column_name) 
                            DO UPDATE SET cocoon_description = EXCLUDED.cocoon_description
                        """, (row['input_model'], row['input_column'], row['output_model'], output_column, description))

                document = updated_df.to_json(orient="split")
                callback(document)

        next_button.on_click(on_button_click)
        display(next_button)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
            return
            
    
        
    
    
        

    

class DBTLineageBuilder(Node):
    default_name = 'DBT Lineage Builder'
    default_description = 'This step reads the DBT manifest and builds the lineage graph.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        if 'dbt_lineage' in self.para:
            dbt_lineage = self.para['dbt_lineage']
        else:
            dbt_directory = self.para.get("dbt_directory")
            if not dbt_directory:
                display(HTML("<div style='color: red;'>Error: DBT directory not found in parameters.</div>"))
                return

            manifest_path = os.path.join(dbt_directory, 'target', 'manifest.json')
            if not os.path.exists(manifest_path):
                display(HTML(f"<div style='color: red;'>Error: {manifest_path} not found. Please ensure your DBT project is compiled.</div>"))
                return

            dbt_lineage = DbtLineage(dbt_directory = dbt_directory)

            json_content = read_from(manifest_path)
            manifest_dict = json.loads(json_content)

            dbt_lineage.populate_from_manifest(manifest_dict)

            self.para['dbt_lineage'] = dbt_lineage

        display(HTML("<div style='line-height: 1.2;'><h2>🎉 Your DBT model lineage is ready for RAG</h2><em>We learned them from <code>target/manifest.json</code>, and your DBT copilot is ready for use.</em></div>"))
        dbt_lineage.display_lineage_widgets()


        text_input = widgets.Text(
            placeholder='Ask any question about the DBT project...',
            layout=widgets.Layout(width='70%')
        )

        send_button = widgets.Button(
            description='Send',
            button_style='primary',
            layout=widgets.Layout(width='10%')
        )

        def on_send(b):
            with self.output_context():
                message = text_input.value
                callback({"question": message})
                return

        send_button.on_click(on_send)

        chat_input = widgets.HBox([text_input, send_button], layout=widgets.Layout(width='100%', margin='10px 0px'))

        display(chat_input)

            
class DBTRAGBranchStep(Node):
    default_name = 'DBT RAG Branch Options'
    default_description = 'Choose a DBT RAG option for your project'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        style = HTML("""
        <style>
        .widget-checkbox { margin-right: 10px; }
        .option-label { 
            font-size: 14px; 
        }
        em { 
            font-size: 12px; 
            color: #666; 
            display: block;
            margin-top: -5px;
        }
        code {
            font-family: 'Fira Code', 'Consolas', 'Monaco', 'Andale Mono', 'Ubuntu Mono', monospace;
            font-size: 14px;
            line-height: 1.4;
        }
        pre.command-line {
            background-color: #f0f0f0;
            border-radius: 5px;
            padding: 10px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            color: #333;
            border: 1px solid #e1e4e8;
        }
        pre.command-line code {
            background-color: transparent;
            padding: 0;
            border: none;
                }
        </style>
        """)
        display(style)

        header_html = f'''
        <div style="display: flex; align-items: center;">
        <img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">
        <div style="margin: 0; padding: 0;">
        <h1 style="margin: 0; padding: 0;">Cocoon: RAG for DBT</h1>
        <em style="margin: 0; padding: 0;">RAG your large and complex DBT project for Multi-Step LLM Agents</em>
        </div>
        </div><hr>
        '''

        display(HTML(header_html))

        build_rag_label = widgets.HTML(value="<div style='line-height: 1.2;'><h2>⚙️ Offline: Build RAG for DBT</h2><em>The more we prepare offline, the better the online query will be</em></div>")
        build_rag_options = [
            ('<span class="option-label"><div><strong>Model Lineage (✅ Done)</strong></div><div><em><b>🕸️ Graph-RAG:</b> Read model lineage directly from manifest.json</em></div></span>', True, True),
            ('<span class="option-label"><div><strong>Model Augmented Documentation (✅ Recommended)</strong></div><div><em><b>🔍 Vector-RAG:</b> LLMs explore models and augment document (fast)</em></div></span>', True, False),
            '<span class="option-label"><div><strong>Column Lineage</strong></div><div><em><b>🕸️ Graph-RAG:</b> LLMs explore columns and build lineage (for deeper analysis)</em></div></span>',
            ('<span class="option-label"><div><strong>Macro Augmented Documentation (🔜 Coming soon...)</strong></div><div><em><b>🧰 Tool-RAG:</b> LLMs explore and prepare Macro </em></div></span>', False, True),
            ('<span class="option-label"><div><strong>Metric Augmented Documentation (🔜 Coming soon...)</strong></div><div><em><b>🧰 Tool-RAG:</b> LLMs explore and prepare Metric</em></div></span>', False, True),
            ('<span class="option-label"><div><strong>Test Augmented Documentation (🔜 Coming soon...)</strong></div><div><em><b>🧰 Tool-RAG:</b> LLMs explore and prepare Test </em></div></span>', False, True)
        ]
        build_rag_checkboxes_widget, build_rag_checkboxes = create_html_radio_buttons(build_rag_options, radio=False)
        build_rag_button = widgets.Button(description="Build RAG", button_style='success', icon='database')

        use_rag_label = widgets.HTML(value="<div style='line-height: 1.2;'><h2>🚀 Online: Query DBT using RAG</h2><em>Query your large and complex DBT project with the prepared RAG</em></div>")
        use_rag_options = [
            '<span class="option-label"><div><strong>DBT Copilot using RAG</strong></div><div><em><b>🤖 General-purpose:</b> Copilot for pipeline exploration, maintenance, and prototyping</em></div></span>',
            '<span class="option-label"><div><strong>Augment yml Document</strong></div><div><em><b>📝 Augmentation:</b> Augment existing yml documentation from RAG (column lineage required)</em></div></span>'
        ]
        use_rag_radio_widget, use_rag_radio_buttons = create_html_radio_buttons(use_rag_options, radio=True)
        use_rag_button = widgets.Button(description="Use RAG", button_style='info', icon='play')

        build_rag_container = widgets.VBox([
            build_rag_label,
            widgets.VBox([build_rag_checkboxes_widget], layout=widgets.Layout(border='1px solid #ddd', padding='10px', margin='5px')),
            build_rag_button
        ])

        use_rag_container = widgets.VBox([
            use_rag_label,
            widgets.VBox([use_rag_radio_widget], layout=widgets.Layout(border='1px solid #ddd', padding='10px', margin='5px')),
            use_rag_button
        ])

        hr = widgets.HTML(value='<hr>')

        display(widgets.VBox([build_rag_container, hr, use_rag_container]))
            
        def on_build_rag_click(b):
            with self.output_context():
                def get_selected_options():
                    option_keys = [
                        'model_lineage',
                        'model_doc',
                        'column_lineage',
                        'macro_doc',
                        'metric_doc',
                        'test_doc'
                    ]
                    
                    selected_options = {}
                    for key, option, checkbox in zip(option_keys, build_rag_options, build_rag_checkboxes):
                        if isinstance(option, tuple):
                            selected_options[key] = checkbox.value
                        else:
                            selected_options[key] = checkbox.value if not checkbox.disabled else False
                    
                    return selected_options
                
                selected_options = get_selected_options()
                self.para["cocoon_dbt_rag_options"] = selected_options
                
                print(selected_options)
                callback({
                    "next_node": "DBT RAG Preparation Workflow"
                })

        def on_use_rag_click(b):
            with self.output_context():
                def get_selected_option():
                    for option, radio_button in zip(use_rag_options, use_rag_radio_buttons):
                        if radio_button.value:
                            if "DBT Copilot" in option:
                                return "rag copilot"
                            elif "Augment yml Document" in option:
                                return "documentation"
                    return None

                selected_option = get_selected_option()
                print(selected_option)
                
                if not selected_option:
                    print("⚠️ Please select an option to use RAG")
                    return
                
                callback({
                    "next_node": selected_option
                })

        build_rag_button.on_click(on_build_rag_click)
        use_rag_button.on_click(on_use_rag_click)
        
class DBTLineageConfig(Node):
    default_name = 'DBT Lineage Configuration'
    default_description = 'This step allows you to configure the DBT project and build the lineage graph.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        dbt_directory_input = widgets.Text(
            value=self.para.get("dbt_directory", "./dbt_project"),
            layout=widgets.Layout(width='70%')
        )

        next_button = widgets.Button(description="Build RAG", button_style='success', icon='database', layout=widgets.Layout(width='10%'))



        def on_button_click(b):
            with self.output_context():
                dbt_directory = dbt_directory_input.value

                manifest_path = os.path.join(dbt_directory, 'target', 'manifest.json')
                if not file_exists(manifest_path):
                    display(HTML(f"<div style='color: red;'>Error: {manifest_path} not found. Please ensure your DBT project is compiled.</div>"))
                    return

                self.para["dbt_directory"] = dbt_directory

                callback({
                    "dbt_directory": dbt_directory,
                })

        next_button.on_click(on_button_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "dbt_directory" in self.para:
                on_button_click(next_button)
                return
        
        display(HTML("<div style='line-height: 1.2;'><h2>🛠️ Provide your DBT Directory for RAG</h2><em>Ensure it's compiled with <code>target/manifest.json</code> available. Prefer full path than relative path.</em></div>"))
        display(widgets.HBox([dbt_directory_input, next_button]))


class ProcessUserQuestion(Node):
    default_name = 'Process User Question'
    default_description = 'This step processes the user question from the DBT Lineage Builder.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        question = self.get_sibling_document('DBT Lineage Builder').get("question", "No question found")
        dbt_lineage = self.para['dbt_lineage']

        chat = ChatHTMLGenerator()
        chat.display()

        agent = DBTLLMAgent(dbt_lineage, chat_ui=chat, debug=False)

        def serve_user_question(question):
            agent.process_user_question(question)

        follow_up_button = widgets.Button(
            description='Follow Up',
            icon='arrow-right',
            button_style='primary',
            layout=widgets.Layout(width='15%'),
            tooltip='Ask a follow-up question'
        )

        new_question_input = widgets.Text(
            placeholder='Enter your question here...',
            layout=widgets.Layout(width='70%')
        )

        new_button = widgets.Button(
            description='New',
            icon='plus',
            button_style='warning',
            layout=widgets.Layout(width='15%'),
            tooltip='Start a new conversation'
        )

        warning_widget = widgets.HTML(
            value='',
            layout=widgets.Layout(width='100%', margin='10px 0px')
        )

        def on_follow_up(b):
            new_question = new_question_input.value
            if new_question:
                serve_user_question(new_question)
                new_question_input.value = ''
            update_warning()

        def on_new(b):
            chat.clear_all_messages()
            nonlocal agent
            agent = DBTLLMAgent(dbt_lineage, chat_ui=chat, debug=False)
            warning_widget.value = ''
            new_question = new_question_input.value
            if new_question:
                serve_user_question(new_question)
                new_question_input.value = ''

        follow_up_button.on_click(on_follow_up)
        new_button.on_click(on_new)

        def update_warning():
            if len(agent.conversation_history) > 10:
                warning_widget.value = '<div style="color: red">Warning: Conversation is getting long. Consider starting a new conversation for better performance.</div>'
            else:
                warning_widget.value = ''

        new_question_box = widgets.HBox([new_question_input, follow_up_button, new_button], 
                                        layout=widgets.Layout(width='100%', margin='20px 0px'))

        display(widgets.VBox([new_question_box, warning_widget]))

        serve_user_question(question)


def create_cocoon_dbt_explore_workflow(con=None, output=None, para={}, viewer=False):
    
    main_workflow = Workflow("DBT Project Explore Workflow", 
                        item = {},
                        description="A workflow to explore a DBT project",
                        output=output,
                        para=para)

    main_workflow.add_to_leaf(DBTLineageConfig(output=output))
    main_workflow.add_to_leaf(DBTLineageBuilder(output=output))
    main_workflow.add_to_leaf(ProcessUserQuestion(output=output)) 

    
    
    
    return main_workflow
    

def create_cocoon_dbt_rag_prep_workflow(para={}, output=None):

    rag_prep_workflow = Workflow("DBT RAG Preparation Workflow", 
                                 description="A workflow to prepare RAG for DBT project",
                                 para=para,
                                 output=output)

    rag_prep_workflow.add_to_leaf(DescribeSQLList(output=output))
    rag_prep_workflow.add_to_leaf(BuildColumnLineageForAll(output=output))

    return rag_prep_workflow

class DbtLineage:
    def __init__(self, dbt_directory='', name=None):
        self.conn = duckdb.connect(':memory:')
        self.dbt_directory = dbt_directory
        
        if name:
            self.name = name
        elif dbt_directory:
            self.name = os.path.basename(os.path.normpath(dbt_directory))
        else:
            self.name = ""
        
        self._create_tables()

        self.tables = {}
                
        self.table_object = {}

    def get_directory(self):
        return self.dbt_directory
    
    def get_name(self):
        return self.name
    
    def _create_tables(self):
        self.conn.execute("""CREATE TABLE model (
    model_name VARCHAR PRIMARY KEY,
    database VARCHAR,
    schema VARCHAR,
    table_name VARCHAR,
    original_file_path VARCHAR,
    raw_code TEXT,
    compiled_path VARCHAR,
    compiled_code TEXT,
    yml_path VARCHAR,
    materialized VARCHAR,
    description TEXT,
    cocoon_description TEXT
)""")
        
        self.conn.execute("""CREATE TABLE model_cocoon_tag (
    model_name VARCHAR,
    cocoon_tag VARCHAR,
    cocoon_description TEXT,
    UNIQUE(model_name, cocoon_tag)
)""")
        
        self.conn.execute("""
CREATE TABLE model_column_cocoon_tag (
    source_model_name VARCHAR,
    source_column_name VARCHAR,
    target_model_name VARCHAR,
    cocoon_tag VARCHAR,
    cocoon_description TEXT,
    UNIQUE(source_model_name, source_column_name, target_model_name, cocoon_tag)
)""")
                
        self.conn.execute("""
CREATE TABLE model_lineage (
    source_model_name VARCHAR,
    target_model_name VARCHAR,
    UNIQUE(source_model_name, target_model_name)
)""")
        
        self.conn.execute("""
CREATE TABLE column_lineage (
    source_model_name VARCHAR,
    source_column_name VARCHAR,
    target_model_name VARCHAR,
    target_column_name VARCHAR,
    cocoon_description TEXT,
    UNIQUE(source_model_name, source_column_name, target_model_name, target_column_name)
)""")
        
        self.conn.execute("""
CREATE TABLE model_column (
    model_name VARCHAR,
    column_name VARCHAR,
    description TEXT,
    cocoon_description TEXT,
    UNIQUE(model_name, column_name)
)""")

    def add_or_update_model(self, model_data):
        columns = ', '.join(model_data.keys())
        placeholders = ', '.join(['?' for _ in model_data])
        values = tuple(model_data.values())
        
        update_columns = [k for k in model_data.keys() if k != 'model_name']
        if update_columns:
            update_clause = ', '.join([f"{k} = excluded.{k}" for k in update_columns])
            query = f"""
            INSERT INTO model ({columns})
            VALUES ({placeholders})
            ON CONFLICT(model_name) DO UPDATE SET
            {update_clause}
            """
        else:
            query = f"""
            INSERT INTO model ({columns})
            VALUES ({placeholders})
            ON CONFLICT(model_name) DO NOTHING
            """
        self.conn.execute(query, values)
            
    def add_model_cocoon_tag(self, model_name, cocoon_tag):
        existing = self.conn.execute("""
            SELECT 1 FROM model_cocoon_tag
            WHERE model_name = ? AND cocoon_tag = ?
        """, (model_name, cocoon_tag)).fetchone()
        
        if not existing:
            self.conn.execute("""
                INSERT INTO model_cocoon_tag (model_name, cocoon_tag)
                VALUES (?, ?)
            """, (model_name, cocoon_tag))

    def add_model_lineage(self, source_model_name, target_model_name):
        existing = self.conn.execute("""
            SELECT 1 FROM model_lineage
            WHERE source_model_name = ? AND target_model_name = ?
        """, (source_model_name, target_model_name)).fetchone()
        
        if not existing:
            self.conn.execute("""
                INSERT INTO model_lineage (source_model_name, target_model_name)
                VALUES (?, ?)
            """, (source_model_name, target_model_name))
            
            self.invalidate_lineage_cache()
            self.get_ordered_models()

    def add_or_update_column_lineage(self, column_lineage_data):
        columns = ', '.join(column_lineage_data.keys())
        placeholders = ', '.join(['?' for _ in column_lineage_data])
        values = tuple(column_lineage_data.values())
        
        self.conn.execute(f"""
            INSERT INTO column_lineage ({columns})
            VALUES ({placeholders})
            ON CONFLICT(source_model_name, source_column_name, target_model_name, target_column_name) 
            DO UPDATE SET cocoon_description = COALESCE(EXCLUDED.cocoon_description, column_lineage.cocoon_description)
        """, values)

    def add_or_update_column(self, column_data):
        columns = ', '.join(column_data.keys())
        placeholders = ', '.join(['?' for _ in column_data])
        values = tuple(column_data.values())
        
        self.conn.execute(f"""
            INSERT INTO model_column ({columns})
            VALUES ({placeholders})
            ON CONFLICT(model_name, column_name) DO UPDATE SET
            description = COALESCE(EXCLUDED.description, model_column.description),
            cocoon_description = COALESCE(EXCLUDED.cocoon_description, model_column.cocoon_description)
        """, values)
        
    def populate_from_manifest(self, manifest_dict):
        nodes = manifest_dict.get('nodes', {})
        
        for node_name, node_data in nodes.items():
            if node_data['resource_type'] != 'model':
                continue
            
            model_data = {
                'model_name': node_name, 
                'database': node_data.get('database'),
                'schema': node_data.get('schema'),
                'table_name': node_data.get('name'),
                'original_file_path': node_data.get('original_file_path'),
                'raw_code': node_data.get('raw_code'),
                'compiled_path': node_data.get('compiled_path'),
                'compiled_code': clean_sql(node_data.get('compiled_code')),
                'materialized': node_data.get('config', {}).get('materialized'),
                'description': node_data.get('description'),
                'yml_path': None,
                'cocoon_description': None
            }
            self.add_or_update_model(model_data)
            
            for upstream_node in node_data.get('depends_on', {}).get('nodes', []):
                if upstream_node.startswith('model.'):
                    self.add_model_lineage(upstream_node, node_name)
            
            for column_name, column_data in node_data.get('columns', {}).items():
                column_info = {
                    'model_name': node_name,
                    'column_name': column_name,
                    'description': column_data.get('description'),
                    'cocoon_description': None
                }
                self.add_or_update_column(column_info)
        
        self.compute_lineage_info()
    
    def compute_lineage_info(self):
        table_exists = self.conn.execute("""
            SELECT table_name FROM information_schema.tables 
            WHERE table_name = 'ordered_models' AND table_schema = 'main'
        """).fetchone()

        if table_exists:
            return

        model_lineage = self.conn.execute("SELECT source_model_name, target_model_name FROM model_lineage").fetchall()
        
        graph = {}
        for source, target in model_lineage:
            graph.setdefault(source, set()).add(target)
        
        visited = set()
        temp_mark = set()
        ordered_models = []
        
        def visit(node):
            if node in temp_mark:
                raise Exception(f"Cycle detected in the model lineage involving '{node}'")
            if node not in visited:
                temp_mark.add(node)
                for neighbor in graph.get(node, []):
                    visit(neighbor)
                temp_mark.remove(node)
                visited.add(node)
                ordered_models.append(node)
        
        all_models = [row[0] for row in self.conn.execute("SELECT model_name FROM model").fetchall()]
        for model_name in all_models:
            if model_name not in visited:
                visit(model_name)
        
        self.conn.execute("CREATE OR REPLACE TABLE ordered_models (model_name VARCHAR, new_rowid INTEGER)")
        for idx, model_name in enumerate(reversed(ordered_models), 1):
            self.conn.execute("INSERT INTO ordered_models (model_name, new_rowid) VALUES (?, ?)", (model_name, idx))

    def populate_from_directory(self, directory=None, file_types=None, forced_refresh=False):
        if directory is None:
            directory = self.dbt_directory
        
        if not directory:
            raise ValueError("No directory specified. Please provide a directory or set dbt_directory when initializing DbtLineage.")

        if file_types is None:
            file_types = [".sql"]

        for root, dirs, files in os.walk(directory):
            for file in files:
                if any(file.endswith(ext) for ext in file_types):
                    file_path = os.path.join(root, file)
                    model_name = os.path.splitext(file)[0]
                    
                    with open(file_path, 'r') as sql_file:
                        sql_content = sql_file.read()
                    
                    if forced_refresh:
                        self.conn.execute("""
                            DELETE FROM model_lineage 
                            WHERE target_model_name = ?
                        """, (model_name,))
                        
                        self.conn.execute("""
                            DROP TABLE IF EXISTS ordered_models
                        """)
                    
                    model_data = {
                        'model_name': model_name,
                        'original_file_path': file_path,
                        'raw_code': sql_content,
                        'database': '',
                        'schema': '',
                        'table_name': model_name,
                        'compiled_path': '',
                        'compiled_code': '',
                        'yml_path': '',
                        'materialized': '',
                        'description': '',
                        'cocoon_description': ''
                    }
                    
                    self.add_or_update_model(model_data)
        
    def get_ordered_models(self):
        self.compute_lineage_info()
        return self.conn.execute("""
            SELECT model_name, new_rowid AS rowid
            FROM ordered_models
            ORDER BY new_rowid
        """).fetchall()
    
    def get_model_lineage(self):
        lineage_query = self.conn.execute("""
            SELECT source_model_name, target_model_name
            FROM model_lineage
        """).fetchall()
        return lineage_query

    def get_index_by_model_name(self, model_name):
        self.compute_lineage_info()
        query = self.conn.execute("""
            SELECT new_rowid
            FROM ordered_models
            WHERE model_name = ?
        """, (model_name,)).fetchone()
        
        if query:
            index = query[0]
            return index
        else:
            return None

    def invalidate_lineage_cache(self):
        self.conn.execute("DROP TABLE IF EXISTS model_parent_count")
        self.conn.execute("DROP TABLE IF EXISTS ordered_models")

    def save_to_disk(self, db_name=None):
        if db_name is None:
            db_name = os.path.join(self.dbt_directory, "cocoon_lineage.db")
        self.conn.execute(f"EXPORT DATABASE '{db_name}'")

    def load_from_disk(self, db_name=None):
        if db_name is None:
            db_name = os.path.join(self.dbt_directory, "cocoon_lineage.db")
        
        if not os.path.exists(db_name):
            if cocoon_main_setting['DEBUG_MODE']:
                print(f"Database file not found: {db_name}")
            return False

        self.conn.close()
        
        self.conn = duckdb.connect(':memory:')
        
        self.conn.execute(f"IMPORT DATABASE '{db_name}'")
        
        return True

    def display_database(self):
        query_widget = QueryWidget(self.conn)
        query_widget.display()
        
    def get_recursive_model_lineage(self, model_name, max_forward_depth=None, max_backward_depth=None):
        def get_direct_lineage(input_model, direction='forward'):
            if direction == 'forward':
                query = """
                SELECT 
                    source_model_name, 
                    target_model_name
                FROM model_lineage
                WHERE source_model_name = ?
                """
            else:
                query = """
                SELECT 
                    source_model_name, 
                    target_model_name
                FROM model_lineage
                WHERE target_model_name = ?
                """
            return self.conn.execute(query, [input_model]).fetchall()

        def traverse_lineage(start_model, direction):
            visited = set()
            result = []
            initial_depth = 1 if direction == 'forward' else -1
            queue = deque([(start_model, initial_depth)])

            while queue:
                current_model, depth = queue.popleft()
                
                if current_model in visited:
                    continue
                
                if direction == 'forward' and max_forward_depth is not None and depth > max_forward_depth:
                    continue
                if direction == 'backward' and max_backward_depth is not None and abs(depth) > max_backward_depth:
                    continue
                
                visited.add(current_model)
                
                direct_lineage = get_direct_lineage(current_model, direction)
                
                for row in direct_lineage:
                    result.append(row + (depth,))
                    if direction == 'forward':
                        next_model = row[1]
                        if next_model not in visited:
                            queue.append((next_model, depth + 1))
                    else:
                        prev_model = row[0]
                        if prev_model not in visited:
                            queue.append((prev_model, depth - 1))

            return result

        forward_lineage = traverse_lineage(model_name, 'forward')
        
        backward_lineage = traverse_lineage(model_name, 'backward')
        
        lineage_data = backward_lineage + forward_lineage
        
        df = pd.DataFrame(lineage_data, columns=[
            'source_model_name', 'target_model_name', 'depth'
        ])
        
        return df
    
    def get_full_file_path(self, table_name):
        query = self.conn.execute("""
            SELECT original_file_path
            FROM model
            WHERE table_name = ? OR model_name = ?
        """, (table_name, table_name)).fetchone()

        if query and query[0]:
            return os.path.join(self.dbt_directory, query[0])
        else:
            return None
    
    def generate_html_summary(self, item_ids=None):
        if item_ids is not None:
            return self.get_model_lineage_html_by_indices(model_indices=item_ids)
        return self.display_model_lineage_html()

    def generate_html_summary_st(self, item_ids=None):
        html_summary = self.generate_html_summary(item_ids=item_ids)
        
        import streamlit.components.v1 as components

        components.html(html_summary, height=550, scrolling=True)

    def display_model_lineage_html(self, numbered=True, model_name=None, max_forward_depth=None, max_backward_depth=None):
        all_models_query = self.get_ordered_models()
        global_model_to_idx = {name: idx for name, idx in all_models_query}

        if model_name is None:
            model_lineage = self.get_model_lineage()
            models_to_display = [name for name, _ in all_models_query]
        else:
            lineage_df = self.get_recursive_model_lineage(model_name, max_forward_depth, max_backward_depth)
            models_to_display = sorted(set(lineage_df['source_model_name'].tolist() + lineage_df['target_model_name'].tolist()))

        if not models_to_display:
            return None
        
        local_model_to_idx = {name: idx for idx, name in enumerate(models_to_display)}

        if numbered:
            nodes = [f"{global_model_to_idx[name]}. {name}" for name in models_to_display]
        else:
            nodes = models_to_display

        if model_name is None:
            edges = [(local_model_to_idx[source], local_model_to_idx[target]) for source, target in model_lineage]
        else:
            edges = [(local_model_to_idx[row['source_model_name']], local_model_to_idx[row['target_model_name']]) 
                    for _, row in lineage_df.iterrows()]

        highlight_index = local_model_to_idx[model_name] if (model_name and model_name in local_model_to_idx) else None

        html_content = generate_workflow_html_multiple(
            nodes, 
            edges, 
            format='svg', 
            layout_direction='LR', 
            height=500,
            highlight_nodes_indices=[highlight_index] if highlight_index is not None else None
        )

        return html_content
    
    def display_model_lineage(self, numbered=True, model_name=None, max_forward_depth=None, max_backward_depth=None):
        html_content = self.display_model_lineage_html(numbered, model_name, max_forward_depth, max_backward_depth)

        display(HTML(html_content))
    
    def display_model_lineage_by_indices(self, model_indices=None, numbered=True):
        html_content = self.get_model_lineage_html_by_indices(model_indices, numbered)
        if html_content:
            display(HTML(html_content))
        else:
            print("No models to display.")

    def get_model_lineage_html_by_indices(self, model_indices=None, numbered=True):
        all_models_query = self.get_ordered_models()
        global_idx_to_model = {idx: name for name, idx in all_models_query}
        
        if model_indices is None:
            models_to_display = list(global_idx_to_model.values())
        else:
            models_to_display = [global_idx_to_model[idx] for idx in model_indices if idx in global_idx_to_model]
        
        if not models_to_display:
            return None
        
        local_model_to_idx = {name: idx for idx, name in enumerate(models_to_display)}
        
        model_to_global_idx = {name: idx for idx, name in global_idx_to_model.items()}

        if numbered:
            nodes = [f"{model_to_global_idx[name]}. {name}" for name in models_to_display]
        else:
            nodes = models_to_display
        
        all_model_lineage = self.get_model_lineage()
        
        edges = []
        for source, target in all_model_lineage:
            if source in local_model_to_idx and target in local_model_to_idx:
                edges.append((local_model_to_idx[source], local_model_to_idx[target]))
        
        html_content = generate_workflow_html_multiple(
            nodes, 
            edges, 
            format='svg', 
            layout_direction='LR', 
            height=500
        )
        
        return html_content

    def generate_model_summary_html_dict(self, model_name, numbered=True):
        model_query = self.conn.execute("""
            SELECT * FROM model WHERE model_name = ?
        """, (model_name,))
        
        columns = [column[0] for column in model_query.description]
        model_info = model_query.fetchone()

        if not model_info:
            return {}

        model_info = dict(zip(columns, model_info))

        tags_query = self.conn.execute("""
            SELECT cocoon_tag, cocoon_description
            FROM model_cocoon_tag
            WHERE model_name = ?
        """, (model_name,))
        tags = dict(tags_query.fetchall())


        columns_df = pd.read_sql("""
            SELECT column_name, description, cocoon_description
            FROM model_column
            WHERE model_name = ?
        """, self.conn, params=(model_name,))

        if columns_df['cocoon_description'].isnull().all():
            columns_df = columns_df.drop('cocoon_description', axis=1)

        model_number = ""
        if numbered:
            model_index = self.get_index_by_model_name(model_name)
            model_number = f"{model_index}. "

        properties = {
            "Model Description": ((model_info.get('description') or '') + ' ' + (model_info.get('cocoon_description') or '')).strip() or None,
            "Tags": "<ul>" + "".join(["<li>" + generate_tag_html(tag, description) + "</li>" for tag, description in tags.items()]) + "</ul>" if tags else None,
            "SQL": highlight_sql_only(model_info.get('raw_code')) if model_info.get('raw_code') else None,
            "Column Description": columns_df.to_html(index=False) if not columns_df.empty else None,
        }

        properties = {k: v for k, v in properties.items() if v is not None}

        return properties
    
    def create_data_selectbox(self, model_ids=None):
        all_models = self.get_ordered_models()
        
        if model_ids is not None:
            filtered_models = [(model_name, model_id) for model_name, model_id in all_models if model_id in model_ids]
        else:
            filtered_models = all_models
        
        options = ["Model Lineage"] + [f"{model_id}. {model_name}" for model_name, model_id in filtered_models]
        
        if model_ids:
            selectbox_key = f"model_select_{'_'.join(map(str, sorted(model_ids)))}"
        else:
            selectbox_key = "model_select_all"
        
        selected_option = st.selectbox("Select a model or view lineage", options=options, key=selectbox_key)
        
        if selected_option == "Model Lineage":
            if model_ids is not None:
                lineage_html = self.get_model_lineage_html_by_indices(model_indices=model_ids)
                st.components.v1.html(lineage_html, height=300, scrolling=True)
            else:
                self.generate_html_summary_st()
        else:
            item_id = int(selected_option.split('.')[0])
            self.display_item_summary_st(item_id)

    def display_item_summary_st(self, item_id, numbered=True):
        model_name_query = self.conn.execute("""
            SELECT model_name
            FROM ordered_models
            WHERE new_rowid = ?
        """, (item_id,)).fetchone()
        
        if model_name_query is None:
            st.error(f"No model found for item ID: {item_id}")
            return
        
        model_name = model_name_query[0]
        
        model_info = self.conn.execute("""
            SELECT description, cocoon_description, raw_code
            FROM model
            WHERE model_name = ?
        """, (model_name,)).fetchone()
        
        if model_info is None:
            st.error(f"No information found for model: {model_name}")
            return
        
        description, cocoon_description, raw_sql = model_info
        
        
        full_description = ((description or '') + ' ' + (cocoon_description or '')).strip()
        if full_description:
            st.markdown("*Pipline model description*")
            st.write(full_description)
            
        st.markdown("*SQL codes for the pipeline model*")
        st.code(raw_sql, language="sql")
        
        columns_df = pd.read_sql("""
            SELECT column_name, description, cocoon_description
            FROM model_column
            WHERE model_name = ?
            ORDER BY column_name
        """, self.conn, params=(model_name,))
        
        if not columns_df.empty:
            st.markdown("*Column description for the pipeline model*")

            columns_df['full_description'] = columns_df['description'].fillna('') + ' ' + columns_df['cocoon_description'].fillna('')
            columns_df['full_description'] = columns_df['full_description'].apply(lambda x: x.strip() if x.strip() else "No description available.")
            
            display_df = columns_df[['column_name', 'full_description']].rename(columns={
                'column_name': 'Column Name',
                'full_description': 'Description'
            })
            
            st.dataframe(display_df, use_container_width=True, hide_index=True)

    def generate_item_text_summary(self, item_id):
        model_summary_dict = self.generate_model_summary_text_dict(model_id=item_id)
        model_summary_text = "\n".join([f"{key}\n{model_summary_dict[key]}" for key in model_summary_dict])
        return model_summary_text

    def get_item_name_by_id(self, item_id):
        model_name_query = self.conn.execute("""
            SELECT model_name
            FROM ordered_models
            WHERE new_rowid = ?
        """, (item_id,)).fetchone()
        
        if model_name_query is None:
            return ""
        
        model_name = model_name_query[0]
        return model_name
    
    def generate_model_summary_text_dict(self, model_name=None, model_id=None):
        if model_name is None and model_id is None:
            raise ValueError("Either model_name or model_id must be provided")

        if model_id is not None:
            model_name_query = self.conn.execute("""
                SELECT model_name
                FROM ordered_models
                WHERE new_rowid = ?
            """, (model_id,)).fetchone()
            
            if model_name_query is None:
                return {}
            
            model_name = model_name_query[0]
            
        model_query = self.conn.execute("""
            SELECT * FROM model WHERE model_name = ?
        """, (model_name,))
        
        columns = [column[0] for column in model_query.description]
        model_info = model_query.fetchone()

        if not model_info:
            return {}

        model_info = dict(zip(columns, model_info))

        columns_df = pd.read_sql("""
            SELECT column_name, description
            FROM model_column
            WHERE model_name = ?
        """, self.conn, params=(model_name,))

        properties = {
            "Description": model_info.get('description') or None,
            "Columns": "\n".join([f"{i+1}. '{row['column_name']}': {row['description'] or 'No description'}" 
                                for i, row in columns_df.iterrows()]) if not columns_df.empty else None,
            "Raw SQL": f"```sql\n{model_info.get('raw_code')}\n```" if model_info.get('raw_code') else None
        }

        properties = {k: v for k, v in properties.items() if v is not None}

        return properties
    
    def generate_multiple_model_summaries_dict(self, model_indices, full=False):
        summaries = OrderedDict()

        if model_indices is None:
            model_indices_query = self.conn.execute("""
                SELECT new_rowid
                FROM ordered_models
                ORDER BY new_rowid
            """).fetchall()
            model_indices = [row[0] for row in model_indices_query]
    

        for idx in model_indices:
            model_query = self.conn.execute("""
                SELECT model_name
                FROM ordered_models
                WHERE new_rowid = ?
            """, [idx]).fetchone()
            
            if model_query is None:
                continue
            
            model_name = model_query[0]
            key = f"{idx}. {model_name}"
            
            html_content = self.generate_model_summary_html(model_name, full=full)
            
            summaries[key] = html_content
        
        return summaries

    def generate_items_summary(self, item_ids=None, full=True):
        if item_ids is None:
            all_indices_query = self.conn.execute("""
                SELECT new_rowid
                FROM ordered_models
                ORDER BY new_rowid
            """).fetchall()
            item_ids = [row[0] for row in all_indices_query]
            return self.generate_multiple_model_summaries(model_indices=item_ids, include_model_lineage=True, full=full)
        else:
            return self.generate_multiple_model_summaries(model_indices=item_ids, full=full)

    def generate_multiple_model_summaries(self, model_indices, full=True, include_model_lineage=False):
        if model_indices is None:
            section_name = "model_summary_all"
        else:
            section_name = f"model_summary_{'_'.join(map(str, model_indices))}"
        
        options = self.generate_multiple_model_summaries_dict(model_indices, full=False)

        if include_model_lineage:
            model_lineage_html = self.display_model_lineage_html(numbered=True)
            
            options = OrderedDict([("Model Lineage", model_lineage_html)] + list(options.items()))
        
        selected = next(iter(options.keys()), None)
        
        combined_html = generate_dropdown_html(section_name, options, default_selection="Explore Model", full=False, selected=selected)

        if not full:
            return combined_html
        
        formatter = HtmlFormatter(style='monokai')
        css_style = formatter.get_style_defs('.highlight')
        html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <style>
        body {{
            font-size: 0.75rem;
        }}
        pre {{ line-height: 125%; }}
        .highlight {{
            background: #272822;
            color: #f8f8f2;
            border-radius: 4px;
            padding: 1em;
            margin: 1em 0;
            overflow-x: auto;
            font-size: 12px;
        }}
        {css_style}
        {tag_css}
        {pandas_css}
    </style>
</head>
<body>
    {combined_html}
</body>
</html>
"""
        return html
    
    def generate_model_summary_html(self, model_name, full=True):
        options = self.generate_model_summary_html_dict(model_name)     

        combined_html = ""
        for section_name, section_content in options.items():
            if section_content:
                combined_html += wrap_in_card(section_name, section_content)

        model_number = ""
        model_index = self.get_index_by_model_name(model_name)
        model_number = f"{model_index}. "

        combined_html = f'<b>{model_number}{model_name}</b><br><small class="text-muted mb-3">Model catalog prepared by Cocoon</small><br><br>' + combined_html
            
        if not full:
            return combined_html
        
        formatter = HtmlFormatter(style='monokai')
        css_style = formatter.get_style_defs('.highlight')
        html = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <style>
        body {{
            font-size: 0.75rem;
        }}
        pre {{ line-height: 125%; }}
        .highlight {{
            background: #272822;
            color: #f8f8f2;
            border-radius: 4px;
            padding: 1em;
            margin: 1em 0;
            overflow-x: auto;
            font-size: 12px;
        }}
        {css_style}
        {tag_css}
        {pandas_css}
    </style>
</head>
<body>
    {combined_html}
</body>
</html>
"""
        return html
    
    def display_model_summary(self, model_name):
        html_content = self.generate_model_summary_html(model_name)
        display(HTML(wrap_in_iframe(html_content)))
        
    def create_lineage_widgets(self, width="100%", height=400):
        all_models = self.get_ordered_models()
        
        options = ["Model Lineage"] + [f"{model_id}. {model_name}" for model_name, model_id in all_models]
        
        dropdown = widgets.Dropdown(
            options=options,
            value="Model Lineage",
            disabled=False,
        )
        
        html_widget = widgets.HTML(
            value=self.display_model_lineage_html(),
            placeholder='',
            description='',
        )
        
        def update_html(change):
            if change['new'] == "Model Lineage":
                html_widget.value = self.display_model_lineage_html()
            else:
                model_name = change['new'].split('. ', 1)[1]
                html_widget.value = wrap_in_iframe(self.generate_model_summary_html(model_name=model_name), width=width, height=height)
        
        dropdown.observe(update_html, names='value')
        
        return dropdown, html_widget

    def display_lineage_widgets(self, width="100%", height=500):
        dropdown, html_widget = self.create_lineage_widgets(width=width, height=height)
        display(dropdown, html_widget)
        
    def generate_text_summary(self, idx=None):
        name = self.get_name()
        intro = f"# This is a data pipeline and how one model references another\n"
        intro += "# Below are models (higher index are downstream):\n"
        return intro + self.generate_text_lineage()

    def generate_text_lineage(self, model_name=None, max_forward_depth=None, max_backward_depth=None):
        all_models_query = self.get_ordered_models()
        global_model_to_idx = {name: idx for name, idx in all_models_query}

        if model_name is None:
            model_lineage = self.get_model_lineage()
            models_to_display = [name for name, _ in all_models_query]
            
            dependencies = {model: set() for model in models_to_display}
            for source, target in model_lineage:
                dependencies[target].add(source)
        else:
            lineage_df = self.get_recursive_model_lineage(model_name, max_forward_depth, max_backward_depth)
            models_to_display = set(lineage_df['source_model_name'].tolist() + lineage_df['target_model_name'].tolist())
            
            dependencies = {model: set() for model in models_to_display}
            for _, row in lineage_df.iterrows():
                dependencies[row['target_model_name']].add(row['source_model_name'])

        models_to_display = sorted(models_to_display, key=lambda x: global_model_to_idx[x])

        text_lineage = []
        for model in models_to_display:
            model_id = global_model_to_idx[model]
            deps = dependencies[model]
            dep_ids = sorted([global_model_to_idx[dep] for dep in deps if dep in global_model_to_idx])
            dep_str = f" <- {', '.join(str(id) for id in dep_ids)}" if dep_ids else ""
            
            text_lineage.append(f"{model_id}. '{model}'{dep_str}")

        return "\n".join(text_lineage)

    def get_lowest_categories(self):
        return []


    def get_story_list(self, exclude_categories=None):
        return []
    
    def add_model(self, model_name):
        model_query = self.conn.execute("""
            SELECT description, cocoon_description
            FROM model
            WHERE model_name = ?
        """, (model_name,)).fetchone()

        if model_query is None:
            if cocoon_main_setting['DEBUG_MODE']:
                print(f"Model '{model_name}' not found in the database.")
            return

        description, cocoon_description = model_query

        table_summary = description or ""
        if cocoon_description:
            table_summary += " " + cocoon_description if table_summary else cocoon_description

        columns_query = self.conn.execute("""
            SELECT column_name, description, cocoon_description
            FROM model_column
            WHERE model_name = ?
            ORDER BY column_name
        """, (model_name,))

        columns = []
        column_desc = OrderedDict()

        for column_name, col_description, col_cocoon_description in columns_query.fetchall():
            columns.append(column_name)
            
            full_description = col_description or ""
            if col_cocoon_description:
                full_description += " " + col_cocoon_description if full_description else col_cocoon_description
            
            column_desc[column_name] = full_description

        table_object = Table(
            table_name=model_name,
            table_summary=table_summary,
            columns=columns,
            column_desc=column_desc
        )

        self.tables[model_name] = columns

        self.table_object[model_name] = table_object

        if cocoon_main_setting['DEBUG_MODE']:
            print(f"Added model '{model_name}' with columns: {columns}")
            print(f"Table summary: {table_summary}")
            print(f"Column descriptions: {column_desc}")

    def get_key_columns(self, table):
        keys_to_exclude = set()
        return keys_to_exclude

        

        
        
        

        
        
        
        


    
        

    

        






    

        





        
        
        
        
        
        

        




        
        




        
        
    
        




                
                


        
    
        
            
            
        

        
        
        
            
            
        
        

        


        
                
                
    
        
        
        
        



                
                
                    
                    
                    
                    
                

        
        
        
                
        





                
                
                
                


        
        
        
        
        
    
            




                    


    
        




        

        
        
        
        
    

        
        
            
            

            



        
            
    
            
            
            
        
    
        
        
                

    

        


            


    




            

        

        
class ConnectSourceDataNode(Node):
    default_name = 'Connect Source Data'
    default_description = 'This step allows you to connect to the source data'


    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        configure_nodes = self.get_sibling_node('DBT Project Config and Read Multiple').nodes
        
        source_node = configure_nodes["source"]
        target_node = configure_nodes["target"]
        
        self.para["source_data_project"] = source_node.para["data_project"]
        self.para["target_data_project"] = target_node.para["data_project"]
        
        callback({})
        
class SourceTableDisplayNode(Node):
    default_name = 'Source Table Display'
    default_description = 'This step displays the source table information and a sample in the chat UI.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        source_table_object = self.para.get("source_table_object")
        source_table_pipeline = self.para.get("source_table_pipeline")
        con = self.item["con"]
         
        sample_size = 5
        sample_df = source_table_pipeline.get_samples(con, sample_size=sample_size)

        message = f"Help me transform the table '{source_table_object.table_name}': \n"
        message += f"<i>{source_table_object.table_summary}</i>\n\n"
        styled_table_html = f"""
        <div style="max-width: 100%; overflow-x: auto;">
            {sample_df.to_html()}
        </div>
        """
        message += styled_table_html

        if "chatui" in self.para:
            self.para["chatui"].add_message("You", message)

        callback({})




def is_test_node(node_id, node_info):
    return node_info.get('resource_type') == 'test'

def clean_sql(sql):
    try:
        return sqlglot.transpile(sql, pretty=True)[0]
    except Exception as e:
        return sql
    
def build_lineage_graph(dbt_path, manifest_path=None, catalog_path=None):
    if manifest_path is None:
        manifest_path = os.path.join(dbt_path, 'target', 'manifest.json')
    if catalog_path is None:
        catalog_path = os.path.join(dbt_path, 'target', 'catalog.json')
    
    if not file_exists(manifest_path):
        raise FileNotFoundError(f"Manifest file not found at {manifest_path}. Please specify the path to the manifest file using the 'manifest_path' argument.")
        
    with open(manifest_path, 'r') as f:
        manifest = json.load(f)
     
    if not file_exists(catalog_path):
        print(f"Catalog file not found at {catalog_path}. Proceeding without catalog data.")
        catalog = None
    else:
        with open(catalog_path, 'r') as f:
            catalog = json.load(f)
    
    nodes = set()
    dependencies = defaultdict(set)
    sql_content_mapping = {}
    column_mapping = {}
    
    def clean_manifest_column(column_info):
        return {
            "name": column_info.get("name"),
            "comment": column_info.get("description"),
            "type": column_info.get("data_type"),
            "index": column_info.get("index")
        }
    
    for node_id, node_info in manifest['nodes'].items():
        if not is_test_node(node_id, node_info):
            nodes.add(node_id)
            
            compiled_path = node_info.get('compiled_path')
            if compiled_path:
                full_sql_path = os.path.join(dbt_path, compiled_path)
                if file_exists(full_sql_path):
                    try:
                        with open(full_sql_path, 'r') as sql_file:
                            sql_content = sql_file.read()
                        sql_content_mapping[node_id] = clean_sql(sql_content)
                    except Exception as e:
                        print(f"Error reading SQL file for {node_id}: {str(e)}")
                else:
                    print(f"Compiled SQL file not found at {full_sql_path}")
            
            manifest_columns = {col_name: clean_manifest_column(col_info) 
                                for col_name, col_info in node_info.get('columns', {}).items()}
            
            catalog_columns = {}
            if catalog and node_id in catalog.get('nodes', {}):
                catalog_columns = catalog['nodes'][node_id].get('columns', {})
            
            merged_columns = {}
            
            for col_name, col_info in manifest_columns.items():
                merged_col = col_info.copy()
                if col_name in catalog_columns:
                    if not merged_col['comment'] and catalog_columns[col_name].get('comment'):
                        merged_col['comment'] = catalog_columns[col_name]['comment']
                    if not merged_col['type'] and catalog_columns[col_name].get('type'):
                        merged_col['type'] = catalog_columns[col_name]['type']
                merged_columns[col_name] = merged_col
            
            for col_name, col_info in catalog_columns.items():
                if col_name not in merged_columns:
                    merged_columns[col_name] = {
                        "name": col_name,
                        "comment": col_info.get('comment'),
                        "type": col_info.get('type'),
                        "index": None
                    }
            
            column_mapping[node_id] = merged_columns
            
            if 'depends_on' in node_info and 'nodes' in node_info['depends_on']:
                for dep in node_info['depends_on']['nodes']:
                    dep_info = manifest['nodes'].get(dep, {})
                    if not is_test_node(dep, dep_info):
                        dependencies[node_id].add(dep)
                        nodes.add(dep)
    
    nodes_list = sorted(list(nodes))
    node_to_index = {node: index for index, node in enumerate(nodes_list)}
    
    edges = []
    for node, deps in dependencies.items():
        for dep in deps:
            edges.append((node_to_index[dep], node_to_index[node]))
    
    return nodes_list, edges, sql_content_mapping, column_mapping

class SelectStageOptions(Node):
    default_name = 'Select Stage Options'
    default_description = 'This step allows you to select the stage options.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        style = HTML("""
        <style>
        .widget-checkbox { margin-right: 10px; }
        .option-label { 
            font-size: 14px; 
        }
        em { 
            font-size: 12px; 
            color: #666; 
            display: block;
            margin-top: -5px;
        }
        code {
            font-family: 'Fira Code', 'Consolas', 'Monaco', 'Andale Mono', 'Ubuntu Mono', monospace;
            font-size: 14px;
            line-height: 1.4;
        }
        pre.command-line {
            background-color: #f0f0f0;
            border-radius: 5px;
            padding: 10px;
            overflow-x: auto;
            white-space: pre-wrap;
            word-wrap: break-word;
            color: #333;
            border: 1px solid #e1e4e8;
        }
        pre.command-line code {
            background-color: transparent;
            padding: 0;
            border: none;
        }
        </style>
        """)
        display(style)

        def create_html_checkboxes(options):
            labels_html = [widgets.HTML(value=option[0]) for option in options]
            checkbox_style = {'description_width': '0px'}
            checkboxes = [widgets.Checkbox(value=option[2], description='', disabled=not option[3], style=checkbox_style, layout={'width': 'initial'}) for option in options]
            checkboxes_widget = widgets.VBox([widgets.HBox([cb, label], layout=widgets.Layout(align_items='center', margin='0')) for cb, label in zip(checkboxes, labels_html)])
            return checkboxes_widget, checkboxes

        mandatory_options = [
            ['<span class="option-label"><div><strong>Table + Column Description</strong></div><div><em>Cocoon describes the table and each column</em></div></span>', 'table_desc', True, False],
        ]

        mandatory_checkboxes_widget, mandatory_checkboxes = create_html_checkboxes(mandatory_options)

        access_control = widgets.RadioButtons(
            options=['Read Data Access', 'Schema Only Access'],
            description='',
            disabled=False,
            layout=widgets.Layout(margin='0 0 0 35px')
        )

        other_options = [
            ['<span class="option-label"><div><strong>Explain Missing Values</strong></div><div><em><b>📖 Read Access:</b> Cocoon explains the percentage and potential reasons for missing values</em></div></span>', 'explain_missing_values', True, True],
            ['<span class="option-label"><div><strong>Detect Regex Patterns</strong></div><div><em><b>📖 Read Access:</b> Cocoon detects regular patterns for string</em></div></span>', 'detect_regex_patterns', True, True],
            ['<span class="option-label"><div><strong>Parse Variant Types (Snowflake-only)</strong></div><div><em><b>📖 Read Access:</b> Cocoon identifies the data type for Variant type</em></div></span>', 'parse_variant_types', True, True],
            ['<span class="option-label"><div><strong>Unique test</strong></div><div><em><b>📖 Read Access:</b> Cocoon writes tests for columns shall be unique</em></div></span>', 'unique_test', True, True],
            ['<span class="option-label"><div><strong>Accepted Values test</strong></div><div><em><b>📖 Read Access:</b> Cocoon writes tests for columns shall have fixed accepted values</em></div></span>', 'category_test', True, True],
            ['<span class="option-label"><div><strong>Cross column tests</strong></div><div><em><b>📖 Read Access:</b> Cocoon performs tests that involve multiple columns</em></div></span>', 'expression_tests', False, True],
            ['<span class="option-label"><div><strong>Projection</strong></div><div><em><b>✍️ Write Access:</b> Cocoon projects out some columns like fivetran sync time, or index</em></div></span>', 'projection', True, True],
            ['<span class="option-label"><div><strong>Column rename</strong></div><div><em><b>✍️ Write Access:</b> Cocoon renames columns to be descriptive</em></div></span>', 'rename', True, True],
            ['<span class="option-label"><div><strong>Deduplicate</strong></div><div><em><b>✍️ Write Access:</b> Cocoon removes duplicated rows</em></div></span>', 'deduplication', True, True],
            ['<span class="option-label"><div><strong>Standardize values</strong></div><div><em><b>✍️ Write Access:</b> Cocoon standardizes inconsistent values and fix typos</em></div></span>', 'standardize_values', True, True],
            ['<span class="option-label"><div><strong>Detect disguised missing values</strong></div><div><em><b>✍️ Write Access:</b> Cocoon detects and fixes missing values not already as NULL</em></div></span>', 'detect_disguised_missing_values', True, True],
            ['<span class="option-label"><div><strong>Transform data type</strong></div><div><em><b>✍️ Write Access:</b> Cocoon casts column with wrong data type</em></div></span>', 'transform_data_type', True, True],
            ['<span class="option-label"><div><strong>Trim whitespaces</strong></div><div><em><b>✍️ Write Access:</b> Cocoon trims leading and trailing whitespaces</em></div></span>', 'trim', True, True],
            ['<span class="option-label"><div><strong>Generate HTML report</strong></div><div><em>Cocoon generates HTML report to summarize the results</em></div></span>', 'generate_html_report', False, True]
        ]

        other_checkboxes_widget, other_checkboxes = create_html_checkboxes(other_options)

        cardinality_threshold = widgets.IntText(
            value=300,
            description='',
            disabled=False,
            style={'description_width': 'auto'},
        )

        cardinality_description = widgets.HTML(
            value='<span class="option-label"><div><strong>Standardize Cardinality Threshold</strong></div><div><em>(If standardize: Cocoon inspects each value for columns whose cardinality is below threshold)</em></div></span>'
        )

        cardinality_widget = widgets.VBox([
            cardinality_threshold,
            cardinality_description
        ], layout=widgets.Layout(margin='0 0 0 45px'))


        pii_description = widgets.HTML(
            value='<span class="option-label"><div><strong>PII Detection</strong></div><div><em>Cocoon detects columns with Personal Identifiable Information</em></div></span>'
        )

        checkbox_style = {'description_width': '0px'}

        remove_pii = widgets.Checkbox(
            value=False,
            description='',
            disabled=False,
            indent=False,
            style=checkbox_style,
            layout={'width': 'initial'}
        )

        pii_widget = widgets.HBox([remove_pii, pii_description], layout=widgets.Layout(align_items='center', margin='0'))

        pii_method_description = widgets.HTML(
            value='<span class="option-label"><strong>PII Access Control:</strong><div><em>Cocoon can only send schema to LLMs for detection</em></div></span>'
        )

        pii_method = widgets.RadioButtons(
            options=['Schema Only', 'Read Data'],
            description='',
            disabled=True
        )

        pii_model_description = widgets.HTML(
            value='<span class="option-label"><strong>PII Detection Model:</strong><div><em>Cocoon can use the specified (e.g., open-sourced) models for PII</em></div></span>'
        )

        pii_model = widgets.Dropdown(
            options=['', 'Llama3Vertex', 'Llama3Bedrock'],
            value=None,
            description='',
            disabled=True
        )

        def update_enabled(change):
            pii_method.disabled = not change['new']
            pii_model.disabled = not change['new']

        remove_pii.observe(update_enabled, names='value')

        pii_container = widgets.VBox([
            pii_widget,
            widgets.VBox([
                widgets.VBox([pii_method_description, pii_method]),
                widgets.VBox([pii_model_description, pii_model])
            ], layout=widgets.Layout(margin='0 0 0 35px'))
        ])

        
        submit_button = widgets.Button(
            description='Submit',
            icon='check',
            button_style='success'
        )

        export_button = widgets.Button(
            description='Export',
            icon='download',
            button_style='info'
        )

        output = widgets.Output()
        
        if hasattr(self, 'para') and 'cocoon_stage_options' in self.para:
            stored_options = self.para['cocoon_stage_options']
            for option, checkbox in zip(options, checkboxes):
                if option[1] in stored_options:
                    checkbox.value = stored_options[option[1]]
            
            if 'cardinality_threshold' in stored_options:
                cardinality_threshold.value = stored_options['cardinality_threshold']
                
            if 'pii' in stored_options:
                remove_pii.value = stored_options['pii']
            
            pii_method.disabled = not remove_pii.value
            pii_model.disabled = not remove_pii.value
            
            if remove_pii.value:
                if 'pii_schema_only' in stored_options and stored_options['pii_schema_only'] is not None:
                    pii_method.value = 'Schema Only' if stored_options['pii_schema_only'] else 'Read Data'
                
                if 'pii_model' in stored_options and stored_options['pii_model'] is not None:
                    pii_model.value = stored_options['pii_model']

        def on_submit(b):
            with self.output_context():
                selected_options = get_selected_options()
                self.para["cocoon_stage_options"] = selected_options
                callback(selected_options)

        def on_export(b):
            with output:
                output.clear_output()
                selected_options = get_selected_options()
                code_snippet = f"""para = {{"cocoon_stage_options": {{{', '.join(f"{repr(k)}: {repr(v)}" for k, v in selected_options.items())}}}}}
create_cocoon_workflow(con=con, output=widgets.Output(), para=para)"""
                display(HTML(f'To reuse the options in the future run:<pre class="command-line"><code>{code_snippet}</code></pre>'))
                
        def get_selected_options():
            selected_options = {
                option[1]: checkbox.value 
                for options, checkboxes in [(mandatory_options, mandatory_checkboxes), (other_options, other_checkboxes)]
                for option, checkbox in zip(options, checkboxes)
            }
            selected_options['description_access_control'] = access_control.value
            
            if selected_options['standardize_values']:
                selected_options['cardinality_threshold'] = cardinality_threshold.value
            
            selected_options['pii'] = remove_pii.value
            
            selected_options['pii_schema_only'] = pii_method.value == 'Schema Only' 
            
            selected_options['pii_model'] = pii_model.value
            
            return selected_options

        submit_button.on_click(on_submit)
        export_button.on_click(on_export)

        widget = widgets.VBox([
            widgets.HTML(value="<div style='line-height: 1.2;'><h2>⚙️ Data Workflow Options</h2><em>Customizing the workflow for all tables in Express Mode. If unsure, leave it as default.</em></div>"),
            widgets.VBox([
                mandatory_checkboxes_widget,
                access_control,
                other_checkboxes_widget, 
                cardinality_widget, 
                pii_container
            ], layout=widgets.Layout(border='1px solid #ddd', padding='10px')),
            widgets.HBox([export_button, submit_button]),
            output
        ])
        
        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_submit(submit_button)
            return

        display(style)
        create_progress_bar_with_numbers(1, model_steps)
        display(widget)
        
        
class ExplainLineage(ListNode):
    default_name = 'Explain Lineage'
    default_description = 'This node explains the lineage of a specific column in a dbt model.'

    def extract(self, item):
        

        dbt_lineage = self.para['dbt_lineage']
        model_name = self.para['model_name']
        column_name = self.para['column_name']
        
        source_lineage, usage_lineage = dbt_lineage.get_column_lineage_raw_dicts(model_name, column_name)

        outputs = [
            [source_lineage, "source"],
            [usage_lineage, "usage"]
        ]
        
        return outputs
    
    def run(self, extract_output, use_cache=True):
        lineage, lineage_type = extract_output
        
        if not lineage:
            return [lineage_type, ""]
        
        lineage_yaml = yaml.dump(lineage, default_flow_style=False)
        
        if lineage_type == "source":
            instruction = "Summarize where this column comes from, including any transformations applied to it."
            example_summary = "This column comes from 'model a'['column a']. It's (1) first cast to TIME in the 'model b', (2) then featurized in 'model c'..."
        else:
            instruction = "Summarize how this column is used in other models or transformations."
            example_summary = "This column is used in many ways. (1) It is joined with 'column1' in models a, b, c. The result 'column2' is then casted in model d ... (2) In model e, it's aggregated to compute..."

        template = f"""You are given the following {lineage_type} lineage for a column:

{lineage_yaml}

{instruction}
Provide a single, concise summary that includes specific evidence such as column names and model names. 
Your response should be in the following format:

```yml
summary: >-
    {example_summary}
```"""

        messages = [{"role": "user", "content": template}]
        
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        result = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        return [lineage_type, result['summary']]

    def run_but_fail(self, extract_output, use_cache=True):
        lineage, lineage_type = extract_output
        return [lineage_type, ""]

    def merge_run_output(self, run_outputs):
        
        merged_output = {}
        for lineage_type, summary in run_outputs:
            merged_output[lineage_type] = summary
        return merged_output
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(run_output)
        
        
def create_cocoon_dbt_lineage_explanation_workflow(output=None, para=None, viewer=False):
    if para is None:
        para = {}

    main_workflow = Workflow("DBT Lineage Explanation Workflow", 
                        item = {},
                        description="A workflow to explain DBT lineage",
                        output=output,
                        para=para)

    main_workflow.add_to_leaf(ExplainLineage(output=output))

    return main_workflow

def read_dbt_models_from_yml(yml_path):
    try:
        with open(yml_path, 'r') as file:
            yml_content = yaml.safe_load(file)
        
        if not isinstance(yml_content, dict):
            return []
        
        models = yml_content.get('models', [])
        
        if isinstance(models, list):
            return [model['name'] for model in models if isinstance(model, dict) and 'name' in model]
        else:
            return []
    
    except FileNotFoundError:
        return []
    except yaml.YAMLError as e:
        return []
    except Exception as e:
        return []
    
read_dbt_models_from_yml("dbt_amazon_ads/compiled_amazon_ads.yml")

def explore_dbt_models(directory, nodes, print_model=False):
    directory_path = Path(directory)
    nodes_directory_mapping = {}
    for node in nodes:
        nodes_directory_mapping[node] = None
    
    for yml_file in directory_path.glob('**/*.yml'):
        if yml_file.is_file():
            if print_model:            
                print(f"YAML Path: {yml_file}")
                
            model_list = read_dbt_models_from_yml(yml_file)
            
            if print_model:
                print(f"List of models: {model_list}")
                print()
            
            for node in nodes:
                node_name = node.split('. ', 1)[1]
                node_parts = node_name.split('.')
                if node_parts[-1] in model_list:
                    nodes_directory_mapping[node] = yml_file
                    
    return nodes_directory_mapping



def append_column_descriptions(yml_path, model_name, full_model_name, dbt_lineage):
    dbt_yaml = YAML()
    dbt_yaml.width = 4096
    dbt_yaml.preserve_quotes = True
    dbt_yaml.indent(mapping=2, sequence=4, offset=2)

    with open(yml_path, 'r') as file:
        data = dbt_yaml.load(file)

    for model in data.get('models', []):
        if model.get('name') == model_name:
            for column in model.get('columns', []):
                if 'description' in column:
                    col_name = column.get('name', '')
                    pattern = re.escape(cocoon_lineage_start) + r'.*?' + re.escape(cocoon_lineage_end)
                    column['description'] = re.sub(pattern, '', column['description'], flags=re.DOTALL)
                    
                    cocoon_lineage_summary = dbt_lineage.get_column_lineage_summary(full_model_name, col_name)
                    
                    if not cocoon_lineage_summary['source'] and not cocoon_lineage_summary['usage']:
                        continue
                    
                    column['description'] += cocoon_lineage_start + "\n"
                    
                    if cocoon_lineage_summary['source']:
                        column['description'] += f"Upstream Lineage: {cocoon_lineage_summary['source']}\n"
                        
                    if cocoon_lineage_summary['usage']:
                        column['description'] += f"Downstream Lineage: {cocoon_lineage_summary['usage']}\n"
                    
                    column['description'] += cocoon_lineage_end
                    
                    print(f"Updated description for '{col_name}'")

    with open(yml_path, 'w') as file:
        dbt_yaml.dump(data, file)
        


class IdentifySchemaMatching(Node):
    default_name = 'Identify Schema Matching'
    default_description = 'Schema Matching for table transformation'

    def extract(self, input_item):
        clear_output(wait=True)
        
        print("🔍 Identifying the schema matching...")
        self.progress = show_progress(1)
        
        target_table_name = self.para.get('target_table')
        source_table_name = self.para.get('source_table_name')
        source_data_project = self.para.get('source_data_project')
        target_data_project = self.para.get('target_data_project')
        
        source_table = source_data_project.table_object[source_table_name]
        target_table = target_data_project.table_object[target_table_name]
        
        source_key_columns = source_data_project.get_key_columns(source_table_name)
        target_key_columns = target_data_project.get_key_columns(target_table_name)
        
        source_columns = [col for col in source_table.columns if col not in source_key_columns]
        target_columns = [col for col in target_table.columns if col not in target_key_columns]
        
        source_column_dict = source_table.get_column_desc_yml(columns=source_columns, show_category=True, show_pattern=True)
        target_column_dict = target_table.get_column_desc_yml(columns=target_columns, show_category=True, show_pattern=True)
        
        source_yml_desc = yaml.dump(source_column_dict, default_flow_style=False)
        target_yml_desc = yaml.dump(target_column_dict, default_flow_style=False)
        
        return source_yml_desc, target_yml_desc, source_columns, target_columns, source_table_name, target_table_name
    
    def run(self, extract_output, use_cache=True):
        source_yml_desc, target_yml_desc, source_columns, target_columns, source_table, target_table = extract_output
        
        template = f"""You have a source table '{source_table}'
{source_yml_desc}

And target table '{target_table}'
{target_yml_desc}

Identify the mapping from source columns to target columns, based on the semantic meaning.
Also indicate if you are confident about the semantic equlity of mapping (True) or not (False).
Also describe the rough SQL (direct copy, value mapping, aggregation, feature extraction, etc.)

Respond in the following json:
```yml
summary: >-
    X columns can be transformed...
column_matching: # for all columns can be transformed
    target_column_name_1:
        source_columns: [source_column_name_1, source_column_name_2, ...]
        transformation: "direct copy"
        confidence: true/false
    target_column_name_2:
        ...
```
"""
        messages = [{"role": "user", "content": template}]

        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        result = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(result, dict):
            raise ValueError("The response is not a valid yml format.")

        if "summary" not in result:
            raise ValueError("The response should have a 'summary' field.")

        if "column_matching" not in result:
            raise ValueError("The response should have a 'column_matching' field.")
        
        if result['column_matching']:
            valid_target_columns = {}
            for target_col, mapping in result['column_matching'].items():
                if target_col in target_columns:
                    source_cols = mapping['source_columns']
                    valid_source_cols = [col for col in source_cols if col in source_columns]
                    if valid_source_cols:
                        mapping['source_columns'] = valid_source_cols
                        valid_target_columns[target_col] = mapping
            result['column_matching'] = valid_target_columns
            
        return result
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"summary": "No columns can be transformed", "column_matching": {}}

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        source_yml_desc, target_yml_desc, source_columns, target_columns, source_table, target_table = extract_output
        result = run_output
        
        callback(run_output)
        return

        nodes = {
            "(source) " + source_table: source_columns,
            "(target) " + target_table: target_columns
        }

        edges = []
        highlighted_edge_indices = []
        
        for target_col, matching in result['column_matching'].items():
            for source_col in matching['source_columns']:
                edge = ("(source) " +  source_table, source_col, "(target) " +  target_table, target_col)
                edges.append(edge)
                
                if matching['confidence']:
                    highlighted_edge_indices.append(len(edges) - 1)
                    
        html_output = generate_schema_graph_graphviz(nodes, edges, highlighted_edge_indices=highlighted_edge_indices)      
        
        
        if "chatui" in self.para:
            highlighted_yml_content = highlight_yml_only(target_yml_desc)
            message = f"😎 <b>RAG from Cocoon</b>: Checking out all the column details for <i>{target_table}</i> ... {highlighted_yml_content}"
            message += f"<h3>Summary:</h3><p>{result['summary']}</p>"
            
            message += html_output
            
            result_list = "<h3>Column Matching Results:</h3><ul>"
            for target_col, matching in result['column_matching'].items():
                result_list += f"<li><b>{target_col}</b>:"
                result_list += f"<ul><li>Source columns: {', '.join(matching['source_columns'])}</li>"
                result_list += f"<li>Transformation: {matching['transformation']}</li></ul></li>"
            result_list += "</ul>"
            
            message += result_list
            
            self.para["chatui"].add_message("GenAI", message)
        
        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
        )
        
        def on_next_button_clicked(b):
            with self.output_context():
                callback(run_output)
        
        next_button.on_click(on_next_button_clicked)

        
        viewer = self.para.get("viewer", False)
        
        if viewer:
            on_next_button_clicked(next_button)
            return
        
        display(HTML(html_output))
        display(next_button)


class SchemaMatchingForAll(MultipleNode):
    default_name = 'Schema Matching For All'
    default_description = 'This node matches the schema for all tables.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["source_table_name"] = element_name
        para["idx"] = idx
        para["total"] = total
        
        node = IdentifySchemaMatching(para=para, id_para="source_table_name")
        node.inherit(self)
        return node
        
        
        

    def extract(self, item):

        previous_node_output = self.get_sibling_document('Business Question Data Sufficiency Check')
        
        related_tables = previous_node_output['related_tables']
        
        self.elements = related_tables
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}


class FindTablesAndMatchSchemaForAll(MultipleNode):
    default_name = 'Find Tables and Match Schema For All'
    default_description = 'This node finds tables and matches schema for all tables.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["target_table"] = element_name
        para["idx"] = idx
        para["total"] = total
        workflow = Workflow("Matching Schema", 
                        description="A workflow to match schema",
                        para=para,
                        id_para="target_table")
        
        workflow.inherit(self)
        
        workflow.add_to_leaf(TableSummaryAnalysisNode())
        workflow.add_to_leaf(SourceToTargetTableAnalysisNode())
        workflow.add_to_leaf(SchemaMatchingForAll())
        workflow.add_to_leaf(RefineSchemaMatching())
        workflow.add_to_leaf(JoinGraphFromMatchingBuilder())
        workflow.add_to_leaf(SQLQueryMatchingConstructor())
        
        return workflow

    def extract(self, item):
        
        related_steps = self.get_sibling_document('Related Steps Finder').get('related_steps', [])
        if related_steps:
            target_data_project = self.para['target_data_project']
            target_tables = extract_related_tables(target_data_project, related_steps)
            target_tables.sort()
        else:
            target_tables = list(self.para['target_data_project'].tables.keys())
        
        self.elements = target_tables
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        
        
class DBTSourceProjectConfigAndRead(Node):
    default_name = 'DBT Project Configuration and Read'
    default_description = 'This step allows you to specify an existing DBT project directory, preview, and read the project.'
    create = True

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        dbt_directory_input = widgets.Text(
            description='Catalog Directory:',
            value=self.para.get("dbt_directory", ""),
            placeholder="/path/to/your/dbt/project",
            style={'description_width': 'initial'},
            layout={'width': '50%'}
        )

        preview_button = widgets.Button(description="Preview Catalog", button_style='info', icon='eye')
        submit_button = widgets.Button(description="Submit", button_style='success', icon='check')
        submit_button.layout.display = 'none'

        output = widgets.Output()

        create = self.class_para.get("create", self.create)
        
        def read_project(directory):
            with self.output_context():
                if not directory:
                    print("Please specify a directory.")
                    return None
                
                display(HTML(f"{running_spinner_html} Creating temp views ..."))
                
                try:
                    con = self.item["con"]
                    database = self.para.get("database", None)
                    schema = self.para.get("schema", None)
                    
                    data_project = read_data_project_from_dir(directory, con, database=database, schema=schema, create=create)
                    
                    self.para["data_project"] = data_project
                    self.para["dbt_directory"] = directory
                    
                    return data_project
                except Exception as e:
                    print(f"Error reading project: {str(e)}")
                    return None

        def on_preview_click(b):
            with self.output_context():
                directory = dbt_directory_input.value
                data_project = read_project(directory)
                if data_project:
                    data_project.display_graph_static()
                    submit_button.layout.display = 'block'

        def on_submit_click(b):
            with self.output_context():
                directory = dbt_directory_input.value
                callback({"dbt_directory": directory})

        preview_button.on_click(on_preview_click)
        submit_button.on_click(on_submit_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "dbt_directory" in self.para:
                dbt_directory_input.value = self.para["dbt_directory"]
                on_preview_click(preview_button)
                
                if submit_button.layout.display != 'none':
                    on_submit_click(submit_button)
                return

        create_progress_bar_with_numbers(0, migration_steps)
        display(HTML("<h3>📂 Specify Source Catalog Directory</h3><em>Built by Cocoon</em>"))
        display(dbt_directory_input)
        display(preview_button)
        display(submit_button)
        display(output)
        
class DBTTargetProjectConfigAndRead(Node):
    default_name = 'DBT Project Configuration and Read'
    default_description = 'This step allows you to specify an existing DBT project directory, preview, and read the project.'
    create = True

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        dbt_directory_input = widgets.Text(
            description='Catalog Directory:',
            value=self.para.get("dbt_directory", ""),
            placeholder="/path/to/your/dbt/project",
            style={'description_width': 'initial'},
            layout={'width': '50%'}
        )

        preview_button = widgets.Button(description="Preview Catalog", button_style='info', icon='eye')
        submit_button = widgets.Button(description="Submit", button_style='success', icon='check')
        submit_button.layout.display = 'none'

        output = widgets.Output()

        create = self.class_para.get("create", self.create)
        
        def read_project(directory):
            with self.output_context():
                if not directory:
                    print("Please specify a directory.")
                    return None
                
                display(HTML(f"{running_spinner_html} Creating temp views ..."))
                
                try:
                    con = self.item["con"]
                    database = self.para.get("database", None)
                    schema = self.para.get("schema", None)
                    
                    data_project = read_data_project_from_dir(directory, con, database=database, schema=schema, create=create)
                    
                    self.para["data_project"] = data_project
                    self.para["dbt_directory"] = directory
                    
                    return data_project
                except Exception as e:
                    print(f"Error reading project: {str(e)}")
                    return None

        def on_preview_click(b):
            with self.output_context():
                directory = dbt_directory_input.value
                data_project = read_project(directory)
                if data_project:
                    data_project.display_graph_static()
                    submit_button.layout.display = 'block'

        def on_submit_click(b):
            with self.output_context():
                directory = dbt_directory_input.value
                callback({"dbt_directory": directory})

        preview_button.on_click(on_preview_click)
        submit_button.on_click(on_submit_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "dbt_directory" in self.para:
                dbt_directory_input.value = self.para["dbt_directory"]
                on_preview_click(preview_button)
                
                if submit_button.layout.display != 'none':
                    on_submit_click(submit_button)
                return

        create_progress_bar_with_numbers(0, migration_steps)
        display(HTML("<h3>📂 Specify Target Catalog Directory</h3><em>Built by Cocoon</em>"))
        display(dbt_directory_input)
        display(preview_button)
        display(submit_button)
        display(output)
        
        
class DBTProjectConfigAndReadMultiple(MultipleNode):
    default_name = 'DBT Project Config and Read Multiple'
    default_description = 'This creates DBTProjectConfigAndRead nodes for source and target directories'
    
        
    
    def construct_node(self, element_name="", dbt_directory="", create=True):
        para = self.para.copy()
        para["element_name"] = element_name
        para["dbt_directory"] = dbt_directory
        
        if element_name == "source":
            node = DBTSourceProjectConfigAndRead(para=para, class_para={"create": create})
        elif element_name == "target":
            node = DBTTargetProjectConfigAndRead(para=para, class_para={"create": create})
        else:
            node = DBTProjectConfigAndRead(para=para, class_para={"create": create})
        node.inherit(self)
        return node
    
    def extract(self, item):
        source_directory = self.para["source_dbt_directory"]
        target_directory = self.para["target_dbt_directory"]
        
        self.elements = ["source", "target"]
        self.nodes = {
            "source": self.construct_node("source", source_directory, create=True),
            "target": self.construct_node("target", target_directory, create=False)
        }
        self.item = item


class TableSummaryAnalysisNode(BusinessQuestionEntityRelationNode):
    def extract(self, item):
        clear_output(wait=True)

        source_data_project = self.para["source_data_project"]
        target_table_name = self.para["target_table"]
        targt_table_object = self.para["target_data_project"].table_object[target_table_name]
        
        create_progress_bar_with_numbers(2, migration_steps)
        display(HTML(f"🔍 Finding the source business process for target table <i>{target_table_name}</i>"))
        self.progress = show_progress(1)
        
        question = f"Transform to target table: '{targt_table_object.table_name}': {targt_table_object.table_summary}."
        story_list = source_data_project.get_story_list()
        
        return source_data_project, question, story_list


    def run(self, extract_output, use_cache=True):
        data_project, business_question, story_list = extract_output
        
        story_desc = "\n".join([f"{i+1}. '{item.get('Name') or item.get('name')}': {item.get('Description') or item.get('description')}" 
                        for i, item in enumerate(story_list)])

        template = f"""{business_question}

You have the source steps and descriptions:
{story_desc}

First reason about how the target table relates to the source steps.
Then provide the source step names that contains information for the target table.

Now respond in thefollowing format:
```yml
reasoning: >-
    The tagret table means ... In the context of source table ...
# leave it empty [] if no related steps
related_steps:
    - name: xx
      explanation: This is related because ...
    - name: xx
      ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'related_steps' not in analysis or not isinstance(analysis['related_steps'], list):
            raise ValueError("Analysis should contain a 'related_steps' key with a list value")
        
        for relation in analysis['related_steps']:
            if not isinstance(relation, dict) or 'name' not in relation or 'explanation' not in relation:
                raise ValueError("Each related relation should be a dictionary with 'name' and 'explanation' keys")
        
        return analysis
    
class SourceToTargetTableAnalysisNode(BusinessQuestionDataSufficiency):
    def extract(self, item):
        clear_output(wait=True)

        data_project = self.para["source_data_project"]        
        target_table_name = self.para["target_table"]
        targt_table_object = self.para["target_data_project"].table_object[target_table_name]

        create_progress_bar_with_numbers(2, migration_steps)
        display(HTML(f"🔍 Analyzing source tables for <i>{target_table_name}</i>..."))
        self.progress = show_progress(1)
        
        question = f"'{targt_table_object.table_name}' has columns:"
        
        column_dict = targt_table_object.get_column_desc_yml()
        question += yaml.dump(column_dict, default_flow_style=False)
        question += f"Goal: find all the source tables transformable to '{targt_table_object.table_name}'"
        
        df = pd.read_json(self.get_sibling_document('Business Question Entity and Relation Analysis')['related_steps'], orient="split")
        related_steps = df[df['Related'] == True]['Name'].tolist()

        tables = extract_related_tables(data_project, related_steps)

        schema_description = yaml.dump(data_project.describe_project_yml(tables=tables, limit=9999, exclude_keys=True))

        return question, schema_description, tables, data_project
    

    def run(self, extract_output, use_cache=True):
        business_question, schema_description, all_related_tables, data_project = extract_output

        template = f"""{business_question}

You have source tables and their columns:
{schema_description}

First find source tables with columns that are semantically transformable to columns in target tables.
E.g., Customer birthdate can be transformed to Client birthyear, because Customer is equivalent to Client.
However, Employee birthdate can't be transformed to Client birthyear due to semantic difference.
If transformable, provide a high-level explanation of what selections, aggregations, and joins might be needed, and over which columns and tables.

Return your analysis in the following format:
```yml
explanation: >-
    The target table means ... Some semantically relavant tables are ... Among these, X can be transformed...
transformable: true/false
sql_approach: >-
    If transformable, provide a detailed step-by-step instruction of the SQL approach.
    Dictates which tables, columns, and operations to use.
    However, don't provide codes.
# from sql_approach, list all related source tables
related_tables: ['table1', 'table2', ...]
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'transformable' not in analysis or not isinstance(analysis['transformable'], bool):
            raise ValueError("Analysis should contain a 'transformable' key with a boolean value")
        
        if 'explanation' not in analysis or not isinstance(analysis['explanation'], str):
            raise ValueError("Analysis should contain an 'explanation' key with a string value")
        
        if analysis['transformable']:
            if 'sql_approach' not in analysis or not isinstance(analysis['sql_approach'], str):
                raise ValueError("When data is transformable, analysis should contain a 'sql_approach' key with a string value")
            
            if 'related_tables' not in analysis or not isinstance(analysis['related_tables'], list):
                raise ValueError("When data is transformable, analysis should contain a 'related_tables' key with a list value")
            
            invalid_tables = [table for table in analysis['related_tables'] if table not in all_related_tables]
            if invalid_tables:
                raise ValueError(f"The following tables were not identified as related during extraction: {', '.join(invalid_tables)}")

        analysis['sufficient'] = analysis['transformable']
        return analysis
    
    
    



class PauseWorkflow(Node):
    default_name = 'Pause Workflow'
    default_description = 'Pauses the workflow and sets a callback function'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        if 'pause' not in self.item:
            self.item['pause'] = {}
        
        
        
        self.item['pause'][str(self.path)] = callback
        
        display(HTML('⏸️ Workflow paused...'))
        
        


       


class RefineSchemaMatching(Node):
    default_name = 'Refine Schema Matching'
    default_description = 'This node refines the schema matching by removing low confidence mappings and building a description.'

    def extract(self, input_item):
        clear_output(wait=True)

        target_table_name = self.para.get('target_table')
        source_data_project = self.para.get('source_data_project')
        target_data_project = self.para.get('target_data_project')
        create_progress_bar_with_numbers(2, migration_steps)
        display(HTML(f"🔍 Refining the schema matching for <i>{target_table_name}</i>..."))
        self.progress = show_progress(1)
        
        target_table = target_data_project.table_object[target_table_name]

        document = self.get_sibling_document('Schema Matching For All').get('Identify Schema Matching', {})

        refined_matching = {}
        mapping_description = []
        database_description = [f"Target Table: {target_table_name}\n{target_table.table_summary}"]

        for source_table_name, matching_info in document.items():
            column_matching = matching_info.get('column_matching', {})
            refined_column_matching = {}

            for target_column, details in column_matching.items():
                if details.get('confidence', False):
                    refined_column_matching[target_column] = details

            if refined_column_matching:
                refined_matching[source_table_name] = {
                    'summary': matching_info.get('summary', ''),
                    'column_matching': refined_column_matching
                }
                
                source_table = source_data_project.table_object[source_table_name]
                database_description.append(f"Source Table: {source_table_name}\n{source_table.table_summary}")

        target_columns = set(col for matching in refined_matching.values() for col in matching['column_matching'])
        target_columns = list(target_columns)
        target_columns.sort()
        for target_column in target_columns:
            mappings = []
            for source_table, matching_info in refined_matching.items():
                if target_column in matching_info['column_matching']:
                    source_columns = matching_info['column_matching'][target_column]['source_columns']
                    transformation = matching_info['column_matching'][target_column]['transformation']
                    mappings.append(f"{source_table} ({', '.join(source_columns)}) - {transformation}")
            
            if mappings:
                mapping_description.append(f"Target column '{target_column}' can be mapped from:\n" + "\n".join(f"- {m}" for m in mappings))

        database_description.sort()
        mapping_description_text = "\n\n".join(mapping_description)
        database_description_text = "\n\n".join(database_description)
        
        

        return  mapping_description_text, database_description_text
    
    def run(self, extract_output, use_cache=True):
        mapping_description_text, database_description_text = extract_output
        
        if not mapping_description_text:
            return self.run_but_fail(extract_output)
        
        template = f"""You have a database with target table and source tables:
{database_description_text}

Here are the potential column mappings:
{mapping_description_text}

Refine the schema matching:
Some target column have mappings from multiple source tables. 
Unless the target column is a derived from multiple source columns, it should only be mapped from one source table.
Propose a refined schema matching in the following format:
```yml
summary: >-
    X columns can be transformed...
column_matching: # for all columns can be transformed
    target_column_name_1:
        source_columns: 
            - table_name: source_table_name_1
              column_name: source_column_name_1
            - table_name: source_table_name_2
              column_name: source_column_name_2
              ...
        transformation: "direct copy"
    target_column_name_2:
        ...
```"""
        messages = [{"role": "user", "content": template}]

        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        result = yaml.load(yml_code, Loader=yaml.SafeLoader)
        return result
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"summary": "Unable to generate a refined schema matching.", "column_matching": {}}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        result = run_output
        target_table_name = self.para.get('target_table')

        nodes = {}
        target_table = f"(target) {target_table_name}"
        nodes[target_table] = []

        for target_col, matching in result['column_matching'].items():
            nodes[target_table].append(target_col)
            
            for source_col in matching['source_columns']:
                source_table = f"(source) {source_col['table_name']}"
                if source_table not in nodes:
                    nodes[source_table] = []
                if source_col['column_name'] not in nodes[source_table]:
                    nodes[source_table].append(source_col['column_name'])

        edges = []
        
        for target_col, matching in result['column_matching'].items():
            for source_col in matching['source_columns']:
                source_table = f"(source) {source_col['table_name']}"
                edge = (source_table, source_col['column_name'], target_table, target_col)
                edges.append(edge)

        html_output = generate_schema_graph_graphviz(nodes, edges)      

        data = []
        for target_col, matching in result['column_matching'].items():
            data.append({
                'Target Column': target_col,
                'Sources': matching['source_columns'],
                'Instruction': matching.get('transformation', '')
            })

        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [False, True, True]
        reset = True
        editable_list = {
            'Sources': {},
        }
        
        grid = create_dataframe_grid(df, editable_columns, reset=reset, editable_list=editable_list)
    
        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
        )
        
        def on_next_button_clicked(b):
            with self.output_context():
                updated_df = grid_to_updated_dataframe(grid)
                document = updated_df.to_json(orient="split")
                callback(document)
        
        next_button.on_click(on_next_button_clicked)

        
        viewer = self.para.get("viewer", False)
        
        if viewer:
            on_next_button_clicked(next_button)
            return
        
        display(HTML(html_output))
        display(HTML(f"<h3>😎 Your feedback:</h3>"))
        display(grid)
        display(next_button)

        
class JoinGraphFromMatchingBuilder(Node):
    default_name = 'Join Graph From Matching Builder'
    default_description = 'This node builds a join graph based on the schema matching.'
    
    def extract(self, item):
        clear_output(wait=True)
        
        display(HTML(f"🔗 Joining the source tables..."))
        self.progress = show_progress(1)
        
        source_data_project = self.para.get('source_data_project')
        
        schema_matching_df = pd.read_json(self.get_sibling_document('Refine Schema Matching'), orient="split")
        document = self.get_sibling_document("Business Question Data Sufficiency Check")
        question = document["business_question"]
        sql_approach = document["sql_approach"]
        
        related_tables = set()
        for _, row in schema_matching_df.iterrows():
            sources = row['Sources']
            if isinstance(sources, str):
                try:
                    sources = eval(sources)
                except:
                    continue
            if isinstance(sources, list):
                for source in sources:
                    if isinstance(source, dict) and 'table_name' in source:
                        related_tables.add(source['table_name'])
        
        related_tables = list(related_tables)
        related_tables = sorted(related_tables)
        
        key_info = source_data_project.get_key_info(related_tables, include_ri=False)
        
        return related_tables, key_info, question, sql_approach
    
    def run(self, extract_output, use_cache=True):
        related_tables, key_info, question, sql_approach = extract_output
        
        if not related_tables:
            return self.run_but_fail(extract_output)
            
        key_info_str = yaml.dump(key_info, default_flow_style=False)
        
        template = f"""Given the task:
{question}

The high-level instruction:
{sql_approach}
        
The following related tables with primary key and foreign key information:
{key_info_str}

Please explain how these tables shall be join to finish the tasks.
Choose the most sensiable join path.

Now respond in the following format:
```yml
reasoning: >-
    Explain how to join {", ".join(related_tables)}
join_graph:
    - ['foreign_table1', 'foreign_key1', 'primary_table1', 'primary_key1']
    - ['foreign_table2', 'foreign_key2', 'primary_table2', 'primary_key2']
    # Add more joins to connect all tables
```"""
        
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'join_graph' not in analysis:
            raise ValueError("Analysis should contain a 'join_graph' key")
        
        if analysis['join_graph'] and not isinstance(analysis['join_graph'], list):
            raise ValueError("Analysis should contain a 'join_graph' key with a list value")
        
        if not analysis['join_graph']:
            analysis['join_graph'] = []
        
        for join in analysis['join_graph']:
            if not isinstance(join, list) or len(join) != 4:
                raise ValueError("Each join in the join_graph should be a list with exactly 4 elements")
        
        return analysis
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"reasoning": "Unable to generate reasoning for the join graph.", "join_graph": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        result = run_output
        
        if len(result["join_graph"]) == 0:
            callback(run_output)
            return
        
        nodes = {}
        edges = []
        
        for join in result['join_graph']:
            foreign_table, foreign_key, primary_table, primary_key = join
            
            foreign_table = f"(source) {foreign_table}"
            primary_table = f"(source) {primary_table}"
            
            if foreign_table not in nodes:
                nodes[foreign_table] = []
            if foreign_key not in nodes[foreign_table]:
                nodes[foreign_table].append(foreign_key)
            
            if primary_table not in nodes:
                nodes[primary_table] = []
            if primary_key not in nodes[primary_table]:
                nodes[primary_table].append(primary_key)
            
            edge = (foreign_table, foreign_key, primary_table, primary_key)
            edges.append(edge)
        
        html_output = generate_schema_graph_graphviz(nodes, edges)
        
        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
        )
        
        def on_next_button_clicked(b):
            with self.output_context():
                callback(run_output)
        
        next_button.on_click(on_next_button_clicked)
        
        viewer = self.para.get("viewer", False)
        
        if viewer:
            on_next_button_clicked(next_button)
            return
        
        display(HTML(html_output))
        display(next_button)

class SQLQueryMatchingConstructor(Node):
    default_name = 'SQL Query Constructor'
    default_description = 'This node constructs the SQL query based on refined schema matching and join graph.'

    def extract(self, item):
        clear_output(wait=True)
        create_progress_bar_with_numbers(2, migration_steps)
        display(HTML(f"🛠️ Constructing the SQL query..."))
        self.progress = show_progress(1)

        schema_matching_df = pd.read_json(self.get_sibling_document('Refine Schema Matching'), orient="split")
        join_graph_output = self.get_sibling_document('Join Graph From Matching Builder')


        previous_node_output = self.get_sibling_document('Business Question Data Sufficiency Check')
        related_tables = previous_node_output['related_tables']
        data_project = self.para["source_data_project"]
        schema_description = yaml.dump(data_project.describe_project_yml(tables=related_tables, limit=9999))


        con = self.item["con"]
        
        database_hint = ""
        if con:
            database_name = get_database_name(con)
            database_hint = f"Note that we use {database_name} syntax. {database_general_hint.get(database_name, '')}"
        
        column_matching = schema_matching_df
        join_graph = join_graph_output['join_graph']
        
        database = self.para.get("database", None)
        schema = self.para.get("schema", None)
        
        identify_instruction = ""
        if database is not None and schema is not None:
            identify_instruction = f"Note that the database is {enclose_table_name(database, con=con)} and the schema is {enclose_table_name(schema, con=con)}. "
            identify_instruction += f"Identify each table. E.g., {enclose_table_name(database, con=con)}.{enclose_table_name(schema, con=con)}.{enclose_table_name('table_name', con=con)}"
            
            
        return column_matching, join_graph, database_hint, identify_instruction, schema_description

    def run(self, extract_output, use_cache=True):
        column_matching, join_graph, database_hint, identify_instruction, schema_description = extract_output

        column_matching_str = column_matching.to_csv(index=False) 
        join_graph_str = yaml.dump(join_graph, default_flow_style=False)
        
        if len(column_matching) == 0:
            return self.run_but_fail(extract_output)
            
        template = f"""Given the following Column Matching:
{column_matching_str}

The input tables schema:
{schema_description}

And the following join path:
{join_graph_str}

Please construct a complete SQL query that selects all mapped columns from the source tables and joins them according to the join graph. 
{database_hint}
{identify_instruction}
Your response should be in the following format:
```yml
reasoning: >-
    [Explain the structure of the SQL query and any important considerations]
sql_query: |
    SELECT 
        [list all mapped columns with appropriate table aliases]
    FROM 
        [start with the main table]
    JOIN 
        [list all necessary joins based on the join graph]
    WHERE 
        [add any necessary conditions]
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'sql_query' not in analysis or not isinstance(analysis['sql_query'], str):
            raise ValueError("Analysis should contain a 'sql_query' key with a string value")
        
        return analysis
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"reasoning": "Unable to generate reasoning for the SQL query.", "sql_query": ""}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        
        if not run_output["sql_query"]:
            callback(run_output)
            return
        
        display(HTML(f"<b>Summary:</b> <i>{run_output['reasoning']}</i>"))

        highlighted_html = highlight_sql(run_output['sql_query'])
        con = self.item["con"]
        if not con:
            highlighted_html += f"""<div style="color: orange; font-style: italic; font-size: 0.9em;">
    ⚠️ SQL query is just for reference based on schema. To ensure correctness, data access is required.
</div>"""
        
        display(HTML(highlighted_html))
        
        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
        )
        
        def on_next_button_clicked(b):
            clear_output(wait=True)
            callback(run_output)
        
        next_button.on_click(on_next_button_clicked)
        display(next_button)
        
        viewer = self.para.get("viewer", False)
        
        if viewer:
            on_next_button_clicked(next_button)
            return

def create_cocoon_table_transform_workflow(con=None, query_widget=None, viewer=False, para=None, output=None):
    
    if para is None:
        para = {}
    
    if query_widget is None:
        query_widget = QueryWidget(con)

    source_table_object = Table()
    item = {
        "con": con,
        "query_widget": query_widget
    }
    
    workflow_para = {"viewer": viewer, "source_table_object": source_table_object}
    
    for key, value in para.items():
        workflow_para[key] = value

    main_workflow = Workflow("Single Table Transformation Workflow", 
                            item = item, 
                            description="A workflow to transform a table into another table",
                            para = workflow_para,
                            output=output)
    
    main_workflow.add_to_leaf(SelectSchema(output = output))

    main_workflow.add_to_leaf(DBTProjectConfigAndReadMultiple(output=output))
    main_workflow.add_to_leaf(ConnectSourceDataNode(output=output))

    main_workflow.add_to_leaf(RelatedCategoriesFinder(output=output)) 
    main_workflow.add_to_leaf(MapBusinessConcepts(output=output))
    main_workflow.add_to_leaf(RelatedStepsFinder(output=output))
    
    main_workflow.add_to_leaf(FindTablesAndMatchSchemaForAll(output=output))
    main_workflow.add_to_leaf(SummarizeSchemaMatching(output=output))
    

    return query_widget, main_workflow
        
class FindTablesAndMatchSchemaForAll(MultipleNode):
    default_name = 'Find Tables and Match Schema For All'
    default_description = 'This node finds tables and matches schema for all tables.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["target_table"] = element_name
        para["idx"] = idx
        para["total"] = total
        workflow = Workflow("Matching Schema", 
                        description="A workflow to match schema",
                        para=para,
                        id_para="target_table")
        
        workflow.inherit(self)
        
        workflow.add_to_leaf(TableSummaryAnalysisNode())
        workflow.add_to_leaf(SourceToTargetTableAnalysisNode())
        workflow.add_to_leaf(SchemaMatchingForAll())
        workflow.add_to_leaf(RefineSchemaMatching())
        workflow.add_to_leaf(JoinGraphFromMatchingBuilder())
        workflow.add_to_leaf(SQLQueryMatchingConstructor())
        
        return workflow

    def extract(self, item):
        
        related_steps = self.get_sibling_document('Related Steps Finder').get('related_steps', [])
        if related_steps:
            target_data_project = self.para['target_data_project']
            target_tables = extract_related_tables(target_data_project, related_steps)
            target_tables.sort()
        else:
            target_tables = list(self.para['target_data_project'].tables.keys())
        
        self.elements = target_tables
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}
        
class DetectPIIColumns(ListNode):
    default_name = 'Detect PII Columns'
    default_description = 'This node identifies potential PII columns based on column names.'

    def extract(self, item):
        clear_output(wait=True)
        
        table_pipeline = self.para["table_pipeline"]
        table_name = table_pipeline.__repr__(full=False)
        con = self.item["con"]
        display(HTML(f"{running_spinner_html} Analyzing columns for PII in <i>{table_name}</i>..."))
        create_progress_bar_with_numbers(1, doc_steps)
        self.progress = show_progress(1)

        schema = table_pipeline.get_schema(self.item["con"])
        columns = list(schema.keys())
        columns = sorted(columns)
        
        column_chunks = [columns[i:i+50] for i in range(0, len(columns), 50)]
        
        sample_size = 5

        column_chunks_with_samples = []

        for chunk in column_chunks:
            table_desc = ""
            
            if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('pii_schema_only') is False):
                chunk_sample_df = table_pipeline.get_samples(con, columns=chunk, sample_size=sample_size)
                chunk_sample_df = chunk_sample_df.applymap(truncate_cell)
                
                table_desc = chunk_sample_df.to_csv(index=False, quoting=1)
            
            column_chunks_with_samples.append([chunk, table_desc])

        return column_chunks_with_samples

    def run(self, extract_output, use_cache=True):
        columns, table_desc = extract_output
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('pii') is False):
            return self.run_but_fail({})

        template = f"""Analyze the following column names and identify any that might contain Personally Identifiable Information (PII).
PII are:
(1) any information that can be used to distinguish or trace an individual's identity, such as name, social security number, date and place of birth, mother's maiden name, or biometric records 
(2) any other information that is linked or linkable to an individual, such as medical, educational, financial, and employment information
 
Column names:
{table_desc if table_desc else ', '.join(columns)}

Return your analysis in the following yml format:
```yml
reasoning: >-
    Your reasoning for identifying PII columns.
pii_columns:
    {columns[0]}: reason for identifying PII
    ...
```"""

        messages = [{"role": "user", "content": template}]
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('pii_model') is not None):
            api_type = self.para['cocoon_stage_options'].get('pii_model')
            response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache, api_type=api_type)
            response['choices'][0]['message']["content"] = response['choices'][0]['message']["content"].replace("\\n", "\n")
        else:
            response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
            
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)

        if not isinstance(analysis, dict):
            raise ValueError("Validation failed: The returned JSON code is not a dictionary.")

        if analysis.get("pii_columns", None):
            invalid_columns = set(analysis["pii_columns"].keys()) - set(columns)
            if invalid_columns:
                raise ValueError(f"Validation failed: One or more column names in the result "
                                f"are not present in the input columns. "
                                f"Invalid columns: {invalid_columns}")

        return analysis
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"reasoning": "Unable to generate reasoning for the PII detection.", "pii_columns": {}}
    
    def merge_run_output(self, run_outputs):
        merged_output = {}
        for run_output in run_outputs:
            merged_output.update(run_output.get('pii_columns', {}))
        return merged_output

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        create_progress_bar_with_numbers(0, doc_steps)
        
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]
        create_explore_button(query_widget, table_pipeline)
        
        data = {
            'Column Name': [],
            'Is PII': [],
            'Explanation': [],
        }

        all_columns = [col for chunk, _ in extract_output for col in chunk]

        for column in all_columns:
            is_pii = column in run_output
            explanation = run_output.get(column, '')

            data['Column Name'].append(column)
            data['Is PII'].append(is_pii)
            data['Explanation'].append(explanation)

        df = pd.DataFrame(data)


        editable_columns = [False, True, True]
        reset = True
        grid = create_dataframe_grid(df, editable_columns, reset=reset)

        submit_button = widgets.Button(description="Accept PII", button_style='success', icon='check')
        reject_button = widgets.Button(description="Reject PII", button_style='danger', icon='times')
        
        def submit(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset)
                
                pii_columns = df[df['Is PII'] == True]['Column Name'].tolist()
                
                if len(pii_columns) > 0:
                    table_object = self.para["table_object"]
                    for pii_column in pii_columns:
                        if pii_column not in table_object.pii:
                            table_object.pii.append(pii_column)
                    
                        
                        
                
                callback(df.to_json(orient="split"))
                return 
        
        def reject(b):
            with self.output_context():
                df = grid_to_updated_dataframe(grid, reset=reset)
                callback(df.to_json(orient="split"))
                return

        submit_button.on_click(submit)
        reject_button.on_click(reject)
        
        if self.viewer or ("viewer" in self.para and self.para["viewer"]):
            submit(submit_button)
            return
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('pii') is False):
            submit(submit_button)
            return
        
        
        display(HTML(f"<b>🧐 We've identified potential PII. Please verify.</b><div><em>We will flag them, and you can remove them next...</em></div>"))
        display(grid)
        button_box = widgets.HBox([reject_button, submit_button])
        display(button_box)

class WriteAcrossColumnTests(Node):
    default_name = 'Write Across-Column Tests'
    default_description = 'This node generates test cases for across-column comparisons and calculations.'

    def extract(self, item):
        clear_output(wait=True)
        display(HTML(f"{running_spinner_html} Writing more advanced expression tests..."))
        create_progress_bar_with_numbers(3, doc_steps)
        con = self.item["con"]
        database_name = get_database_name(con)
        table_pipeline = self.para["table_pipeline"]
        
        sample_size = 5
        sample_df = table_pipeline.get_samples(con, sample_size=sample_size)
        sample_df = sample_df.applymap(truncate_cell)
        
        return sample_df

    def run(self, extract_output, use_cache=True):
        sample_df = extract_output
        
        if (hasattr(self, 'para') and 
            isinstance(self.para, dict) and 
            isinstance(self.para.get('cocoon_stage_options'), dict) and 
            self.para['cocoon_stage_options'].get('expression_tests') is False):
            return self.run_but_fail({})
        
        if len(sample_df.columns) <= 1:
            return self.run_but_fail()

        template = f"""Given the following table sample data:
{sample_df.to_csv(index=False, quoting=1)}

Task: Generate a list of meaningful multi-column comparisons relations. 

Examples of good test cases:
1. For "volume 1", "volume 2", and "total volume",  "volume 1 + volume 2 = total volume" because "total volumne" is defined as sum.
2. For "start date", "end date" columns, "start date < end date" because "end" is after "start".

Focus on COMPARISON, NOT domain/regular pattern/NULL checks.
Should be MULTI-column, NOT SINGLE column range.
Use common sense for obvious relations. Don't infer strange. Remeber you only see a sample.

For each test case, provide: Test name, Short explanation, and SQL expression

Return the results in the following JSON format:
```yml
summary: >-
    There exist obvious comparison relations across multiple columns...
# if not exist, leave tests as []
tests:
  - test_name: Volume Sum Check
    explanation: The total volumne should be the sum of two
    where_clause: volume_1 + volume_2 = total_volume
  - test_name: Date Range Check
    explanation: The end date should be after start.
    where_clause: start_date < end_date
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        test_cases = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(test_cases, dict) or 'tests' not in test_cases:
            raise ValueError("The returned YAML should be a dictionary with a 'tests' key containing a list of test cases.")
        
        for test_case in test_cases['tests']:
            if not all(key in test_case for key in ["test_name", "explanation", "where_clause"]):
                raise ValueError("Each test case should have 'test_name', 'explanation', and 'where_clause'.")
            
        
        valid_test_cases = []
        con = self.item["con"]
        table_pipeline = self.para['table_pipeline']
        with_context = table_pipeline.get_codes(mode="WITH")
        
        for test_case in test_cases['tests']:
            query = f"SELECT COUNT(*) AS CNT FROM {table_pipeline} WHERE ({test_case['where_clause']})"
            query = with_context + "\n" + query
            try:
                df = run_sql_return_df(con, query)
                violation_count = df['CNT'].iloc[0]
                test_case['violation_count'] = int(violation_count)
                valid_test_cases.append(test_case)
            except Exception as e:
                if cocoon_main_setting['DEBUG_MODE']:
                    print(f"Error running test '{test_case['test_name']}': {str(e)}")
        
        return {
            'summary': test_cases.get('summary', 'Across-column comparison tests'),
            'tests': valid_test_cases
        }

    def run_but_fail(self, extract_output, use_cache=True):
        return {"summary": "No across-column comparison tests found", "tests": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))
        
        table_pipeline = self.para["table_pipeline"]
        query_widget = self.item["query_widget"]
        with_context_clause = table_pipeline.get_codes(mode="WITH")
        
        tests = run_output['tests']
        
        if len(tests) == 0:
            callback(tests)
            return
        
        
        
        def create_test_widget(test):
            result_text = (
                f"<div style='color: red;'>❌ {test['violation_count']} rows don't satisfy the expressions</div>"
                if test['violation_count'] > 0
                else "<div style='color: green;'>✔️ All rows satisfy the expressions</div>"
            )
            
            html_content = f"""
            <div style='line-height: 1.2;'>
                <h3 style='margin-bottom: 5px;'>{test['test_name']}</h3>
                <em style='display: block; margin-bottom: 5px;'>{test['explanation']}</em>
                <pre style='margin: 5px 0;'><code>{test['where_clause']}</code></pre>
                {result_text}
            </div>
            """
            content_widget = widgets.HTML(html_content)
            
            run_button = widgets.Button(description="Run SQL", button_style='info', icon='play')
            
            endorse_checkbox = widgets.Checkbox(description="Endorse Test", value=True)
            
            def on_run_button_click(b):
                with self.output_context():
                    query = "SELECT *"
                    query += f'\nFROM {table_pipeline}'
                    query += f'\nWHERE ({test["where_clause"]})'
                    query_widget.update_context_value(with_context_clause)
                    query_widget.run_query(query)
                    print("😎 Query submitted. Check out the data widget!")
            
            run_button.on_click(on_run_button_click)
            
            return widgets.VBox([
                content_widget, 
                widgets.HBox([run_button, endorse_checkbox])
            ])

        def create_all_tests_widget():
            test_widgets = [create_test_widget(test) for test in tests]
            return widgets.VBox(test_widgets, layout=widgets.Layout(border='1px solid #ddd', padding='10px'))

        custom_css = """
        <style>
        .widget-html h3 { margin-top: 0; }
        .widget-vbox { margin-bottom: 10px; }
        </style>
        """
        display(HTML(custom_css))

        main_widget = create_all_tests_widget()
        display(main_widget)

        def get_checkbox_values(widget):
            checkbox_values = []
            for child in widget.children:
                if isinstance(child, widgets.VBox):
                    checkbox = child.children[1].children[1]
                    if isinstance(checkbox, widgets.Checkbox):
                        checkbox_values.append(checkbox.value)
            return checkbox_values
        
        next_button = widgets.Button(description='Accept Tests',  button_style='success', icon='check')
        reject_button = widgets.Button(description="Reject Tests", button_style='danger', icon='times')
        
        def on_next_button_clicked(b):
            with self.output_context():
                checkbox_values = get_checkbox_values(main_widget)
                endorsed_tests = [test for test, endorse in zip(tests, checkbox_values) if endorse]
                table_object = self.para["table_object"]
                table_object.true_expressions += endorsed_tests
                callback(endorsed_tests)
                return
        
        def reject(b):
            with self.output_context():
                callback([])
                return
        
        next_button.on_click(on_next_button_clicked)
        reject_button.on_click(reject)
        
        next_button.on_click(on_next_button_clicked)
        
        viewer = self.para.get("viewer", False)
        
        if viewer:
            on_next_button_clicked(next_button)
            return
        
        button_box = widgets.HBox([reject_button, next_button])
        display(button_box)



def distribute_groups(total, target_size):
    if total <= target_size:
        return [total]
    
    num_groups = total // target_size
    remainder = total % target_size
    
    groups = [target_size] * num_groups
    
    if remainder > 0:
        for i in range(remainder):
            groups[i % len(groups)] += 1
    
    return groups





class GetColumnCategoriesList(ListNode):
    default_name = 'Get Column Descriptions'
    default_description = 'This node provides summaries of columns.'

    def extract(self, item):
        self.input_item = item
        columns = self.para.get("columns", [])
        unit = self.para.get("unit", "columns")
        target_group_size = self.para.get("target_group_size", 100)

        group_sizes = distribute_groups(len(columns), target_group_size)
        
        outputs = []
        start = 0
        
        for group_size in group_sizes:
            end = start + group_size
            outputs.append((columns[start:end], unit))
            start = end
        
        return outputs

    def run(self, extract_output, use_cache=True):
        columns, unit = extract_output

        template = f"""You have the following {unit}:
{columns}
Please provide a high-level summary of the {unit}, in < 100 words.
Now, respond in yml format:
```yml
Summary: >-
    These {unit} are about ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.safe_load(yml_code)
        
        return summary['Summary']

    def merge_run_output(self, run_outputs):
        return "\n".join([f"{i+1}. {summary}" for i, summary in enumerate(run_outputs)])
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)
    
class ColumnCategorizerNode(Node):
    default_name = 'Column Categorizer'
    default_description = 'This node categorizes columns into groups based on their descriptions.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML(f"🔍 Categorizing columns into groups..."))
        self.progress = show_progress(1)

        self.input_item = item
        unit = self.para.get("unit", "columns")
        summary_str = self.get_sibling_document('Get Column Descriptions')

        return summary_str, unit

    def run(self, extract_output, use_cache=True):
        summary_str, unit = extract_output

        template = f"""You have the following column summary:
{summary_str}
Please categorize the {unit} into at most 5 but preferably fewer groups, and a short description (<100 words) for each. 
The groups should be mutually exclusive, and collectively exhaustive for all {unit}.
Now, respond in yml format:
```yml
groups: 
    Customer Information: >-
        This group contains {unit} related to customer information, such as name, address, and email.
    ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.safe_load(yml_code)
        
        if not isinstance(summary, dict):
            raise ValueError("Summary should be a dictionary")
        
        if 'groups' not in summary or not isinstance(summary['groups'], dict):
            raise ValueError("Summary should contain a 'groups' key with a dictionary value")
        
        groups = summary["groups"]
        
        for group_name, description in groups.items():
            if not isinstance(description, str):
                raise ValueError(f"Description for group '{group_name}' should be a string")
        
        return [(group_name, description.strip()) for group_name, description in groups.items()]

    def run_but_fail(self, extract_output, use_cache=True):
        unit = extract_output[1]
        default_groups = [
            ("All", f"This group contains all {unit} that could not be categorized due to an error in the categorization process."),
        ]
        return default_groups
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)

class ClassifyColumnsList(ListNode):
    default_name = 'Classify Columns'
    default_description = 'This node classifies columns into groups and suggests new groups if necessary.'

    def extract(self, item):
        self.input_item = item
        columns = self.para.get("columns", [])
        unit = self.para.get("unit", "columns")
        target_group_size = self.para.get("target_group_size", 50)

        groups = self.get_sibling_document("Column Categorizer")
        self.groups = groups.copy()

        group_sizes = distribute_groups(len(columns), target_group_size)
        
        outputs = []
        start = 0
        
        for group_size in group_sizes:
            end = start + group_size
            outputs.append((columns[start:end], self.groups, unit))
            start = end
        
        return outputs

    def run(self, extract_output, use_cache=True):
        columns, groups, unit = extract_output

        group_str = "\n".join([f"{i+1}. {group}: {description}" for i, (group, description) in enumerate(groups)])
        column_str = "\n".join([f"{i+1}. {col}" for i, col in enumerate(columns)])

        template = f"""You have the following {unit}:
{column_str}

You have the following {unit} groups:
{group_str}

First, provide a summary of whether the current groups are sufficient to categorize all the {unit}.
Most of the time it is sufficient.
In rare cases, the current groups are not sufficient.
Then suggest 1 or 2 groups with short descriptions.

If you suggest new groups, their indices should be incremental to the existing ones.
For example, if there are 5 existing groups, the new groups should start from index 6.

Then, categorize the {unit}. For each {unit}, give the group index it belongs to.
Please make sure each {unit} is in exactly one group.

Create a new category if there are {unit} that don't fit into any existing category.
Do not use an "Other" category. Instead, create specific new categories as needed.

Respond in yml format:
```yml
summary:
    assessment: These groups are sufficient / insufficient and new groups are desperately needed
    sufficient: true/false
    # only include the following if 'sufficient' is false
    new_groups:
        6. New Group Name: Short description of the new group
        7. Another New Group: Short description of another new group
        ...

Category:
    1: group_index
    2: group_index
    ...
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        summary = yaml.safe_load(yml_code)
        
        if 'summary' in summary:
            if not summary['summary']['sufficient'] and 'new_groups' in summary['summary']:
                for new_group_entry, description in summary['summary']['new_groups'].items():
                    index, new_group = new_group_entry.split('. ', 1)
                    if new_group not in [group for group, _ in self.groups]:
                        self.groups.append((new_group, description))

        categories = summary["Category"]
        
        self.validate_categories(categories, len(columns), len(self.groups), unit)
        
        final_categories = {}
        for index, group_index in categories.items():
            column_index = int(index) - 1
            final_categories[columns[column_index]] = group_index - 1

        return final_categories

    def validate_categories(self, categories, size, num_groups, unit):
        all_indices = set(range(1, size+1))
        seen_indices = set(map(int, categories.keys()))

        if not seen_indices.issubset(all_indices):
            extra_indices = seen_indices - all_indices
            raise ValueError(f"Some categorized {unit} indices are out of range: {extra_indices}")

        if seen_indices != all_indices:
            unseen_indices = all_indices - seen_indices
            print(f"Warning: Not all {unit} were categorized. Uncategorized {unit} indices: {unseen_indices}")
        
        for index, group_index in categories.items():
            if not isinstance(group_index, int):
                raise ValueError(f"{unit.capitalize()} can only belong to one group. {unit.capitalize()} {index} is assigned an invalid group type.")
            if group_index < 1 or group_index > num_groups:
                raise ValueError(f"Invalid group index {group_index} for {unit} {index}. Group index should be between 1 and {num_groups}.")

    def merge_run_output(self, run_outputs):
        merged_categories = {}
        for output in run_outputs:
            merged_categories.update(output)
        return merged_categories, self.groups
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)
    
    
def create_category_workflow(para=None, output=None):

    if para is None:
        para = {}
    main_workflow = Workflow("Category Workflow", 
                            item = {}, 
                            description="A workflow to category values",
                            para = para,
                            output=output)
    
    main_workflow.add_to_leaf(GetColumnCategoriesList(output=output))
    main_workflow.add_to_leaf(ColumnCategorizerNode(output=output))
    main_workflow.add_to_leaf(ClassifyColumnsList(output=output))
    
    return main_workflow


def build_hierarchy(columns, initial_category, size_threshold=30, unit="columns"):
    if len(columns) < size_threshold:
        return {initial_category: {"description": "", "children": columns}}
        
    def recurse(columns, current_category):
        if len(columns) < size_threshold:
            return columns

        category_hierarchy = {}

        main_workflow = create_category_workflow(para={"columns":columns, "unit": unit})
        main_workflow.start()
        classified_columns, updated_categories = main_workflow.global_document["Category Workflow"].get("Classify Columns")
        
        for i, (sub_cat, sub_cat_desc) in enumerate(updated_categories):
            sub_cat_columns = [col for col, cat_index in classified_columns.items() if cat_index == i]
            category_hierarchy[sub_cat] = {
                "description": sub_cat_desc,
                "children": recurse(sub_cat_columns, (sub_cat, sub_cat_desc))
            }

        return category_hierarchy

    return {initial_category: {
        "description": "",
        "children": recurse(columns, (initial_category, "Top-level category"))
    }}



def get_grouped_dfs(self):
    relation_df = pd.read_json(self.get_sibling_document('Refine Relations'), orient="split")
    entity_df = pd.read_json(self.get_sibling_document('Entity Understand'), orient="split")

    single_entity_relations = relation_df[relation_df['Entities'].apply(len) == 1]

    unique_entities = set(entity for entities in single_entity_relations['Entities'] for entity in entities)

    exclusive_entities = []
    for entity in unique_entities:
        entity_relations = relation_df[relation_df['Entities'].apply(lambda x: entity in x)]
        if all(entity_relations['Entities'].apply(len) == 1):
            exclusive_entities.append(entity)

    single_entity_result = entity_df[entity_df['Entity Name'].isin(exclusive_entities)][['Entity Name', 'Entity Description']]

    data_project = self.para["data_project"]
    group_df = data_project.groups

    relation_df = relation_df[relation_df['Relation Name'] != ""]

    return relation_df, group_df, single_entity_result




class ExtractTableGroups(Node):
    default_name = 'Extract Table Groups'
    default_description = 'This node extracts table groups from the data project.'

    def extract(self, item):
        clear_output(wait=True)

        
        relation_df, group_df, single_entity_result = get_grouped_dfs(self)

        combined_desc = {}
        
        relation_df = relation_df[relation_df['Relation Name'] != ""]
        for idx, row in relation_df.iterrows():
            name = row['Relation Name']
            desc = f"it involves {', '.join(row['Entities'])}, {row['Relation Description']}"
            combined_desc[name] = {
                'Type': 'Relation',
                'Description': desc
            }
        
        for idx, row in single_entity_result.iterrows():
            name = row['Entity Name']
            desc = row['Entity Description']
            combined_desc[name] = {
                'Type': 'Entity',
                'Description': desc
            }
        
        for idx, row in group_df.iterrows():
            name = row['Group Name']
            desc = row['Group Summary']
            combined_desc[name] = {
                'Type': 'Group',
                'Description': desc
            }
        
        return combined_desc
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        callback(extract_output)
        
def extract_titles(data):
    if isinstance(data, dict):
        for key, value in data.items():
            data[key] = extract_titles(value)
        return data
    elif isinstance(data, list):
        return [extract_title(item) for item in data]
    elif isinstance(data, str):
        return extract_title(data)
    else:
        return data

def extract_title(s):
    if s.startswith("'") and "'" in s[1:]:
        return s[1:s.index("'", 1)]
    return s



class BuildTableHierarchy(Node):
    default_name = 'Build Table Hierarchy'
    default_description = 'This node builds a hierarchy of tables based on the extracted table groups.'

    def extract(self, item):
        clear_output(wait=True)

        display(HTML("🌳 Clustering the core relations and groups..."))
        create_progress_bar_with_numbers(3, model_steps)

        combined_desc = self.get_sibling_document('Extract Table Groups')

        tables = [f"'{name}': {details['Description']}" for name, details in combined_desc.items()]

        dbt_project_name = self.para["dbt_name"]

        category_hierarchy = build_hierarchy(tables, dbt_project_name, unit="tables")
        extracted_category = extract_titles(category_hierarchy)
        
        return extracted_category
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        extracted_category = extract_output
        
        def clean_category(category):
            if isinstance(category, dict):
                for key, value in list(category.items()):
                    if isinstance(value, dict) and 'children' in value:
                        clean_category(value['children'])
                        
                        if not value['children']:
                            del category[key]
                    elif isinstance(value, dict):
                        clean_category(value)
                        
                        if not value:
                            del category[key]
                
                if 'children' in category:
                    category['children'] = {k: v for k, v in category['children'].items() if v}
                    
                    if isinstance(category['children'], dict):
                        category['children'] = {k: v for k, v in category['children'].items() if v}
                    
                    if not category['children']:
                        del category['children']
            
            return category


        cleaned_category = clean_category(extracted_category)
        
        def on_next_button_clicked(b):
            with self.output_context():
                callback(cleaned_category)
                return
            
        if isinstance(cleaned_category, dict) and len(cleaned_category) == 1:
            key = next(iter(cleaned_category))
            
            if 'children' in cleaned_category[key] and isinstance(cleaned_category[key]['children'], list):
                on_next_button_clicked(None)
                return
        
        clear_output(wait=True)
        display(HTML("😎 We have clustered the relations based on the concepts..."))
        
        html_output = generate_html_tree(cleaned_category)
        html_content = f"""
<style>
{tree_hierarchy_css}
</style>
<body>
{html_output}
</body>
"""

        next_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check' 
        )
        

        
        next_button.on_click(on_next_button_clicked)

        viewer = self.para.get("viewer", False)
        
        if viewer:
            on_next_button_clicked(next_button)
            return
        
        display(HTML(html_content))
        display(next_button)


def extract_category_paths(data, current_path=None):
    if current_path is None:
        current_path = []
    
    result = []
    
    if isinstance(data, dict):
        for key, value in data.items():
            new_path = current_path + [key]
            if isinstance(value, dict):
                if 'children' in value:
                    if isinstance(value['children'], list):
                        result.append((new_path, value['children']))
                    elif isinstance(value['children'], dict):
                        result.extend(extract_category_paths(value['children'], new_path))
            elif isinstance(value, list):
                result.append((new_path, value))
    
    if isinstance(data, list):
        result.append((["All"], data))
    return result


class ReorderRelationToStoryForAll(MultipleNode):
    default_name = 'Reorder Relation to Story for All'
    default_description = 'This node allows users to reorder relations for all paths in the story.'

    def construct_node(self, element_name, content=None, group_desc=None, 
                       relation_df=None, group_df=None, single_entity_result=None,
                       idx=0, total=0):
        para = self.para.copy()
        para["path_name"] = str(element_name)
        para["path"] = element_name
        para["content"] = content
        para["group_desc"] = group_desc
        para["path_idx"] = idx
        para["total_paths"] = total
        para["relation_df"] = relation_df
        para["group_df"] = group_df
        para["single_entity_result"] = single_entity_result
        node = ReorderRelationToStory(para=para, id_para="path_name")
        node.inherit(self)
        return node
    
    def extract(self, item):
        extracted_category = self.get_sibling_document('Build Table Hierarchy')
        group_desc = self.get_sibling_document('Extract Table Groups')
        paths = extract_category_paths(extracted_category)
        
        relation_df, group_df, single_entity_result = get_grouped_dfs(self)
        
        self.elements = [str(path) for path, _ in paths]
        self.nodes = {str(path): self.construct_node(path, content, group_desc, relation_df, group_df, single_entity_result, idx, len(paths)) 
                      for idx, (path, content) in enumerate(paths)}


class DocumentUploadNode(Node):
    default_name = 'Relevant Document Upload'
    default_description = 'Upload relevant documents or code to help understand the data.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        next_button = widgets.Button(description="Skip", button_style='success', icon='check')

        def on_button_click(b):
            with self.output_context():
                callback({})

        next_button.on_click(on_button_click)

        button_style = '''<style>
.modern-button {
    background-color: white;
    border: 1px solid black;
    color: #333;
    padding: 10px 20px;
    text-align: center;
    text-decoration: none;
    display: inline-flex;
    align-items: center;
    justify-content: center;
    font-size: 16px;
    margin: 4px 2px;
    cursor: pointer;
    border-radius: 8px;
    transition: all 0.3s ease 0s;
    box-shadow: 0 2px 5px rgba(0,0,0,0.1);
    height: 20px;
    width: 50px;
}

.modern-button:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 10px rgba(0,0,0,0.2);
}

.modern-button:active {
    transform: translateY(1px);
    box-shadow: 0 2px 3px rgba(0,0,0,0.1);
}

.modern-button svg {
    width: 24px;
    height: 24px;
    margin-right: 10px;
}
</style>
        '''

        def create_button(img, text):
            return f'''<div class="modern-button">
        {img}  <span>&nbsp;{text}</span>
        </div>
        '''

        buttons_html = f'''
        {button_style}
        {create_button(dbt_img, "dbt")}
        {create_button(air_flow_img, "Airflow")}
        {create_button(looker_img, "Looker")}
        {create_button(tableau_img, "Tableau")}
        {create_button(powerbi_img, "PowerBI")}
        {create_button(document_img, "Files")}
        '''

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            on_button_click(next_button)
        
        create_progress_bar_with_numbers(1, model_steps)
        display(HTML("<div style='line-height: 1.2;'><h3>📁 Upload Relevant Documents</h3><em>Add any documents or code that will help understand the data</em></div>"))
        display(HTML(buttons_html))
        display(next_button)
        

class RelatedCategoriesFinder(Node):
    default_name = 'Related Categories Finder'
    default_description = 'This node finds related target categories given a source story.'
    
    def extract(self, item):
        clear_output(wait=True)
        
        create_progress_bar_with_numbers(1, migration_steps)
        display(HTML(f"🔍 Identifying relavant categories ..."))
        self.progress = show_progress(1)
        
        source_data_project = self.para.get('source_data_project')
        target_data_project = self.para.get('target_data_project')
        
        source_story_list = source_data_project.get_story_list()
        source_story_str = "\n".join([f"'{item['name']}': {item['description']}" for item in source_story_list])
        
        target_categories = target_data_project.get_lowest_categories()
        
        return source_story_str, target_categories
    
    def run(self, extract_output, use_cache=True):
        source_story_str, target_categories = extract_output
        
        if not source_story_str or not target_categories:
            return self.run_but_fail(extract_output)
        
        target_categories_str = "\n".join(f"'{name}': {description}" for name, description in target_categories.items())
        
        template = f"""Given the following source stories:

{source_story_str}

And the following target categories:

{target_categories_str}

Please analyze the source stories and find the most relevant target categories. Only select categories from the provided list. Return your response in the following format:

```yml
reasoning: >-
    Explain your thought process in determining the related categories
related_categories:
    - category1
    - category2
    # Add more categories as needed, ensuring they are from the provided list
```"""
        
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict):
            raise ValueError("Analysis should be a dictionary")
        
        if 'reasoning' not in analysis or not isinstance(analysis['reasoning'], str):
            raise ValueError("Analysis should contain a 'reasoning' key with a string value")
        
        if 'related_categories' not in analysis or not isinstance(analysis['related_categories'], list):
            raise ValueError("Analysis should contain a 'related_categories' key with a list value")
        
        invalid_categories = [cat for cat in analysis['related_categories'] if cat not in target_categories]
        if invalid_categories:
            raise ValueError(f"The following categories are not in the target categories: {', '.join(invalid_categories)}")
        
        return analysis
    
    def run_but_fail(self, extract_output, use_cache=True):
        source_story_str, target_categories = extract_output
        if not target_categories:
            return {"reasoning": "Unable to determine related categories.", "related_categories": []}
        return {"reasoning": "Unable to determine related categories.", "related_categories": list(target_categories.keys())}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        source_story_str, target_categories = extract_output
        analysis = run_output

        if not analysis['related_categories']:
            callback([])
            return

        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        self.progress.value += 1


        display(HTML(f"<b>Summary:</b> <i>{analysis['reasoning']}</i>"))

        related_categories = analysis['related_categories']
        display(HTML(f"<b>Relavant Categories in Target Data Model:</b>"))

        excluded_categories = list(set(target_categories.keys()) - set(related_categories))
        
        target_data_project = self.para['target_data_project']
        target_data_project.display_tree(exclude_categories=excluded_categories)

        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                callback(excluded_categories)

        submit_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)

        display(submit_button)
        
        viewer = self.para.get("viewer", False)
        if self.viewer or viewer:
            on_button_clicked(submit_button)
            return

class MapBusinessConcepts(Node):
    default_name = 'Map Business Concepts'
    default_description = 'This node maps business concepts between source and target stories.'
    
    def extract(self, item):
        clear_output(wait=True)
        
        create_progress_bar_with_numbers(1, migration_steps)
        display(HTML(f"🔗 Mapping business concepts..."))
        self.progress = show_progress(1)
        
        source_data_project = self.para.get('source_data_project')
        target_data_project = self.para.get('target_data_project')
        
        excluded_categories = self.get_sibling_document('Related Categories Finder')
        source_story_list = source_data_project.get_story_list()
        target_story_list = target_data_project.get_story_list(exclude_categories=excluded_categories)
        
        return source_story_list, target_story_list
    
    def run(self, extract_output, use_cache=True):
        source_story_list, target_story_list = extract_output
        
        if len(source_story_list) == 0 or len(target_story_list) == 0:
            return self.run_but_fail(extract_output=extract_output)
        
        source_stories = "\n".join([f"{item['description']}" for item in source_story_list])
        target_stories = "\n".join([f"{item['description']}" for item in target_story_list])
        
        template = f"""Given the following source stories:
{source_stories}

And the following target stories:
{target_stories}

Please map the terminology between source and target stories. 
Identify key business concepts that might be referred to differently in the source and target. 
For each mapping, provide a brief summary of what the concept means or any nuanced differences.

Return your response in the following format:

```yml
summary: >-
    Brief explanation of the source and target concept mapping
concept_mappings:
  - source_term: customer
    target_term: client
    summary: >-
        both refer to those who purchase goods or services
  # Add more mappings as needed
```"""
        
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict) or 'concept_mappings' not in analysis:
            raise ValueError("Analysis should be a dictionary with a 'concept_mappings' key")
        
        if not isinstance(analysis['concept_mappings'], list):
            raise ValueError("concept_mappings should be a list")
        
        for mapping in analysis['concept_mappings']:
            if not all(key in mapping for key in ['source_term', 'target_term', 'summary']):
                raise ValueError("Each mapping should have 'source_term', 'target_term', and 'summary' keys")
        
        
        filtered_mappings = [
            mapping for mapping in analysis['concept_mappings']
            if mapping.get('source_term') and mapping.get('target_term')
        ]

        analysis['concept_mappings'] = filtered_mappings

        return analysis
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"summary": "Unable to determine business concept mappings", "concept_mappings": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):

        data = {
            'Source Term': [],
            'Target Term': [],
            'Explanation': [],
            'Endorse': []
        }
        
        for mapping in run_output['concept_mappings']:
            data['Source Term'].append(mapping['source_term'])
            data['Target Term'].append(mapping['target_term'])
            data['Explanation'].append(mapping['summary'])
            data['Endorse'].append(True)
        
        df = pd.DataFrame(data)
        
        if len(df) == 0:
            callback(df.to_json(orient="split"))
            return
        
        editable_columns = [True, True, False, True]
        reset = True
        long_text = ['Explanation']
        grid = create_dataframe_grid(df, editable_columns, reset=reset, long_text=long_text)
        
        display(HTML(f"<b>Summary</b>: <i>{run_output['summary']}</i>"))
        
        display(grid)
        
        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )
        
        def on_button_click(b):
            with self.output_context():
                clear_output(wait=True)
                
                updated_df = grid_to_updated_dataframe(grid, reset=reset, long_text=long_text)
        
                callback(updated_df.to_json(orient="split"))
        
        submit_button.on_click(on_button_click)
        display(submit_button)
        
        viewer = self.para.get("viewer", False)
        if self.viewer or viewer:
            on_button_click(submit_button)
            return

class RelatedStepsFinder(Node):
    default_name = 'Related Steps Finder'
    default_description = 'This node finds related steps in target stories given a source story and business concept mapping.'
    
    def extract(self, item):
        clear_output(wait=True)
        
        create_progress_bar_with_numbers(1, migration_steps)
        display(HTML(f"🔍 Finding related steps..."))
        self.progress = show_progress(1)
        
        source_data_project = self.para.get('source_data_project')
        target_data_project = self.para.get('target_data_project')
        
        excluded_categories = self.get_sibling_document('Related Categories Finder')
        concept_mappings_df = pd.read_json(self.get_sibling_document('Map Business Concepts'), orient="split")

        endorsed_df = concept_mappings_df[concept_mappings_df['Endorse'] == True]
            
        concept_mappings_str = "Business concept mapping:\n"

        for _, row in endorsed_df.iterrows():
            source = row['Source Term']
            target = row['Target Term']
            explanation = row['Explanation']
            
            concept_mappings_str += f"'{source}' -> '{target}'\n"
            concept_mappings_str += f"{explanation}\n"

        source_story_list = source_data_project.get_story_list()
        target_story_list = target_data_project.get_story_list(exclude_categories=excluded_categories)
        
        return source_story_list, target_story_list, concept_mappings_str
    
    def run(self, extract_output, use_cache=True):
        source_story_list, target_story_list, concept_mappings_str = extract_output
        
        if len(source_story_list) == 0 or len(target_story_list) == 0:
            return self.run_but_fail(extract_output = extract_output)
        
        source_stories = "\n".join([f"'{item['name']}': {item['description']}" for item in source_story_list])
        target_stories = "\n".join([f"'{item['name']}': {item['description']}" for item in target_story_list])
                
        template = f"""Given the following source story steps:
{source_stories}

And the following target story steps:
{target_stories}

Consider the following business concept mappings:
{concept_mappings_str}

From the target list, identify the names of related steps to the source story. 
Consider the business concept mappings when making these connections. 
Return your response in the following format:

```yml
summary: >-
    The related steps are ...
related_steps:
    - step_name 1
    - step_name 2
    # Add more related steps here
```"""
        
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yml_code(response['choices'][0]['message']["content"])
        analysis = yaml.load(yml_code, Loader=yaml.SafeLoader)
        
        if not isinstance(analysis, dict) or 'summary' not in analysis or 'related_steps' not in analysis:
            raise ValueError("Analysis should be a dictionary with 'summary' and 'related_steps' keys")
        
        if ('related_steps' in analysis) and (analysis['related_steps']) and (not isinstance(analysis['related_steps'], list)):
            raise ValueError("related_steps should be a list")
        
        target_story_names = [item['name'] for item in target_story_list]
        invalid_steps = [step for step in analysis['related_steps'] if step not in target_story_names]
        if invalid_steps:
            raise ValueError(f"The following steps are not in the target story list: {', '.join(invalid_steps)}")
        
        return analysis
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {"summary": "Unable to find related steps.", "related_steps": []}
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        analysis = run_output

        if not analysis['related_steps']:
            callback(analysis)
            return

        if icon_import:
            display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        self.progress.value += 1

        display(HTML(f"<b>Summary:</b> <i>{analysis['summary']}</i>"))

        display(HTML(f"<b>Related Steps:</b> {', '.join(analysis['related_steps'])}"))


        def on_button_clicked(b):
            with self.output_context():
                clear_output(wait=True)
                callback(analysis)

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='success',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)

        display(submit_button)
        
        viewer = self.para.get("viewer", False)
        if self.viewer or viewer:
            on_button_clicked(submit_button)
            return
        
class SummarizeSchemaMatching(Node):
    default_name = 'Summarize Schema Matching'
    default_description = 'Summarizes the results of schema matching'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        
        create_progress_bar_with_numbers(2, migration_steps)
        display(HTML("🎉 We have matched all the tables and drafted the SQL"))
        document = self.get_sibling_document('Find Tables and Match Schema For All').get('Matching Schema', {})

        tab_data = []

        nodes = {}

        edges = []
                
        for target_table_name in document:
            schema_matching_df = pd.read_json(document[target_table_name]['Refine Schema Matching'], orient="split")
            result_match = {}

            for _, row in schema_matching_df.iterrows():
                target_col = row['Target Column']
                sources = row['Sources']
                instruction = row['Instruction']
                
                result_match[target_col] = {
                    'source_columns': ast.literal_eval(sources),
                    'transformation': instruction if instruction else None
                }


            target_table = f"(target) {target_table_name}"
            nodes[target_table] = []

            for target_col, matching in result_match.items():
                if target_col not in nodes[target_table]:
                    nodes[target_table].append(target_col)
                
                for source_col in matching['source_columns']:
                    source_table = f"(source) {source_col['table_name']}"
                    if source_table not in nodes:
                        nodes[source_table] = []
                    if source_col['column_name'] not in nodes[source_table]:
                        nodes[source_table].append(source_col['column_name'])



            for target_col, matching in result_match.items():
                for source_col in matching['source_columns']:
                    source_table = f"(source) {source_col['table_name']}"
                    edge = (source_table, source_col['column_name'], target_table, target_col)
                    edges.append(edge)

        html_output = generate_schema_graph_graphviz(nodes, edges)

        tab_data.append(["All Column Matching", html_output]) 
        
        for target_table_name in document:
            result_match = {}

            schema_matching_df = pd.read_json(document[target_table_name]['Refine Schema Matching'], orient="split")

            for _, row in schema_matching_df.iterrows():
                target_col = row['Target Column']
                sources = row['Sources']
                instruction = row['Instruction']
                
                result_match[target_col] = {
                    'source_columns': ast.literal_eval(sources),
                    'transformation': instruction if instruction else None
                }

            edges = []

            nodes = {}

            target_table = f"(target) {target_table_name}"
            nodes[target_table] = []

            for target_col, matching in result_match.items():
                if target_col not in nodes[target_table]:
                    nodes[target_table].append(target_col)
                
                for source_col in matching['source_columns']:
                    source_table = f"(source) {source_col['table_name']}"
                    if source_table not in nodes:
                        nodes[source_table] = []
                    if source_col['column_name'] not in nodes[source_table]:
                        nodes[source_table].append(source_col['column_name'])


            for target_col, matching in result_match.items():
                for source_col in matching['source_columns']:
                    source_table = f"(source) {source_col['table_name']}"
                    edge = (source_table, source_col['column_name'], target_table, target_col)
                    edges.append(edge)
                    
            html_output = generate_schema_graph_graphviz(nodes, edges)

            con = self.item.get("con")

            if con:
                sql_query = document[target_table_name]['Debug SQL']['sql_query']
                if 'Debug SQL' in document[target_table_name] and 'sample_output' in document[target_table_name]['Debug SQL']:
                    sample_output_html = pd.DataFrame(document[target_table_name]['Debug SQL']['sample_output']).to_html()
                else:
                    sample_output_html = ""
            else:
                run_output = document[target_table_name]['SQL Query Constructor']
                sql_query = run_output['sql_query']
                sample_output_html = """<div style="color: orange; font-style: italic; font-size: 0.9em;">
                ⚠️ SQL query is just for reference based on schema. To ensure correctness, data access is required.
                </div>"""

            if sql_query:
                highlighted_html = highlight_sql(sql_query)
            else:
                highlighted_html = ""
            
            final_html = html_output + highlighted_html + sample_output_html
            tab_data.append([target_table_name, final_html])
            
        
        tabs = create_dropdown_with_content(tab_data)
        display(tabs)
        
        options = {element[0]: element[1] for element in tab_data}
        model_html = generate_dropdown_html("cocoon", options, full=False)
        
        formatter = HtmlFormatter(style='monokai')
        css_style = formatter.get_style_defs('.highlight')
        transfortm_html = f"""<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <style>
        pre {{ line-height: 125%; }}
        .highlight {{
            background: #272822;
            color: #f8f8f2;
            border-radius: 4px;
            padding: 1em;
            margin: 1em 0;
            overflow-x: auto;
            font-size: 12px;
        }}
        {css_style}
        {tag_css}
        {pandas_css}
    </style>
</head>
<body>
    <div class="container mt-5">
        <div class="row mb-4">
            <div class="row mb-4">
                <div class="col-md-6">
                    <a href="https://github.com/Cocoon-Data-Transformation/cocoon" target="_blank"
                        class="text-decoration-none text-dark">
                        <div class="d-flex align-items-center">
                            <img src="https://raw.githubusercontent.com/Cocoon-Data-Transformation/cocoon/main/images/cocoon_icon.png"
                                alt="cocoon icon" width="40" class="mr-3">
                            <div class="mx-3">
                                <h4 class="mb-0">Cocoon Transformation</h4>
                                <small class="mb-0 text-muted">Transform from Source to Target using LLMs</small>
                            </div>
                        </div>
                    </a>
                </div>
                <div class="col-md-6 d-flex align-items-center justify-content-end">
                    <h3></h3>
                </div>
            </div>
            {model_html}
        </div>
    </div>    
    <footer class="footer mt-auto py-3 bg-light">
        <div class="container">
            <div style="height: 6rem;"></div>
        </div>
    </footer>
</body>
</html>
        """
        labels = ["HTML"]
        file_names = [os.path.join(".", "transform.html")]
        contents = [transfortm_html]
        
        overwrite_checkbox, save_button, save_files_click = ask_save_files(labels, file_names, contents)

def generate_full_step_page(cocoon_title, cocoon_note, project_title, side_list_html, tab_content_html, additional_css=""):
    model_html = f"""<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cocoon - Data Modeling</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/css/bootstrap.min.css"
        integrity="sha384-TX8t27EcRE3e/ihU7zmQxVncDAy5uIKz4rEkgIXeMed4M0jlfIDPvg6uqKI2xXr2" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.5.1.slim.min.js"
        integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj"
        crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.5.3/dist/js/bootstrap.bundle.min.js"
        integrity="sha384-ho+j7jyWK8fNQe+A12Hb8AhRq26LrZ/JpcUGGOn+Y7RsweNrtN/tE3MoK7ZeZDyx"
        crossorigin="anonymous"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-sql.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.24.1/components/prism-yaml.min.js"></script>
</head>
<style>
    /* for list-group-item highlight */
    .list-group-item.active {{
        background-color: #464646;
        border-color: #f8f9fa;
        color: #ffffff;
    }}

    .list-group-item.active:hover {{
        background-color: #bebebe;
        border-color: #e9ecef;
        color: #ffffff;
    }}

    /* for nav-link highlight */
    .nav-link {{
        color: #6c757d !important;
        /* Gray color for inactive links */
    }}

    .nav-link.active {{
        color: #000 !important;
        /* Black color for active link */
    }}

{small_table_style}
{code_container_style}
{additional_css}

    .code_container {{
        max-height: calc(60vh);
        overflow: auto;
    }}

    .code_container pre[class*="language-"] {{
        border-radius: 4px;
        padding: 1em;
        margin: .5em 0;
        overflow: auto;
        font-size: 12px;
    }}


    .carousel-indicators li {{
        background-color: #808080;
        /* Gray for inactive indicators */
    }}

    .carousel-indicators .active {{
        background-color: #000000;
        /* Black for active indicator */
    }}
</style>

<body>


    <div class="container mt-4">
        <!-- New row for the header -->
        <div class="row mb-4">
            <div class="col-md-6">
                <a href="https://github.com/Cocoon-Data-Transformation/cocoon" target="_blank"
                    class="text-decoration-none text-dark">
                    <div class="d-flex align-items-center">
                        <img src="https://raw.githubusercontent.com/Cocoon-Data-Transformation/cocoon/main/images/cocoon_icon.png"
                            alt="cocoon icon" width="40" class="mr-3">
                        <div>
                            <h4 class="mb-0">{cocoon_title}</h4>
                            <small class="mb-0">{cocoon_note}</small>
                        </div>
                    </div>
                </a>
            </div>
            <div class="col-md-6 d-flex align-items-center justify-content-end">
                <h2>{project_title}</h2>
            </div>
        </div>


        <div class="row">
            <div class="col-md-9">
                <div class="card">
                    <div class="card-body">
                        <div class="tab-content">
                            {tab_content_html}
                        </div>
                    </div>
                </div>

            </div>
            <div class="col-md-3">
                <div class="list-group" id="myTab" role="tablist">
                    {side_list_html}
                </div>
            </div>
        </div>
    </div>
</body>

</html>"""
    return model_html




def get_project_html(dbt_directory=None, con=None, model_directory=None, project_name="project", database = None, schema = None, sample_size=0):
    if con is None:
        con = duckdb.connect(database=':memory:')
        data_project = read_data_project_from_dir(dbt_directory, con, database=database, schema=schema)
    if model_directory is None:
        model_directory = os.path.join(dbt_directory, "models")
    catalog_page_items = []

    source_tables = []
    sources_file_path = os.path.join(model_directory, "sources.yml")

    def create_sample_string(sample_size):
        if sample_size == 0:
            return "schema only"
        elif sample_size == 1:
            return "first 1 row"
        else:
            return f"first {sample_size} rows"
    
    if os.path.exists(sources_file_path):
        with open(sources_file_path, 'r') as file:
            sources_yml = yaml.safe_load(file)
        
        sources = sources_yml.get("sources", [])
        for source in sources:
            if source.get("name") == "cocoon":
                source_tables = source.get("tables", [])
                database = source.get("database")
                schema = source.get("schema")
                
                table_names = [table["name"] for table in source_tables if isinstance(table, dict) and "name" in table]
                
                if database is not None and schema is not None:
                    source_tables = [
                        (f'{enclose_table_name(database, con)}.{enclose_table_name(schema, con)}.{enclose_table_name(table_name, con)}', table_name)
                        for table_name in table_names
                    ]
                break

        source_content = {}
        for table_full, table_short in source_tables:
            try:
                df = run_sql_return_df(con, f'SELECT * FROM {enclose_table_name(table_full, con)} LIMIT {sample_size}')
                source_content[table_short] = f'<p class="text-center mb-0">Table Preview<small class="text-muted">({create_sample_string(sample_size)})</small></p><div class="code_container mb-4 mx-4">{df.to_html()}</div>'
            except Exception as e:
                print(f"Error processing table {table_full}: {str(e)}")

        source_items = {
            'id': 'source',
            'title': 'Source',
            'list_description': 'Initial tables from data warehouses',
            'nav_description': 'We display the source tables from the data warehouses to model.',
            'content': generate_div_html('source', source_content),
            'nav_content': generate_choice_html('source', list(source_content.keys()), "Select Source Table")
        }
        
        catalog_page_items.append(source_items)

    partition_tables = []
    partition_dir = os.path.join(model_directory, "partition")

    if os.path.exists(partition_dir):
        partition_tables = [f[:-4] for f in os.listdir(partition_dir) if f.endswith('.yml')]

        partition_content = {}
        for table in partition_tables:
            try:
                file_path = os.path.join(partition_dir, f"{table}.yml")
                with open(file_path, 'r') as file:
                    yml_content = file.read()
                
                partition_yml_dict = yaml.safe_load(yml_content)
                
                nodes = [partition_yml_dict['models'][0]['name']] + partition_yml_dict['cocoon_meta']['partitions']
                edges = [(i, 0) for i in range(1, len(nodes))]
                
                partition_cluster_output = generate_workflow_html_multiple(nodes, edges, directional=True)
                
                partition_html = f"""
<p class="text-center">Table Partitions <small class="text-muted">(Partitions to cluster)</small></p>
<div class="container mb-4">{partition_cluster_output}</div>
<p class="text-center">{table}.yml <small class="text-muted"> (Document the partitions)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-yaml">{yml_content}</code></pre>
</div>
"""
        
                partition_content[table] = partition_html
            except Exception as e:
                print(f"Error processing partition table {table}: {str(e)}")

        partition_items = {
            'id': 'partition',
            'title': 'Partition',
            'list_description': 'Cluster table partitions',
            'nav_description': 'We display the partition tables and their configurations.',
            'content': generate_div_html('partition', partition_content),
            'nav_content': generate_choice_html('partition', list(partition_content.keys()), "Select Partition")
        }
        catalog_page_items.append(partition_items)

    stage_dir = os.path.join(model_directory, "stage")
    
    if os.path.exists(stage_dir):
        stage_tables = [f[:-4] for f in os.listdir(stage_dir) if f.endswith('.sql')]
        stage_content = {}
        for table in stage_tables:
            try:
                yml_file_path = os.path.join(stage_dir, f"{table}.yml")
                with open(yml_file_path, 'r') as file:
                    yml_content = file.read()

                sql_file_path = os.path.join(stage_dir, f"{table}.sql")
                with open(sql_file_path, 'r') as file:
                    sql_content = file.read()
                
                df = run_sql_return_df(con, f'SELECT * FROM {enclose_table_name(table, con)} LIMIT {sample_size}')
                
                stage_html = f"""
<p class="text-center mb-0">Table Preview <small class="text-muted">({create_sample_string(sample_size)})</small></p>
<div class="code_container mb-4 mx-4">{df.to_html()}</div>

<p class="text-center">{table}.sql <small class="text-muted">(clean the table)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-sql">{sql_content}</code></pre>
</div>
<p class="text-center">{table}.yml <small class="text-muted"> (Document the table)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-yaml">{yml_content}</code></pre>
</div>
"""
                stage_content[table] = stage_html
            except Exception as e:
                print(f"Error processing stage table {table}: {str(e)}")

        stage_items = {
            'id': 'stage',
            'title': 'Stage',
            'list_description': 'Clean and document the tables',
            'nav_description': 'We display the stage tables, their SQL transformations, and their configurations.',
            'content': generate_div_html('stage', stage_content),
            'nav_content': generate_choice_html('stage', list(stage_content.keys()), "Select Stage Table")
        }
        catalog_page_items.append(stage_items)

    snapshot_dir = os.path.join(model_directory, "snapshot")
    
    if os.path.exists(snapshot_dir):
        snapshot_tables = [f[:-4] for f in os.listdir(snapshot_dir) if f.endswith('.sql')]
        snapshot_content = {}
        for table in snapshot_tables:
            try:
                yml_file_path = os.path.join(snapshot_dir, f"{table}.yml")
                with open(yml_file_path, 'r') as file:
                    yml_content = file.read()

                sql_file_path = os.path.join(snapshot_dir, f"{table}.sql")
                with open(sql_file_path, 'r') as file:
                    sql_content = file.read()
                
                df = run_sql_return_df(con, f'SELECT * FROM {enclose_table_name(table, con)} LIMIT {sample_size}')
                
                snapshot_html = f"""
<p class="text-center mb-0">Table Preview <small class="text-muted">({create_sample_string(sample_size)})</small></p>
<div class="code_container mb-4 mx-4">{df.to_html()}</div>

<p class="text-center">{table}.sql <small class="text-muted">(snapshot definition)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-sql">{sql_content}</code></pre>
</div>
<p class="text-center">{table}.yml <small class="text-muted"> (Document the snapshot)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-yaml">{yml_content}</code></pre>
</div>
"""
                snapshot_content[table] = snapshot_html
            except Exception as e:
                print(f"Error processing snapshot table {table}: {str(e)}")

        snapshot_items = {
            'id': 'snapshot',
            'title': 'SCD Model',
            'list_description': 'Snapshot from the change events',
            'nav_description': 'We display the snapshot tables, their SQL definitions, and their configurations.',
            'content': generate_div_html('snapshot', snapshot_content),
            'nav_content': generate_choice_html('snapshot', list(snapshot_content.keys()), "Select Snapshot Table")
        }
        catalog_page_items.append(snapshot_items)
        
    join_content = ""
    join_file_path = os.path.join(model_directory, "join", "cocoon_join.yml")

    if os.path.exists(join_file_path):
        with open(join_file_path, 'r') as file:
            join_yml_content = file.read()

        join_yml_dict = yaml.safe_load(join_yml_content)
        foreign_key = reverse_join_graph(join_yml_dict)
        nodes, edges = generate_nodes_edges(foreign_key)
        
        for entry in join_yml_dict['join_graph']:
            table = entry['table_name']
            if table not in nodes:
                nodes.append(table)
                
        join_graph_output = generate_workflow_html_multiple(nodes, edges, directional=True)

        join_content = f"""
<p class="text-center">Join Graph <small class="text-muted">(FK to PK)</small></p>
<div class="container mb-4">{join_graph_output}</div>
<p class="text-center">cocoon_join.yml <small class="text-muted"> (Document the joins)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-yaml">{join_yml_content}</code></pre>
</div>
"""

        join_items = {
            'id': 'join',
            'title': 'Integration',
            'list_description': 'Join graph from PK/FK',
            'nav_description': 'We identify the primary key (PK) and foreign key (FK) from tables. We build a join graph that connects FK to PK.',
            'content': join_content,
            'nav_content': ''
        }
        catalog_page_items.append(join_items)

    er_content = ""
    er_file_path = os.path.join(model_directory, "er", "cocoon_er.yml")

    if os.path.exists(er_file_path):
        with open(er_file_path, 'r') as file:
            er_yml_content = file.read()
        
        er_yml_dict = yaml.safe_load(er_yml_content)
        
        relation_map = {entry['relation_name']: entry['entities'] 
                        for entry in er_yml_dict['relations'] 
                        if 'relation_name' in entry}
        
        if 'story' in er_yml_dict and isinstance(er_yml_dict['story'], dict):
            tree_output = generate_tree_html(er_yml_dict['story'])

            cat_content = f"""
<p class="text-center">Tree Categories</p>
<div class="container mb-4">{tree_output}</div>
<p class="text-center mt-4">cocoon_er.yml <small class="text-muted">(Document the entity relationships)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-yaml">{er_yml_content}</code></pre>
</div>
"""

            cat_items = {
                'id': 'tree',
                'title': 'Category',
                'list_description': 'Category of the tables',
                'nav_description': 'We recrusively summarize the data.',
                'content': cat_content,
                'nav_content': ''
            }
            catalog_page_items.append(cat_items)

    knowledge_content = {}
    path_names = {}

    if os.path.exists(er_file_path):
        with open(er_file_path, 'r') as file:
            er_yml_content = file.read()
        
        er_yml_dict = yaml.safe_load(er_yml_content)
        
        paths = extract_category_paths(er_yml_dict['story'])
        
        relation_map = {entry['relation_name']: entry['entities'] 
                        for entry in er_yml_dict['relations'] 
                        if 'relation_name' in entry}
        
        for path_idx, (path, content) in enumerate(paths):
            
            relation_details = content

            er_list_html = "\n".join([f'<li data-target="#cocoon_er_story_carousel_{path_idx}" data-slide-to="{idx}" class="{"active" if idx == 0 else ""}"></li>' 
                                    for idx in range(len(relation_details))])
            
            er_carousel_html = "\n".join([f'<div class="carousel-item {"active" if idx == 0 else ""}" style="transition: none;">{create_html_content_er_story(relation_map, relation_details, page_no=idx)}</div>' 
                                        for idx in range(len(relation_details))])

            knowledge_html = f"""
<p class="text-center">Process Story</p>
<div class="container">
    <div id="cocoon_er_story_carousel_{path_idx}" class="carousel slide" data-interval="false">
        <div class="d-flex justify-content-between align-items-center mt-3">
            <a class="btn btn-secondary" href="#cocoon_er_story_carousel_{path_idx}" role="button"
                data-slide="prev">
                Prev
            </a>
            <a class="btn btn-secondary" href="#cocoon_er_story_carousel_{path_idx}" role="button"
                data-slide="next">
                Next
            </a>
        </div>
        <ol class="carousel-indicators" style="position: relative; bottom: 30px;">
            {er_list_html}
        </ol>
        <div class="carousel-inner">
            {er_carousel_html}
        </div>
    </div>
</div>
"""     

            if isinstance(path, list) and len(path) == 1 and path[0] == "All":
                knowledge_html += f"""<p class="text-center mt-4">cocoon_er.yml <small class="text-muted">(Document the entity relationships)</small></p>
<div class="code_container mb-4 mx-4">
    <pre><code class="language-yaml">{er_yml_content}</code></pre>
</div>"""
            path_id = "_".join([p.replace(" ", "_") for p in path])
            path_name = '>'.join(path)
            knowledge_content[path_id] = knowledge_html
            path_names[path_id] = path_name

        knowledge_items = {
            'id': 'knowledge',
            'title': 'Process',
            'list_description': 'Process behind the tables',
            'nav_description': 'We illustrate the step-by-step process behind the data.',
            'content': generate_div_html('knowledge', knowledge_content),
            'nav_content': generate_choice_html('knowledge', path_names, "Select Category")
        }
        catalog_page_items.append(knowledge_items)

    side_list_html, tab_content_html = generate_side_list_and_tab_content(catalog_page_items)

    model_html = generate_full_step_page("Cocoon Catalog", "Verified by human, RAG by LLMs", project_name, side_list_html, tab_content_html, additional_css=tree_hierarchy_css)
    return model_html



class ChatHTMLGenerator:
    def __init__(self):
        self.messages = []
        self.html_widget = widgets.HTML()
        formatter = HtmlFormatter(style='monokai')
        self.css_style = formatter.get_style_defs('.highlight')

        self.html_template = '''
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <title>Chat UI</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
        }}
        .chat-body {{
            height: calc(100% - 56px);
            overflow-y: auto;
            padding: 1rem;
            background-color: rgba(242, 242, 247, 0.7);
        }}
        .message-container {{
            display: flex;
            align-items: flex-start;
            margin-bottom: 1rem;
        }}
        .message {{
            width: 900px;
            padding: 0.75rem 1rem;
            border-radius: 18px;
            line-height: 1.4;
            font-size: 0.75rem;
        }}
        .message-party1 {{
            background-color: #007aff;
            color: #ffffff;
            margin-left: auto;
        }}
        .message-party2 {{
            background-color: #ffffff;
            color: #1c1c1e;
            border: 1px solid #e5e5ea;
        }}
        .sender-name {{
            font-weight: 600;
            margin-bottom: 0.25rem;
            font-size: 0.9rem;
            color: #8e8e93;
        }}
        .main-title {{
            color: #ffffff;
            text-align: center;
            margin-bottom: 2rem;
            font-weight: 600;
        }}
        .avatar {{
            width: 40px;
            height: 40px;
            border-radius: 50%;
            margin-right: 10px;
            flex-shrink: 0;
            background-color: white;
            background-size: contain;
            background-position: center;
            background-repeat: no-repeat;
        }}
        .message-party1-container {{
            flex-direction: row-reverse;
        }}
        .message-party1-container .avatar {{
            margin-right: 0;
            margin-left: 10px;
        }}
        {css_style}
        {tag_css}
        {pandas_css}
        {better_scrollable_css}
        {copy_button_css}
        {border_style}
    </style>
</head>
<body>
    <div class="chat-body">
        {messages}
    </div>
</body>

</html>
'''

        self.update_widget()
        
    def add_message(self, content, party, sender=None, icon=""):
        party_class = f"message-party{party}"
        container_class = f"message-container message-party{party}-container" if party == 1 else "message-container"
        avatar_html = f'<div class="avatar" style="background-image: url(data:image/png;base64,{icon});"></div>' if sender else ''
        sender_html = f'<div class="sender-name">{sender}</div>' if sender else ''
        
        message_html = f'''
        <div class="{container_class}">
            {avatar_html}
            <div class="message {party_class}">
                {sender_html}
                <div>{content}</div>
            </div>
        </div>
        '''
        self.messages.append(message_html)
        self.update_widget()
    
    def clear_all_messages(self):
        self.messages = []
        self.update_widget()


    def generate_full_html(self,background_color="linear-gradient(135deg, #667eea 0%, #764ba2 100%)", title="RAG large and complex dbt project by lineage"):
        messages_html = '\n'.join(self.messages)
        html_content = f"""
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cocoon: {title}</title>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/css/bootstrap.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/bootstrap/5.3.0/js/bootstrap.bundle.min.js"></script>
    <title>Chat UI</title>
    <style>
        body {{
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen-Sans, Ubuntu, Cantarell, 'Helvetica Neue', sans-serif;
            background: {background_color};
            display: flex;
            flex-direction: column;
        }}
        .chat-body {{
            height: calc(100% - 56px);
            overflow-y: auto;
            padding: 1rem;
            background-color: rgba(242, 242, 247, 0.7);
        }}
        .message-container {{
            display: flex;
            align-items: flex-start;
            margin-bottom: 1rem;
        }}
        .message {{
            width: 700px;
            padding: 0.75rem 1rem;
            border-radius: 18px;
            line-height: 1.4;
            font-size: 0.75rem;
        }}
        .message-party1 {{
            background-color: #007aff;
            color: #ffffff;
            margin-left: auto;
        }}
        .message-party2 {{
            background-color: #ffffff;
            color: #1c1c1e;
            border: 1px solid #e5e5ea;
        }}
        .sender-name {{
            font-weight: 600;
            margin-bottom: 0.25rem;
            font-size: 0.9rem;
            color: #8e8e93;
        }}
        .main-title {{
            color: #ffffff;
            text-align: center;
            margin-bottom: 2rem;
            font-weight: 600;
        }}
        .avatar {{
            width: 40px;
            height: 40px;
            border-radius: 50%;
            margin-right: 10px;
            flex-shrink: 0;
            background-color: white;
            background-size: contain;
            background-position: center;
            background-repeat: no-repeat;
        }}
        .message-party1-container {{
            flex-direction: row-reverse;
        }}
        .message-party1-container .avatar {{
            margin-right: 0;
            margin-left: 10px;
        }}
        {self.css_style}
        {tag_css}
        {pandas_css}
        {better_scrollable_css}
        {copy_button_css}
        {border_style}
        {float_notification_css}
    </style>
</head>
<body>
    <div class="container-fluid mt-5">
        <h1 class="text-center text-white">
            <a href="https://github.com/Cocoon-Data-Transformation/cocoon" style="text-decoration: none; color: inherit;">
                <span class="fw-bold">Cocoon:</span> {title}
            </a>
        </h1>
    </div>
    
    <div class="container-fluid mt-5 mb-5">
        <div class="col-12 col-md-10 col-lg-8 mx-auto">
            <div class="chat-body p-4 rounded">
                {messages_html}
            </div>
        </div>
    </div>
    
    <div class="floating-notification">
        <span class="notification-icon rotate">⟳</span>
        New chat results may be available. Refresh to check.
    </div>

</body>
</html>"""

        return html_content
        
    def update_widget(self):
        messages_html = '\n'.join(self.messages)
        self.html_widget.value = self.html_template.format(messages=messages_html,
                                                            css_style=self.css_style,
                                                         tag_css=tag_css,
                                                         pandas_css=pandas_css,
                                                         better_scrollable_css=better_scrollable_css,
                                                         copy_button_css=copy_button_css,
                                                         border_style=border_style)
    
    def display(self):
        display(self.html_widget)




def replace_sql_with_highlighted(content, dbt_lineage=None):
    
    content = re.sub(r'\n*```sql', '```sql', content)
    content = re.sub(r'```\n*', '```', content)

    def replace_sql(match):
        sql_query = match.group(1).strip()
        sql_path = ""
        if dbt_lineage is not None:
            model_name_match = re.match(r"^-- model_name: '([^']+)'", sql_query)
            if model_name_match:
                model_name = model_name_match.group(1)
                sql_query = re.sub(r"^-- model_name: '[^']+'\s*", "", sql_query).strip()
                file_path = dbt_lineage.get_full_file_path(model_name)
                if file_path:
                    sql_path = f'<br><a href="{file_path}" target="_blank">{model_name}</a><br>'
                else:
                    sql_path = f'<br>{model_name}<br>'
        
        highlighted_sql = highlight_sql(sql_query, add_copy_button=True, add_css=False)
        return f"{sql_path}{highlighted_sql}"

    placeholder = "SQL_BLOCK_PLACEHOLDER"
    sql_blocks = []

    def capture_sql(match):
        sql_blocks.append(match.group(0))
        return placeholder

    content_with_placeholders = re.sub(r'```sql(.*?)```', capture_sql, content, flags=re.DOTALL)

    content_with_br = content_with_placeholders.replace('\n', '<br>')

    def replace_placeholder(match):
        return replace_sql(re.match(r'```sql(.*?)```', sql_blocks.pop(0), flags=re.DOTALL))

    final_content = re.sub(placeholder, replace_placeholder, content_with_br)

    final_content = re.sub(r'<br>```sql', '```sql', final_content)
    final_content = re.sub(r'```<br>', '```', final_content)

    if dbt_lineage is not None:
        def replace_model_name(match):
            model_name = match.group(1)
            file_path = dbt_lineage.get_full_file_path(model_name)
            if file_path:
                return f'<a href="{file_path}" target="_blank">{model_name}</a>'
            return f'`{model_name}`'

        final_content = re.sub(r'`([^`]+)`', replace_model_name, final_content)
    return final_content






class DBTLLMAgent:
    def __init__(self, dbt_lineage, debug=True, chat_ui=None):
        self.dbt_lineage = dbt_lineage
        self.conversation_history = []
        self.step_count = 0
        self.debug = debug
        self.chat_ui = chat_ui
        self.explored_model_ids = set()

    def generate_initial_prompt(self, user_question):
        model_context = self.dbt_lineage.generate_text_lineage()
        prompt = f"""# DBT Project models (higher index models are downstream):
{model_context}

## General Principles
1. Prioritize SQL-based answers using existing models.
2. Be evidence-based, referencing `model_name` and ```sql codes``` frequently
3. Explore upstream models for data sufficiency, prefer downstream models for final answers.
4. Keep responses concise.
5. Persist in exploration if data seems insufficient.

## Syntax Guidelines
- Reference models: `model_name`
- SQL code blocks:
  ```sql
  SELECT ...
  ```
- New model creation:
  ```sql
  -- model_name: 'new_model_name'
  SELECT ...
  ```

## Editing Existing Models
- Highlight changes only:
  ```sql
  -- model_name: 'existing_model_name'
  -- ... existing code ...
  {{ edit_1 }}
  -- ... existing code ...
  {{ edit_2 }}
  -- ... existing code ...
  ```
- Skip unchanged parts (start/end of file).
- Rewrite entire file only if explicitly requested.
- Consider downstream effects when editing.
- Propose edits for affected downstream models.

## Your actions (one at a time):
1. Explore the dbt model to answer the user's question. 
```yml
reason: >-
    The most relavant models are ...
action: explore
model_ids: [<list of up to 10 model ids to explore; preferably include their upstream models for better context>]
```
2. Answer the question based on the current context.
```yml
reason: >-
    I'm going to build new model/edit existing models and their downstream models. I have explored all the relavant models/ I still need to explore ...[action becomes explore]
action: conclude
answer: |
    <reference `model_name` and ```sql codes```>
```

Now answer the question: "{user_question}"
Your action:"""
        self.print_message("user", prompt)
        return prompt
    
    def generate_follow_up(self, user_question):
        return f'User follow-up question: "{user_question}"\nYour action:'

    def process_user_question(self, user_question):
        if not self.conversation_history:
            initial_prompt = self.generate_initial_prompt(user_question)
            self.conversation_history = [{"role": "user", "content": initial_prompt}]
            
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
                self.chat_ui.add_message("<b>Here is the model lineage for the dbt project. Let me know which models you want to explore.</b><br><br>" + self.dbt_lineage.display_model_lineage_html(), 2, "Cocoon RAG", icon=cocoon_icon_64)
        else:
            follow_up_prompt = self.generate_follow_up(user_question)
            self.conversation_history.append({"role": "user", "content": follow_up_prompt})
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
        

        max_rounds = 10
        for _ in range(max_rounds):
            for attempt in range(5): 
                
                try:
                    response = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1)
                    llm_message = response['choices'][0]['message']
                    yaml_content = extract_yml_code(llm_message["content"])
                    
                    decision = yaml.safe_load(yaml_content)
                    if decision['action'] in ['explore', 'conclude']:
                        self.conversation_history.append(llm_message)
                        self.print_message(llm_message["role"], llm_message["content"])
                        
                        if decision['action'] == 'explore':
                            self.explore_dbt_models(decision.get('reason', 'No reason provided'), decision.get('model_ids', []))
                        else:
                            self.conclude(decision)
                            return decision
                        
                        break
                except:
                    if cocoon_main_setting['DEBUG_MODE']:
                        raise
                    
                    if attempt == 4:
                        decision = {
                            "action": "conclude",
                            "answer": "There seems to be some issue processing your request. Could you please rephrase your question or provide more details?"
                        }
                        self.conclude(decision)
                        return decision
                    
                    time.sleep(5*attempt)

                    continue
        
        return {"action": "conclude", "answer": "Max rounds reached without a conclusion. Please try rephrasing your question."}

    def explore_dbt_models(self, reason, model_ids):
        exploration_results = []
        newly_explored_models = []
        for model_id in model_ids:
            if model_id in self.explored_model_ids:
                exploration_results.append(f"Model ID {model_id}:\nAlready explored before")
            else:
                model_summary_dict = self.dbt_lineage.generate_model_summary_text_dict(model_id=model_id)
                model_summary_text = "\n".join([f"{key}\n{model_summary_dict[key]}" for key in model_summary_dict])
                exploration_results.append(f"Model ID {model_id}:\n{model_summary_text}")
                self.explored_model_ids.add(model_id)
                newly_explored_models.append(model_id)
        
        if self.debug:
            self.dbt_lineage.display_model_lineage_by_indices(newly_explored_models)

        combined_results = "\n\n".join(exploration_results)
        follow_up_prompt = f"""Exploration results:
{combined_results}

Your next action (Explore or Conclude):"""
        self.print_message("user", follow_up_prompt)
        self.conversation_history.append({"role": "user", "content": follow_up_prompt})

        if self.chat_ui:
            self.chat_ui.add_message(f"<b>Thinking:</b> {reason}.<br><b>Exploring:</b> I want to explore these models in detail:<br><br>{self.dbt_lineage.get_model_lineage_html_by_indices(model_indices=model_ids)}", 2, "LLM Agent", icon=claude_icon_64)
            self.chat_ui.add_message(f"<b>Here are the details for the specific models:</b><br><br>{wrap_in_iframe(self.dbt_lineage.generate_multiple_model_summaries(model_indices=model_ids), width='100%')}", 2, "Cocoon RAG", icon=cocoon_icon_64)
    
    def conclude(self, decision):
        if self.chat_ui:
            content = decision.get('answer', '')
            self.chat_ui.add_message(replace_sql_with_highlighted(content, self.dbt_lineage), 2, "LLM Agent", icon=claude_icon_64)

    def print_message(self, role, content):
        if self.debug:
            self.step_count += 1
            print(f"\n--- Step {self.step_count}: {role.capitalize()} Message ---\n")
            print(content)
            print("\n--- End of Message ---\n")




        
class InferSQLOutputColumnsForAll(MultipleNode):
    default_name = 'Infer SQL Output Columns For All'
    default_description = 'This node infers output columns for all models with missing column information.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["model_name"] = element_name
        para["idx"] = idx
        para["total"] = total
        
        node = InferSQLOutputColumns(para=para, id_para="model_name")
        node.inherit(self)
        return node

    def extract(self, item):
        dbt_lineage = self.para['dbt_lineage']
        
        sql_query = """
        SELECT DISTINCT m.model_name
        FROM model m
        LEFT JOIN model_column mc ON m.model_name = mc.model_name
        JOIN ordered_models om ON m.model_name = om.model_name
        WHERE mc.model_name IS NULL AND m.raw_code IS NOT NULL
        ORDER BY om.new_rowid ASC
        """

        df = dbt_lineage.conn.execute(sql_query).df()
        
        self.elements = df['model_name'].tolist()
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}


class InferSQLOutputColumns(Node):
    default_name = 'Infer SQL Output Columns'
    default_description = 'This node infers the output columns of a SQL query.'

    def extract(self, item):
        clear_output(wait=True)

        dbt_lineage = self.para['dbt_lineage']
        model_name = self.para['model_name']
        
        sql_query = """
        SELECT raw_code
        FROM model
        WHERE model_name = ?
        """
        df = dbt_lineage.conn.execute(sql_query, [model_name]).df()
        
        if len(df) == 0:
            return None
        
        raw_sql = df['raw_code'][0]

        upstream_query = """
        SELECT source_model_name
        FROM model_lineage
        WHERE target_model_name = ?
        """
        upstream_models = dbt_lineage.conn.execute(upstream_query, [model_name]).df()['source_model_name'].tolist()

        upstream_columns = {}
        for upstream_model in upstream_models:
            column_query = """
            SELECT column_name, description
            FROM model_column
            WHERE model_name = ?
            """
            columns = dbt_lineage.conn.execute(column_query, [upstream_model]).df()
            upstream_columns[upstream_model] = columns.to_dict('records')

        
        downstream_query = """
        SELECT target_model_name
        FROM model_lineage
        WHERE source_model_name = ?
        """
        downstream_models = dbt_lineage.conn.execute(downstream_query, [model_name]).df()['target_model_name'].tolist()

        downstream_code = {}
        for downstream_model in downstream_models:
            code_query = """
            SELECT raw_code
            FROM model
            WHERE model_name = ?
            """
            code = dbt_lineage.conn.execute(code_query, [downstream_model]).df()
            if not code.empty:
                downstream_code[downstream_model] = code['raw_code'][0]

        return {
            'raw_sql': raw_sql,
            'upstream_columns': upstream_columns,
            'downstream_code': downstream_code
        }
    
    def run(self, extract_output, use_cache=True):
        if extract_output is None:
            return self.run_but_fail(extract_output)
        
        raw_sql = extract_output['raw_sql']
        upstream_columns = extract_output['upstream_columns']
        downstream_code = extract_output['downstream_code']
        
        prompt = f"""You have the following model:
{self.para['model_name']}
with the following SQL query:
{raw_sql}
"""
        if len(upstream_columns) > 0:
            prompt += "\nUpstream Model Columns:\n"
            for model, columns in upstream_columns.items():
                prompt += f"\n{model}:\n"
            for column in columns:
                prompt += f"- {column['column_name']}: {column['description']}\n"

        if len(downstream_code) > 0:
            prompt += "\nDownstream Model Code (Check what columns are used in the downstream model):\n"
            for model, code in downstream_code.items():
                prompt += f"\n{model}:\n```sql\n{code}\n```\n"


        prompt += f"""
Please provide the inferred columns for the modle {self.para['model_name']} in the following format:
```yml
columns:
- name: column_name
description: Brief description of the column
- ...
```"""

        messages = [{"role": "user", "content": prompt}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yaml_content = extract_yml_code(response['choices'][0]['message']["content"])
        inferred_columns = yaml.safe_load(yaml_content)['columns']


        return {
            "model_name": self.para['model_name'],
            "inferred_columns": inferred_columns
        }
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {
            "model_name": self.para['model_name'],
            "inferred_columns": []
        }

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        dbt_lineage = self.para['dbt_lineage']
        model_name = run_output['model_name']
        inferred_columns = run_output['inferred_columns']

        next_button = widgets.Button(
            description='Next',
            button_style='success',
            tooltip='Proceed to the next step'
        )
        
        def on_next_button_click(b):
            with self.output_context():
                for column in inferred_columns:
                    dbt_lineage.conn.execute("""
                        INSERT INTO model_column (model_name, column_name, description, cocoon_description)
                        VALUES (?, ?, NULL, ?)
                        ON CONFLICT (model_name, column_name) DO UPDATE SET
                        description = COALESCE(model_column.description, NULL),
                        cocoon_description = EXCLUDED.cocoon_description
                    """, [model_name, column['name'], column['description']])

                callback(run_output)
        
        next_button.on_click(on_next_button_click)

        if viewer:
            on_next_button_click(next_button)

        print(f"Model: {model_name}")
        print("\nRaw SQL:")
        print(extract_output['raw_sql'])
        
        print("\nUpstream Model Columns:")
        for upstream_model, columns in extract_output['upstream_columns'].items():
            print(f"\n{upstream_model}:")
            for column in columns:
                print(f"- {column['column_name']}: {column['description']}")
        
        print("\nInferred columns:")
        for column in inferred_columns:
            print(f"- {column['name']}: {column['description']}")
                
        display(next_button)

def get_tag_style(tag):
    tag_styles = {
        "Direct": {"bg": "#000000", "color": "#FFFFFF"},
        "Filtering": {"bg": "#BBDEFB", "color": "#1565C0"},
        "Cleaning": {"bg": "#C8E6C9", "color": "#2E7D32"},
        "Deduplication": {"bg": "#FFF9C4", "color": "#F9A825"},
        "Featurization": {"bg": "#E1BEE7", "color": "#6A1B9A"},
        "Integration": {"bg": "#C5CAE9", "color": "#283593"},
        "Aggregation": {"bg": "#F8BBD0", "color": "#AD1457"},
        "Other": {"bg": "#F5F5F5", "color": "#616161"}
    }

    return next((style for key, style in tag_styles.items() if key.lower() == tag.lower()), tag_styles["Other"])

def generate_tag_html(tag, explanation):
    style = get_tag_style(tag)
    return f'<span class="tag" style="background-color: {style["bg"]}; color: {style["color"]}; border: 1px solid {style["color"]};">{tag}</span> {explanation}'


class DBLLMAgent:
    def __init__(self, data_project, debug=True, chat_ui=None):
        self.data_project = data_project
        self.conversation_history = []
        self.step_count = 0
        self.debug = debug
        self.chat_ui = chat_ui
        self.explored_table_ids = set()

    def generate_initial_prompt(self, user_question):
        table_context = self.data_project.generate_text_join()
        prompt = f"""# Database table join path (arrow links foreign keys <- primary key):
{table_context}

## General Principles
1. Prioritize SQL-based answers using existing tables.
2. Be evidence-based, referencing `table_name` and ```sql codes``` frequently
3. Explore related joinable tables for context
4. Keep responses concise.
5. Persist in exploration if data seems insufficient.

## Syntax Guidelines
- Reference tables: `table_name`
- SQL code blocks:
  ```sql
  SELECT ...
  ```
- New table creation:
  ```sql
  -- table_name: 'new_table_name'
  SELECT ...
  ```

## Your actions (one at a time):
1. Explore the tables to answer the user's question. 
```yml
reason: >-
    The most relavant tables are ...
action: explore
table_ids: [<list of up to 10 table ids to explore; preferably include joinable tables for better context>]
```
2. Answer the question based on the current context.
```yml
reason: >-
    I'm going to write SQL code to answer the user's question. I have explored all the relavant tables/ I still need to explore ...[action becomes explore]
action: conclude
answer: |
    <reference `table_name` and ```sql codes```>
```

Now answer the question: "{user_question}"
Your action:"""
        self.print_message("user", prompt)
        return prompt
    
    def generate_follow_up(self, user_question):
        return f'User follow-up question: "{user_question}"\nYour action:'

    def process_user_question(self, user_question):
        if not self.conversation_history:
            initial_prompt = self.generate_initial_prompt(user_question)
            self.conversation_history = [{"role": "user", "content": initial_prompt}]
            
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
                self.chat_ui.add_message("<b>Here is the database. Let me know which tables you want to explore.</b><br><br>" + self.data_project.generate_html_graph_static(), 2, "Cocoon RAG", icon=cocoon_icon_64)
        else:
            follow_up_prompt = self.generate_follow_up(user_question)
            self.conversation_history.append({"role": "user", "content": follow_up_prompt})
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
        

        max_rounds = 10
        for _ in range(max_rounds):
            for attempt in range(5): 
                
                try:
                    response = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1)
                    llm_message = response['choices'][0]['message']
                    yaml_content = extract_yml_code(llm_message["content"])
                    
                    decision = yaml.safe_load(yaml_content)
                    if decision['action'] in ['explore', 'conclude']:
                        self.conversation_history.append(llm_message)
                        self.print_message(llm_message["role"], llm_message["content"])
                        
                        if decision['action'] == 'explore':
                            self.explore_tables(decision.get('reason', 'No reason provided'), decision.get('table_ids', []))
                        else:
                            self.conclude(decision)
                            return decision
                        
                        break
                except:
                    
                    
                    if attempt == 4:
                        decision = {
                            "action": "conclude",
                            "answer": "There seems to be some issue processing your request. Could you please rephrase your question or provide more details?"
                        }
                        self.conclude(decision)
                        return decision
                    
                    time.sleep(5*attempt)

                    continue
        
        return {"action": "conclude", "answer": "Max rounds reached without a conclusion. Please try rephrasing your question."}

    def explore_tables(self, reason, table_ids):
        exploration_results = []
        newly_explored_tables = []
        for table_id in table_ids:
            if table_id in self.explored_table_ids:
                exploration_results.append(f"table ID {table_id}:\nAlready explored before")
            else:
                table_summary_text = self.data_project.describe_table_in_text(table_input=table_id)
                exploration_results.append(f"table ID {table_id}:\n{table_summary_text}")
                self.explored_table_ids.add(table_id)
                newly_explored_tables.append(table_id)

        combined_results = "\n\n".join(exploration_results)
        follow_up_prompt = f"""Exploration results:
{combined_results}

Your next action (Explore or Conclude):"""
        self.print_message("user", follow_up_prompt)
        self.conversation_history.append({"role": "user", "content": follow_up_prompt})

        if self.chat_ui:
            self.chat_ui.add_message(f"{reason}. Therefore, I want to explore these tables in detail:<br><br>{self.data_project.generate_html_graph_static(selected_nodes=table_ids)}", 2, "LLM Agent", icon=claude_icon_64)
            self.chat_ui.add_message(f"<b>Here are the details for the specific tables:</b><br><br>{wrap_in_iframe(self.data_project.generate_multiple_table_summaries(model_indices=table_ids), width='100%')}", 2, "Cocoon RAG", icon=cocoon_icon_64)
    
    def conclude(self, decision):
        if self.chat_ui:
            content = decision.get('answer', '')
            self.chat_ui.add_message(replace_sql_with_highlighted(content), 2, "LLM Agent", icon=claude_icon_64)

    def print_message(self, role, content):
        if self.debug:
            self.step_count += 1
            print(f"\n--- Step {self.step_count}: {role.capitalize()} Message ---\n")
            print(content)
            print("\n--- End of Message ---\n")

class DBTCatalogConfig(Node):
    default_name = 'DBT Catalog Configuration'
    default_description = 'This step allows you to configure the DBT catalog for exploration.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        catalog_file_input = widgets.Text(
            value=self.para.get("dbt_directory", ""),
            layout=widgets.Layout(width='70%')
        )

        next_button = widgets.Button(description="Load", button_style='success', icon='book', layout=widgets.Layout(width='10%'))

        def on_button_click(b):
            with self.output_context():
                dbt_directory = catalog_file_input.value

                if not file_exists(dbt_directory):
                    print(f"Error: The directory '{dbt_directory}' does not exist.")
                    return
                
                self.para["dbt_directory"] = dbt_directory

                callback({
                    "dbt_directory": dbt_directory,
                })

        next_button.on_click(on_button_click)

        if viewer or ("viewer" in self.para and self.para["viewer"]):
            if "dbt_directory" in self.para:
                on_button_click(next_button)
                return
        
        display(HTML("<div style='line-height: 1.2;'><h2>📚 Provide your DBT Catalog File</h2><em>Enter the path to your DBT catalog JSON file. Prefer full path over relative path.</em></div>"))
        display(widgets.HBox([catalog_file_input, next_button]))


class DBTCatalogBuilder(Node):
    default_name = 'DBT Catalog Builder'
    default_description = 'This step reads the DBT catalog and builds the data project.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        dbt_directory = self.para.get("dbt_directory")
        if not dbt_directory:
            display(HTML("<div style='color: red;'>Error: DBT directory not found in parameters.</div>"))
            return

        data_project = read_data_project_from_dir(dbt_directory)

        self.para['dbt_lineage'] = data_project

        display(HTML("<div style='line-height: 1.2;'><h2>🎉 Your DBT catalog is ready for exploration</h2><em>We've loaded the data project from your DBT directory.</em></div>"))

        html_content = data_project.generate_multiple_table_summaries(include_join_graph=True)
        display(HTML(wrap_in_iframe(html_content)))

        text_input = widgets.Text(
            placeholder='Ask any question about the DBT project...',
            layout=widgets.Layout(width='70%')
        )

        send_button = widgets.Button(
            description='Send',
            button_style='primary',
            layout=widgets.Layout(width='10%')
        )

        def on_send(b):
            with self.output_context():
                message = text_input.value
                callback({"question": message})
                return

        send_button.on_click(on_send)

        chat_input = widgets.HBox([text_input, send_button], layout=widgets.Layout(width='100%', margin='10px 0px'))

        display(chat_input)

class ProcessUserQuestionCatalog(Node):
    default_name = 'Process User Question Catalog'
    default_description = 'This step processes the user question from the DBT Lineage Builder.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        question = self.get_sibling_document('DBT Catalog Builder').get("question", "No question found")
        dbt_lineage = self.para['dbt_lineage']

        chat = ChatHTMLGenerator()
        chat.display()

        agent = DBLLMAgent(dbt_lineage, chat_ui=chat, debug=False)

        def serve_user_question(question):
            agent.process_user_question(question)

        follow_up_button = widgets.Button(
            description='Follow Up',
            icon='arrow-right',
            button_style='primary',
            layout=widgets.Layout(width='15%'),
            tooltip='Ask a follow-up question'
        )

        new_question_input = widgets.Text(
            placeholder='Enter your question here...',
            layout=widgets.Layout(width='70%')
        )

        new_button = widgets.Button(
            description='New',
            icon='plus',
            button_style='warning',
            layout=widgets.Layout(width='15%'),
            tooltip='Start a new conversation'
        )

        warning_widget = widgets.HTML(
            value='',
            layout=widgets.Layout(width='100%', margin='10px 0px')
        )

        def on_follow_up(b):
            new_question = new_question_input.value
            if new_question:
                serve_user_question(new_question)
                new_question_input.value = ''
            update_warning()

        def on_new(b):
            chat.clear_all_messages()
            nonlocal agent
            agent = DBTLLMAgent(dbt_lineage, chat_ui=chat, debug=False)
            warning_widget.value = ''
            new_question = new_question_input.value
            if new_question:
                serve_user_question(new_question)
                new_question_input.value = ''

        follow_up_button.on_click(on_follow_up)
        new_button.on_click(on_new)

        def update_warning():
            if len(agent.conversation_history) > 10:
                warning_widget.value = '<div style="color: red">Warning: Conversation is getting long. Consider starting a new conversation for better performance.</div>'
            else:
                warning_widget.value = ''

        new_question_box = widgets.HBox([new_question_input, follow_up_button, new_button], 
                                        layout=widgets.Layout(width='100%', margin='20px 0px'))

        display(widgets.VBox([new_question_box, warning_widget]))

        serve_user_question(question)

    



def create_cocoon_mul_catalog_explore_workflow(con=None, output=None, para={}, viewer=False):
    
    main_workflow = Workflow("Multi-Catalog Explore Workflow", 
                        item = {},
                        description="A workflow to explore a DBT catalog",
                        output=output,
                        para=para)

    main_workflow.add_to_leaf(MultipleCatalogConfig(output=output))
    main_workflow.add_to_leaf(MultipleCatalogBuilder(output=output))
    main_workflow.add_to_leaf(ProcessUserQuestionMultipleCatalogs(output=output)) 

    return main_workflow

class DataCatalogLLMAgent:
    def __init__(self, data_catalog, debug=True, chat_ui=None):
        self.data_catalog = data_catalog
        self.conversation_history = []
        self.step_count = 0
        self.debug = debug
        self.chat_ui = chat_ui
        self.explored_item_ids = set()

    def get_name(self):
        return "DataCatalogLLMAgent"

    def generate_initial_prompt(self, user_question):
        catalog_context = self.data_catalog.generate_text_summary()
        prompt = f"""{catalog_context}

## General Principles
1. Prioritize answers based on existing catalog items.
2. Be evidence-based, referencing `item_name` and ```sql codes``` frequently.
3. Explore related items for context.
4. Keep responses concise.
5. Persist in exploration if information seems insufficient.

## Syntax Guidelines
- Reference items: `item_name`
- SQL code blocks:
  ```sql
  SELECT ...
  ```
- New table creation:
  ```sql
  -- table_name: 'item_name'
  SELECT ...
  ```

## Your actions (one at a time):
1. Explore the catalog to answer the user's question. 
```yml
reason: >-
    The most relevant items are ...
action: explore
item_ids: [<list of up to 10 item ids to explore; preferably include related items for better context>]
```
2. Answer the question based on the current context.
```yml
reason: >-
    I'm going to answer the user's question. I have explored all the relevant items / I still need to explore ...[action becomes explore]
action: conclude
answer: |
    <reference `item_name` and ```sql codes```>
```

Now answer the question: "{user_question}"
Your action:"""
        self.print_message("user", prompt)
        return prompt
    
    def generate_follow_up(self, user_question):
        return f'User follow-up question: "{user_question}"\nYour action:'

    def process_user_question(self, user_question):
        if not self.conversation_history:
            initial_prompt = self.generate_initial_prompt(user_question)
            self.conversation_history = [{"role": "user", "content": initial_prompt}]
            
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
                self.chat_ui.add_message("<b>Here is the data catalog summary. Let me know which items you want to explore.</b><br><br>" + self.data_catalog.generate_html_summary(), 2, "Cocoon RAG", icon=cocoon_icon_64)
        else:
            follow_up_prompt = self.generate_follow_up(user_question)
            self.conversation_history.append({"role": "user", "content": follow_up_prompt})
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
        
        max_rounds = 10
        for _ in range(max_rounds):
            for attempt in range(5):
                try:
                    response = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1)
                    llm_message = response['choices'][0]['message']
                    yaml_content = extract_yml_code(llm_message["content"])
                    
                    decision = yaml.safe_load(yaml_content)
                    if decision['action'] in ['explore', 'conclude']:
                        self.conversation_history.append(llm_message)
                        self.print_message(llm_message["role"], llm_message["content"])
                        
                        if decision['action'] == 'explore':
                            self.explore_items(decision.get('reason', 'No reason provided'), decision.get('item_ids', []))
                        else:
                            self.conclude(decision)
                            return decision
                        
                        break
                except:
                    if attempt == 4:
                        decision = {
                            "action": "conclude",
                            "answer": "There seems to be some issue processing your request. Could you please rephrase your question or provide more details?"
                        }
                        self.conclude(decision)
                        return decision
                    
                    time.sleep(5*attempt)
                    continue
        
        return {"action": "conclude", "answer": "Max rounds reached without a conclusion. Please try rephrasing your question."}

    def explore_items(self, reason, item_ids):
        exploration_results = []
        newly_explored_items = []
        for item_id in item_ids:
            if item_id in self.explored_item_ids:
                exploration_results.append(f"Item ID {item_id}:\nAlready explored before")
            else:
                item_summary_text = self.data_catalog.generate_item_text_summary(item_id=item_id)
                exploration_results.append(f"Item ID {item_id}:\n{item_summary_text}")
                self.explored_item_ids.add(item_id)
                newly_explored_items.append(item_id)

        combined_results = "\n\n".join(exploration_results)
        follow_up_prompt = f"""Exploration results:
{combined_results}

Your next action (Explore or Conclude):"""
        self.print_message("user", follow_up_prompt)
        self.conversation_history.append({"role": "user", "content": follow_up_prompt})

        if self.chat_ui:
            self.chat_ui.add_message(f"<b>Thinking:</b> {reason}.<br><b>Exploring:</b> I want to explore these items in detail:<br><br>{self.data_catalog.generate_html_summary(item_ids=item_ids)}", 2, "LLM Agent", icon=claude_icon_64)
            self.chat_ui.add_message(f"<b>Here are the details for the specific items:</b><br><br>{wrap_in_iframe(self.data_catalog.generate_items_summary(item_ids=item_ids), width='100%')}", 2, "Cocoon RAG", icon=cocoon_icon_64)
    
    def conclude(self, decision):
        if self.chat_ui:
            content = decision.get('answer', '')
            self.chat_ui.add_message(replace_sql_with_highlighted(content), 2, "LLM Agent", icon=claude_icon_64)

    def print_message(self, role, content):
        if self.debug:
            self.step_count += 1
            print(f"\n--- Step {self.step_count}: {role.capitalize()} Message ---\n")
            print(content)
            print("\n--- End of Message ---\n")
            
            
class MultiDataCatalogLLMAgent:
    def __init__(self, data_catalogs, debug=True, chat_ui=None):
        self.data_catalogs = data_catalogs
        self.conversation_history = []
        self.step_count = 0
        self.debug = debug
        self.chat_ui = chat_ui
        self.explored_item_ids = set()

    def get_name(self):
        return "DataCatalogLLMAgent"

    def generate_initial_prompt(self, user_question):
        catalog_contexts = []
        for idx, catalog in enumerate(self.data_catalogs, 1):
            catalog_context = catalog.generate_text_summary(idx=idx)
            catalog_contexts.append(f"Catalog {idx}: '{catalog.get_name()}'\n{catalog_context}")
        
        all_catalog_contexts = "\n\n".join(catalog_contexts)
        
        prompt = f"""{all_catalog_contexts}

## General Principles
1. Prioritize SQL-based answers using existing items in the catalogs.
2. Be evidence-based, referencing `item_name` and ```sql codes``` frequently.
3. Explore related items for context.
4. Keep responses concise.
5. Persist in exploration if information seems insufficient.

## Syntax Guidelines
- Reference items: `item_name`
- SQL code blocks:
  ```sql
  SELECT ...
  ```

## Editing Existing Pipeline Models
- Highlight changes only:
  ```sql
  -- model_name: 'existing_model_name'
  -- ... existing code ...
  {{ edit_1 }}
  -- ... existing code ...
  {{ edit_2 }}
  -- ... existing code ...
  ```
- Skip unchanged parts (start/end of file).
- Rewrite entire file only if explicitly requested.
- Consider downstream effects when editing.
- Propose edits for affected downstream models.

## Your actions (one at a time):
1. Explore the catalogs to answer the user's question. 
```yml
reason: >-
    The most relevant items are ...
action: explore
items:
  - catalog_id: 1
    item_ids: [<list of up to 10 item ids to explore; preferably include related items for better context>]
  - catalog_id: 2
    item_ids: [<list of up to 10 item ids to explore; preferably include related items for better context>]
```
2. Answer the question based on the current context.
```yml
reason: >-
    I'm going to answer the user's question. I have explored all the relevant items / I still need to explore ...[action becomes explore]
action: conclude
answer: |
    <reference `item_name` and ```sql codes```>
```

Now answer the question: "{user_question}"
Your action:"""
        self.print_message("user", prompt)
        return prompt
    
    def generate_follow_up(self, user_question):
        return f'User follow-up question: "{user_question}"\nYour action:'

    def process_user_question(self, user_question):
        if not self.conversation_history:
            initial_prompt = self.generate_initial_prompt(user_question)
            self.conversation_history = [{"role": "user", "content": initial_prompt}]
            
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
                summaries = []
                for idx, catalog in enumerate(self.data_catalogs, 1):
                    summaries.append(f"<b>Catalog {idx}: {catalog.get_name()}</b><br><br>{catalog.generate_html_summary()}")
                all_summaries = "<br>".join(summaries)
                self.chat_ui.add_message(f"Here are the data catalog summaries. Let me know which items you want to explore.<br><br>{all_summaries}", 2, "Cocoon RAG", icon=cocoon_icon_64)
        else:
            follow_up_prompt = self.generate_follow_up(user_question)
            self.conversation_history.append({"role": "user", "content": follow_up_prompt})
            if self.chat_ui:
                self.chat_ui.add_message(f"{escape_html(user_question)}", 1)
        
        max_rounds = 10
        for _ in range(max_rounds):
            for attempt in range(5):
                try:
                    response = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1)
                    llm_message = response['choices'][0]['message']
                    yaml_content = extract_yml_code(llm_message["content"])
                    
                    decision = yaml.safe_load(yaml_content)
                    if decision['action'] in ['explore', 'conclude']:
                        self.conversation_history.append(llm_message)
                        self.print_message(llm_message["role"], llm_message["content"])
                        
                        if decision['action'] == 'explore':
                            self.explore_items(decision.get('reason', 'No reason provided'), decision.get('items', []))
                        else:
                            self.conclude(decision)
                            return decision
                        
                        break
                except:
                    if cocoon_main_setting['DEBUG_MODE']:
                        raise
                    
                    if attempt == 4:
                        decision = {
                            "action": "conclude",
                            "answer": "There seems to be some issue processing your request. Could you make sure the cocoon_llm_setting is set up correctly (prefer claude sonnet 3.5), or rephrase your question?"
                        }
                        self.conclude(decision)
                        return decision
                    
                    time.sleep(5*attempt)
                    continue
        
        return {"action": "conclude", "answer": "Max rounds reached without a conclusion. Please try rephrasing your question."}

    def explore_items(self, reason, items):
        exploration_results = []
        newly_explored_items = []
        for item in items:
            catalog_id = item['catalog_id']
            catalog = self.data_catalogs[catalog_id - 1]
            catalog_results = []
            for item_id in item['item_ids']:
                item_name = catalog.get_item_name_by_id(item_id)
                full_item_id = f"{catalog_id}.{item_id}"
                if full_item_id in self.explored_item_ids:
                    catalog_results.append(f"{item_id}. {item_name}:\nAlready explored before")
                else:
                    item_summary_text = catalog.generate_item_text_summary(item_id=item_id)
                    catalog_results.append(f"{item_id}. {item_name}\n{item_summary_text}")
                    self.explored_item_ids.add(full_item_id)
                    newly_explored_items.append(full_item_id)
            
            if catalog_results:
                catalog_name = catalog.get_name()
                catalog_results_text = "\n\n".join(catalog_results)
                exploration_results.append(f"# Catalog {catalog_id}: {catalog_name}\n\n{catalog_results_text}")

        combined_results = "\n\n".join(exploration_results)
        follow_up_prompt = f"""Exploration results:
{combined_results}

Your next action (Explore or Conclude):"""
        self.print_message("user", follow_up_prompt)
        self.conversation_history.append({"role": "user", "content": follow_up_prompt})

        if self.chat_ui:
            explored_items_html = []
            for item in items:
                catalog_id = item['catalog_id']
                catalog = self.data_catalogs[catalog_id - 1]
                explored_items_html.append(f"<b>Catalog {catalog_id}: {catalog.get_name()}</b><br><br>{catalog.generate_html_summary(item_ids=item['item_ids'])}")
            
            all_explored_items_html = "<br>".join(explored_items_html)
            self.chat_ui.add_message(f"{reason} Therefore, I want to explore these in detail:<br><br>{all_explored_items_html}", 2, "LLM Agent", icon=claude_icon_64)
            
            details_html = []
            for item in items:
                catalog_id = item['catalog_id']
                catalog = self.data_catalogs[catalog_id - 1]
                details_html.append(f"<b>Catalog {catalog_id}: {catalog.get_name()}</b><br><br>{wrap_in_iframe(catalog.generate_items_summary(item_ids=item['item_ids']), width='100%')}<br>")
            
            all_details_html = "<br>".join(details_html)
            self.chat_ui.add_message(f"Here are the details for the specific items:<br><br>{all_details_html}", 2, "Cocoon RAG", icon=cocoon_icon_64)

    def conclude(self, decision):
        if self.chat_ui:
            content = decision.get('answer', '')
            self.chat_ui.add_message(replace_sql_with_highlighted(content), 2, "LLM Agent", icon=claude_icon_64)

    def print_message(self, role, content):
        if self.debug:
            self.step_count += 1
            print(f"\n--- Step {self.step_count}: {role.capitalize()} Message ---\n")
            print(content)
            print("\n--- End of Message ---\n")


class MultipleCatalogConfig(Node):
    default_name = 'Multiple Catalog Configuration'
    default_description = 'This step allows you to configure multiple catalogs for exploration.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)

        self.catalogs = {}
        self.html_widget = widgets.HTML()

        self.input_box = widgets.Text(
            value=self.para.get("dbt_directory", ""),
            placeholder="Enter your catalog directory",
            layout=widgets.Layout(width='70%')
        )
        self.add_button = widgets.Button(icon="plus", button_style="success", 
                                         layout=widgets.Layout(width="40px"), tooltip="Add")
        
        self.catalog_box = widgets.VBox()
        
        self.add_button.on_click(self.add_catalog)

        def on_send(b):
            with self.output_context():
                if not self.catalogs and self.input_box.value.strip():
                    self.add_catalog(None)
                
                if not self.catalogs:
                    self.html_widget.value = "<div style='color: red;'>No catalogs have been added; please add a catalog by clicking the '+' button or entering a valid catalog path.</div>"
                    return
                
                callback(list(self.catalogs.values()))
                return
            
        self.submit_button = widgets.Button(description="Submit", button_style='success', icon='check')
        self.submit_button.on_click(on_send)

        html_box = widgets.VBox([self.html_widget])
        
        input_container = widgets.VBox([
            widgets.HBox([self.input_box, self.add_button]),
            self.catalog_box,
            self.submit_button
        ])

        display(HTML('''
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<div style="font-family: Arial, sans-serif; ">
    <h3>📚 Provide your Catalog Directories</h3>
    <p>
        Catalogs can be:
        <ol style="margin-top: 10px; margin-bottom: 10px;">
            <li>dbt projects as table catlogs built by Cocoon, or</li>
            <li>existing compiled dbt projects (make sure <code style="background-color: #e9ecef; padding: 2px 4px; border-radius: 4px;">target/manifest.json</code> is available).</li>
        </ol>
        Please provide the project folder (not target/model folder). Prefer full paths over relative paths.
    </p>
</div>
'''))
        display(html_box)
        display(input_container)

    def add_catalog(self, _):
        catalog = self.input_box.value.strip()
        if catalog:
            if os.path.exists(catalog):
                if catalog not in self.catalogs:
                    self.html_widget.value = f"{running_spinner_html} Reading the catalog..."
                    obj = validate_catalog_and_generate_object(catalog)
                    if obj:
                        self.catalogs[catalog] = obj
                        self.update_catalog_box()
                        self.input_box.value = ""
                        self.html_widget.value = ""
                        self.print_catalog(catalog)
                    else:
                        self.html_widget.value = "<div style='color: red;'>Failed to process the catalog</div>"
                else:
                    self.html_widget.value = "<div style='color: orange;'>This catalog is already in the list</div>"
            else:
                self.html_widget.value = "<div style='color: red;'>Directory not found</div>"
        else:
            self.html_widget.value = "<div style='color: red;'>Please enter a directory path</div>"

    def update_catalog_box(self):
        self.catalog_box.children = [
            widgets.HBox([
                widgets.HTML(value=self.get_catalog_display_html(i+1, catalog, obj),
                             layout=widgets.Layout(width='70%')),
                widgets.Button(icon="eye", layout=widgets.Layout(width="40px"), 
                               button_style="info", tooltip="Print"),
                widgets.Button(icon="times", layout=widgets.Layout(width="40px"), 
                               button_style="danger", tooltip="Remove")
            ])
            for i, (catalog, obj) in enumerate(self.catalogs.items())
        ]
        for i, (catalog, _) in enumerate(self.catalogs.items()):
            self.catalog_box.children[i].children[1].on_click(lambda _, cat=catalog: self.print_catalog(cat))
            self.catalog_box.children[i].children[2].on_click(lambda _, cat=catalog: self.remove_catalog(cat))

    def get_catalog_display_html(self, number, catalog_path, catalog_obj):
        try:
            name = catalog_obj.get_name()
        except AttributeError:
            name = os.path.basename(catalog_path)
        
        return f"{number}. <b>{name}</b>: {catalog_path}"

    def remove_catalog(self, catalog):
        del self.catalogs[catalog]
        self.update_catalog_box()
        if self.catalogs:
            self.print_catalog(list(self.catalogs.keys())[-1])
        else:
            self.html_widget.value = ""

    def print_catalog(self, catalog):
        obj = self.catalogs[catalog]
        self.html_widget.value = obj.generate_html_summary()

def validate_catalog_and_generate_object(catalog):
    db_path = os.path.join(catalog, 'cocoon_lineage.db')
    if os.path.exists(db_path):
        try:
            dbt_lineage = DbtLineage(dbt_directory=catalog)
            dbt_lineage.load_from_disk(db_path)
            return dbt_lineage
        except Exception as e:
            print(f"Error loading DbtLineage from {db_path}: {str(e)}")
            return None

    manifest_path = os.path.join(catalog, 'target', 'manifest.json')
    if os.path.exists(manifest_path):
        try:
            dbt_lineage = DbtLineage(dbt_directory=catalog)
            with open(manifest_path, 'r') as f:
                manifest_dict = json.load(f)
            dbt_lineage.populate_from_manifest(manifest_dict)
            return dbt_lineage
        except Exception as e:
            print(f"Error processing DBT manifest for {catalog}: {str(e)}")
            return None
    
    try:
        data_project = read_data_project_from_dir(catalog)
        return data_project
    except Exception as e:
        print(f"Error processing Data Project for {catalog}: {str(e)}")
        return None
    
class MultipleCatalogBuilder(Node):
    default_name = 'Multiple Catalog Builder'
    default_description = 'This step reads the DBT catalog and builds the data project.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):

        clear_output(wait=True)

        display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"><h2>🎉 You have successfully loaded your catalogs!</h2><em>You can now chat with your data.</em>'''))

        catalogs = self.get_sibling_document("Multiple Catalog Configuration")

        name_to_catalog = {catalog.get_name(): catalog for catalog in catalogs}
        options = ["Choose a catalog to explore"] + list(name_to_catalog.keys())
        
        self.dropdown = widgets.Dropdown(
            options=options,
            value="Choose a catalog to explore",
            disabled=False,
            layout=widgets.Layout(width='50%')
        )

        self.html_widget = widgets.HTML(
            value='',
            placeholder='',
            description='',
        )

        def update_html(change):
            if change['new'] != "Choose a catalog to explore":
                selected_catalog = name_to_catalog[change['new']]
                try:
                    self.html_widget.value = f"{running_spinner_html} Loading the catalog..."
                    html_summary = selected_catalog.generate_items_summary()
                    self.html_widget.value = wrap_in_iframe(html_summary, width='100%', height='600px')
                except Exception as e:
                    self.html_widget.value = f"<p>Error processing Data Project for {selected_catalog.get_name()}: {str(e)}</p>"

        self.dropdown.observe(update_html, names='value')

        text_input = widgets.Text(
            placeholder='Ask any question about the DBT project...',
            layout=widgets.Layout(width='70%')
        )

        send_button = widgets.Button(
            description='Send',
            button_style='primary',
            layout=widgets.Layout(width='10%')
        )

        def on_send(b):
            with self.output_context():
                message = text_input.value
                if message:
                    callback({"question": message})
                    return
                else:
                    display(HTML("<p style='color: red;'>Please enter a question before sending.</p>"))

        send_button.on_click(on_send)

        chat_input = widgets.HBox([text_input, send_button], layout=widgets.Layout(width='100%', margin='10px 0px'))

        display(self.dropdown)
        display(self.html_widget)
        display(chat_input)


class ProcessUserQuestionMultipleCatalogs(Node):
    default_name = 'Process User Question Multiple Catalogs'
    default_description = 'This step processes the user question across multiple catalogs.'

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        clear_output(wait=True)
        display(HTML('''<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> '''))

        question = self.get_sibling_document('Multiple Catalog Builder').get("question", "No question found")
        catalogs = self.get_sibling_document("Multiple Catalog Configuration")

        chat = ChatHTMLGenerator()
        chat.display()

        agent = MultiDataCatalogLLMAgent(data_catalogs=catalogs, chat_ui=chat, debug=False)

        def serve_user_question(question):
            agent.process_user_question(question)

        follow_up_button = widgets.Button(
            description='Follow Up',
            icon='arrow-right',
            button_style='primary',
            layout=widgets.Layout(width='15%'),
            tooltip='Ask a follow-up question'
        )

        new_question_input = widgets.Text(
            placeholder='Enter your question here...',
            layout=widgets.Layout(width='70%')
        )

        new_button = widgets.Button(
            description='New',
            icon='plus',
            button_style='warning',
            layout=widgets.Layout(width='15%'),
            tooltip='Start a new conversation'
        )

        warning_widget = widgets.HTML(
            value='',
            layout=widgets.Layout(width='100%', margin='10px 0px')
        )

        def on_follow_up(b):
            new_question = new_question_input.value
            if new_question:
                serve_user_question(new_question)
                new_question_input.value = ''
            update_warning()

        def on_new(b):
            chat.clear_all_messages()
            nonlocal agent
            agent = MultiDataCatalogLLMAgent(data_catalogs=catalogs, chat_ui=chat, debug=False)
            warning_widget.value = ''
            new_question = new_question_input.value
            if new_question:
                serve_user_question(new_question)
                new_question_input.value = ''

        follow_up_button.on_click(on_follow_up)
        new_button.on_click(on_new)

        def update_warning():
            if len(agent.conversation_history) > 10:
                warning_widget.value = '<div style="color: red">Warning: Conversation is getting long. Consider starting a new conversation for better performance.</div>'
            else:
                warning_widget.value = ''

        new_question_box = widgets.HBox([new_question_input, follow_up_button, new_button], 
                                        layout=widgets.Layout(width='100%', margin='20px 0px'))

        display(widgets.VBox([new_question_box, warning_widget]))

        serve_user_question(question)


class DBTLLMAgentStreamlit:
    def __init__(self, dbt_lineage, debug=False, track_cost=False):
        self.dbt_lineage = dbt_lineage
        self.conversation_history = []
        self.internal_chat = []
        self.step_count = 0
        self.debug = debug
        self.explored_model_ids = set()
        self.track_cost = track_cost
        self.session_cost = 0 if track_cost else None
        
    def generate_initial_prompt(self, user_question):
        model_context = self.dbt_lineage.generate_text_lineage()
        prompt = f"""# DBT Project models (higher index models are downstream):
{model_context}

## General Principles
1. Prioritize SQL-based answers using existing models.
2. Be evidence-based, referencing `model_name` and ```sql codes``` frequently
3. Explore upstream models for data sufficiency, prefer downstream models for final answers.
4. Keep responses concise.
5. Persist in exploration if data seems insufficient.

## Syntax Guidelines
- Reference models: `model_name`
- SQL code blocks:
  ```sql
  SELECT ...
  ```
- New model creation:
  ```sql
  -- model_name: 'new_model_name'
  SELECT ...
  ```

## Editing Existing Models
- Highlight changes only:
  ```sql
  -- model_name: 'existing_model_name'
  -- ... existing code ...
  {{ edit_1 }}
  -- ... existing code ...
  {{ edit_2 }}
  -- ... existing code ...
  ```
- Skip unchanged parts (start/end of file).
- Rewrite entire file only if explicitly requested.
- Consider downstream effects when editing.
- Propose edits for affected downstream models.

## Your actions (one at a time):
1. Explore the dbt model to answer the user's question. 
```yml
reason: >-
    The most relavant models are ...
action: explore
model_ids: [<list of up to 10 model ids to explore; preferably include their upstream models for better context>]
```
2. Answer the question based on the current context.
```yml
reason: >-
    I'm going to build new model/edit existing models and their downstream models. I have explored all the relavant models/ I still need to explore ...[action becomes explore]
action: conclude
answer: |
    <reference `model_name` and ```sql codes```>
```

Now answer the question: "{user_question}"
Your action:"""
        self.print_message("user", prompt)
        return prompt
    
    def generate_follow_up(self, user_question):
        return f'User follow-up question: "{user_question}"\nYour action:'

    def process_user_question(self, user_question):
        if not self.conversation_history:
            initial_prompt = self.generate_initial_prompt(user_question)
            self.conversation_history = [{"role": "user", "content": initial_prompt}]
        else:
            follow_up_prompt = self.generate_follow_up(user_question)
            self.conversation_history.append({"role": "user", "content": follow_up_prompt})
        
        self.internal_chat.append(("user", [("markdown", user_question)]))

        max_rounds = 10
        for _ in range(max_rounds):
            for attempt in range(5): 
                try:
                    if self.track_cost:
                        response, cost = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1, return_cost=True)
                        self.session_cost += cost
                    else:
                        response = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1)
                    
                    llm_message = response['choices'][0]['message']
                    yaml_content = extract_yml_code(llm_message["content"])
                    
                    decision = yaml.safe_load(yaml_content)
                    if decision['action'] in ['explore', 'conclude']:
                        self.conversation_history.append(llm_message)
                        self.print_message(llm_message["role"], llm_message["content"])
                        
                        if decision['action'] == 'explore':
                            self.explore_dbt_models(decision.get('reason', 'No reason provided'), decision.get('model_ids', []))
                        else:
                            self.conclude(decision)
                            return decision
                        
                        break
                except:
                    if cocoon_main_setting['DEBUG_MODE']:
                        raise
                    
                    if attempt == 4:
                        decision = {
                            "action": "conclude",
                            "answer": "There seems to be some issue processing your request. Could you please rephrase your question or provide more details?"
                        }
                        self.conclude(decision)
                        return decision
                    
                    time.sleep(5*attempt)
                    continue
        
        return {"action": "conclude", "answer": "Max rounds reached without a conclusion. Please try rephrasing your question."}

    def explore_dbt_models(self, reason, model_ids):
        exploration_results = []
        newly_explored_models = []
        for model_id in model_ids:
            if model_id in self.explored_model_ids:
                exploration_results.append(f"Model ID {model_id}:\nAlready explored before")
            else:
                model_summary_dict = self.dbt_lineage.generate_model_summary_text_dict(model_id=model_id)
                model_summary_text = "\n".join([f"{key}\n{model_summary_dict[key]}" for key in model_summary_dict])
                exploration_results.append(f"Model ID {model_id}:\n{model_summary_text}")
                self.explored_model_ids.add(model_id)
                newly_explored_models.append(model_id)
        
        if self.debug:
            self.dbt_lineage.display_model_lineage_by_indices(newly_explored_models)

        combined_results = "\n\n".join(exploration_results)
        follow_up_prompt = f"""Exploration results:
{combined_results}

Your next action (Explore or Conclude):"""
        self.print_message("user", follow_up_prompt)
        self.conversation_history.append({"role": "user", "content": follow_up_prompt})

        self.internal_chat.append(("Cocoon", [
            ("markdown", f"**RAG Data Pipeline...**\n\n{reason}"),
            ("cocoon", model_ids)
        ]))

    def conclude(self, decision):
        content = decision.get('answer', '')
        self.internal_chat.append(("assistant", [("markdown", content)]))

    def print_message(self, role, content):
        if self.debug:
            self.step_count += 1
            print(f"\n--- Step {self.step_count}: {role.capitalize()} Message ---\n")
            print(content)
            print("\n--- End of Message ---\n")

    def get_session_cost(self):
        if self.track_cost:
            return f"${self.session_cost:.4f}"
        else:
            return "not enabled"

    def display_chat_history(self):
        for role, content_list in self.internal_chat:
            with st.chat_message(role):
                for content_type, content in content_list:
                    if content_type == 'markdown':
                        st.markdown(content, unsafe_allow_html=True)
                    elif content_type == 'html':
                        st.components.v1.html(content, height=300, scrolling=True)
                    elif content_type == 'sql':
                        st.code(content, language="sql")
                    elif content_type == 'yml':
                        st.markdown(
                            f"""
                            <div style="max-height: 500px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;">
                            <pre><code class="language-yaml">{content}</code></pre>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                    elif content_type == 'cocoon':
                        self.dbt_lineage.create_dbt_lineage_selectbox(model_ids=content)
                    else:
                        st.text(content)
        
        if self.track_cost:
            st.markdown(f"<small>Cost estimation (based on Claude 3.5 Sonnet): {self.get_session_cost()}</small>", unsafe_allow_html=True)

class ColumnMapperNode(Node):
    default_name = 'Column Mapper'
    default_description = 'This node maps input columns to target table columns.'

    def extract(self, item):
        con = self.para['con']
        target_tables = self.para['target_tables']

        target_descriptions = {}
        for table, df in target_tables.items():
            columns = []
            for idx, row in df.iterrows():
                col_info = f"{idx + 1}. \"{row['name']}\" ({row['type']})"
                if row['description']:
                    col_info += f": \"{row['description']}\""
                columns.append(col_info)
            target_descriptions[f"{table}"] = "\n".join(columns)

        sample_query = "SELECT * FROM input_table LIMIT 5"
        sample_df = con.execute(sample_query).df()
        sample_df = sample_df.applymap(truncate_cell)

        return {
            'target_descriptions': target_descriptions,
            'sample_df': sample_df
        }

    def run(self, extract_output, use_cache=True):
        target_descriptions = extract_output['target_descriptions']
        sample_df = extract_output['sample_df']

        template = f"""You are an expert in data mapping. Given the following information:

## Target tables and their columns:
{yaml.dump(target_descriptions, default_flow_style=False)}

## Sample of the input table:
{sample_df.to_csv(index=False, quoting=2)}

Please map each column from the input table to the most appropriate target table and column(s). 
One input column can be mapped to multiple output columns across different tables.
If an input column doesn't match any existing target columns but its meaning is clear and relates to a specific table, 
suggest a new column name for the best matching target table.

Respond in YAML format as follows:

```yaml
mapping:
  - input_col: [input column name]
    description: |
        [brief description of the input column]
    outputs:
      - output_table: [target table name]
        output_col: [target column name or suggested new column name]
      - output_table: [another target table name]
        output_col: [another target column name]
```

Ensure all input columns are mapped, and provide clear, concise descriptions."""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yaml_code(response['choices'][0]['message']["content"])
        mapping = yaml.safe_load(yml_code)

        if not isinstance(mapping, dict) or 'mapping' not in mapping:
            raise ValueError("Invalid mapping format")
        
        validated_mapping = []
        mapped_input_cols = set()
        input_columns = sample_df.columns.tolist()

        for item in mapping['mapping']:
            if item['input_col'] in input_columns:
                mapped_input_cols.add(item['input_col'])
                
                validated_outputs = []
                for output in item.get('outputs', []):
                    if output['output_table'] in target_descriptions:
                        if output['output_col']:
                            output['output_col'] = clean_column_name(output['output_col'])
                        else:
                            output['output_col'] = clean_column_name(item['input_col'])
                        
                        validated_outputs.append(output)
                
                item['description'] = item['description'].strip()
                
                item['outputs'] = validated_outputs
                
                validated_mapping.append(item)

        for col in input_columns:
            if col not in mapped_input_cols:
                validated_mapping.append({
                    'input_col': col,
                    'description': 'Unmapped column',
                    'outputs': []
                })

        return validated_mapping
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {
            'mapping': []
        }
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)


def create_mapping_df(mapping):
    rows = []
    for item in mapping:
        input_col = item.get('input_col', '')
        description = item.get('description', '').strip()
        outputs = item.get('outputs', [])
        
        if outputs:
            for output in outputs:
                if output.get('output_table') and output.get('output_col'):
                    rows.append({
                        'Input Column': input_col,
                        'Description': description,
                        'Output Table': output.get('output_table', ''),
                        'Output Column': output.get('output_col', '')
                    })
    
    df = pd.DataFrame(rows, columns=['Input Column', 'Description', 'Output Table', 'Output Column'])
    return df

class ColumnTransformerNode(Node):
    default_name = 'Column Transformer'
    default_description = 'This node suggests SQL transformations for mapped columns.'

    def extract(self, item):
        con = self.para['con']
        mapping = self.get_sibling_document('Column Mapper')
        target_tables = self.para['target_tables']
        
        mapping_df = create_mapping_df(mapping)

        sample_query = "SELECT * FROM input_table LIMIT 100"
        sample_df = con.execute(sample_query).df()
        sample_df = sample_df.applymap(truncate_cell)

        grouped_transformations = []
        for (output_table, output_col), group in mapping_df.groupby(['Output Table', 'Output Column']):
            if output_table and output_col:
                transformation = OrderedDict([
                    ('output_table', output_table),
                    ('output_col', output_col),
                    ('input_cols', group['Input Column'].tolist()),
                    ('description', group['Description'].iloc[0])
                ])
                
                if output_table in target_tables and output_col in target_tables[output_table]['name'].values:
                    col_info = target_tables[output_table].loc[target_tables[output_table]['name'] == output_col].iloc[0]
                    transformation['type'] = col_info['type']
                    if col_info.get('description'):
                        transformation['description'] = col_info['description']
                
                grouped_transformations.append(transformation)

        return {
            'grouped_transformations': grouped_transformations,
            'sample_df': sample_df
        }

    def run(self, extract_output, use_cache=True):
        grouped_transformations = extract_output['grouped_transformations']
        sample_df = extract_output['sample_df']

        if not grouped_transformations:
            return self.run_but_fail(extract_output, use_cache)

        template = f"""You are an expert in SQL and data transformation. Given the following information:

## Input table sample (QUOTE_NONNUMERIC):
{sample_df.to_csv(index=False, quoting=2)}

## Column mappings and descriptions:
{yaml.dump(grouped_transformations, default_flow_style=False)}

For each output column, suggest an appropriate SQL transformation clause using DuckDB syntax if needed.
Most columns do not need transformation, use an empty string "" if that's the case.

Example transformations:
{duckdb_cocoon_hint}

Respond in YAML format as follows:

```yaml
transformations:
  - output_table: target table name
    output_col: target column name
    sql_transform: |
      [SQL transformation clause or ""]
```

Ensure all mapped columns are included in the response."""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yaml_code(response['choices'][0]['message']["content"])
        transformations = yaml.safe_load(yml_code)

        if not isinstance(transformations, dict) or 'transformations' not in transformations:
            raise ValueError("Invalid transformations format")

        validated_transformations = []
        for transform in transformations['transformations']:
            matching_group = next((g for g in grouped_transformations 
                                   if g['output_table'] == transform['output_table'] 
                                   and g['output_col'] == transform['output_col']), None)
            
            if matching_group:
                transform['sql_transform'] = transform['sql_transform'].strip()

                if transform['sql_transform'] == '""':
                    transform['sql_transform'] = ''

                if transform['sql_transform'].strip():    
                    sql_query = f"SELECT {transform['sql_transform']} AS \"{transform['output_col']}\" FROM input_table LIMIT 1"
                    try:
                        self.para['con'].execute(sql_query)
                    except Exception as e:
                        print(f"SQL error for {transform['output_table']}.{transform['output_col']}. SQL is: {sql_query}. Error Message: {str(e)}")
                        transform['sql_transform'] = ""
                
                transform['input_cols'] = matching_group['input_cols']
                validated_transformations.append(transform)

        for group in grouped_transformations:
            if not any(t['output_table'] == group['output_table'] and t['output_col'] == group['output_col'] 
                       for t in validated_transformations):
                validated_transformations.append({
                    'output_table': group['output_table'],
                    'output_col': group['output_col'],
                    'input_cols': group['input_cols'],
                    'sql_transform': ""
                })

        return validated_transformations

    def run_but_fail(self, extract_output, use_cache=True):
        return []
    
    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        return callback(run_output)

class ParseDFNode(Node):
    default_name = 'Parse DataFrame'
    default_description = 'This node parses and cleans input DataFrames using LLM-generated code.'

    def extract(self, item):
        df = self.para['df']
        self.para['df'] = df
        sample_df = df.head(10).iloc[:, :10]
        sample_df = sample_df.applymap(lambda x: truncate_cell(x, max_length=1000))
        return {'sample_df': sample_df}

    def run(self, extract_output, use_cache=True):
        sample_df = extract_output['sample_df']

        template = f"""Task: Analyze if the df has parsing issues to address. For example:

1. Incorrect headers
(e.g., if the headers are actually in row)
"","Unnamed__0","Unnamed__1","Unnamed__2"
"0","","",""
"1","","",""
"2","","First Name","Last Name"
"3","1.0","John","Doe"

```
def clean_df(df):
    # Replace column names with the third row of data (index 2)
    df.columns = df.iloc[2]
    
    # Drop all rows up to and including the header row
    df = df.drop(df.index[:3]).reset_index(drop=True)
    
    return df_clean
```

2. Missing headers 
(e.g., if the header is not column names but values)
"","John Doe","28"
"0","Jane Smith","34"
"1","Alice Johnson","45"

```
def clean_df(df):
    df.loc[-1] = df.columns  # Add the headers as a new row
    df.index = df.index + 1  # Shift index to accommodate the new row
    df = df.sort_index()     # Sort by index to ensure the new row is at the top

    # Assign new column names based on your understanding of the data
    df.columns = ["name", "age"]
    return df
```

3. Separated values in cells. 
(E.g., there is only one column 'name|age', but the values are separated by '|'.)
"","name|age"
"0","John Doe|28"
"1","Jane Smith|34"
"2","Alice Johnson|45"

```
def clean_df(df):
    # Split the 'name|age' column into separate 'name' and 'age' columns
    df_clean[['name', 'age']] = df_clean['name|age'].str.split('|', expand=True)
    
    # Drop the original combined column
    df_clean = df_clean.drop(columns=['name|age'])
    
    return df_clean
```

* 4. DON'T REMOVE ANY COLUMN
Do nothing! This is not a parsing issue. We will handle it later.
"","Name","","Age"
"0","Alice","4","25"
"1","Bob","5","30"
"2","Charlie","6","35"

```
explanation: >-
    The second column has empty header, but I will not remove it according to the instruction.
need_to_clean: false
```

### NOW THE RESULT OF df.to_csv(index=True, quoting=1), where first line (not indexed) is the header###
{sample_df.to_csv(index=True, quoting=1)}
### END OF THE RESULT ###

First reason about if there is any parsing issue.
If there is an issue, fill in the python function below, with detailed comments. 

DONT: change the data type. 
DONT: change the function name, and the return clause.
DONT: remove the columns.

```yaml
explanation: >-
    The header contains... The rows are about ...
    It (has/doesn't have) any parsing issue.
    If it does, any code snippet above can reference?
    I will not include any codes to change the data type / remove the columns.
need_to_clean: true/false
# If need_to_clean is true, then the python_code is required.
# Please follow the provided python example and avoid new codes
python_code: |
    def clean_df(df):
        # Your code here: Simple and short (usually just a few lines), with detailed comments.
        ...
        return df_clean
```"""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yaml_code(response['choices'][0]['message']["content"])
        result = yaml.safe_load(yml_code)

        if not result['need_to_clean']:
            return self.para['df']

        python_code = result['python_code']

        for _ in range(2):
            try:
                df_input = self.para['df'].copy()
                if 'clean_df' in globals():
                    del globals()['clean_df']
                exec(python_code, globals())
                df_clean = clean_df(df_input)
                return df_clean
            except Exception:
                detailed_error_info = get_detailed_error_info()
                if cocoon_main_setting['DEBUG_MODE']:
                    print(detailed_error_info)
                    print(df_input)
                error_message = f"""There is a bug in the code: {detailed_error_info}.
First, study the error message and point out the problem.
Then, fix the bug and return the complete corrected function in the following format:

```yml
explanation: >-
    The error is due to ...
python_code: |
    def clean_df(df):
        # Your code here
        ...
        return df_clean
```"""
                messages.append({"role": "user", "content": error_message})
                response = call_llm_chat(messages, temperature=0.1, top_p=0.1)
                yml_code = extract_yml_code(response['choices'][0]['message']['content'])
                result = yaml.safe_load(yml_code)
                messages.append(response['choices'][0]['message'])
                python_code = result['python_code']

        raise Exception("The code is not correct after multiple attempts. Please try again.")

    def run_but_fail(self, extract_output, use_cache=True):
        return self.para['df']

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        df = run_output
        df.columns = [clean_column_name(col) for col in df.columns]
        con = self.para['con']
        table_name = "input_table"
        con.execute(f"CREATE OR REPLACE TABLE {table_name} AS SELECT * FROM df")
        callback({})
        return 

def create_column_map_workflow(para=None, output=None):
    if para is None:
        para = {}
    main_workflow = Workflow("Column Map Workflow",
                             item={},
                             description="A workflow to map columns",
                             para=para,
                             output=output)
    
    main_workflow.add_to_leaf(ParseDFNode(output=output))
    main_workflow.add_to_leaf(ColumnMapperNode(output=output))
    main_workflow.add_to_leaf(ColumnTransformerNode(output=output))
    return main_workflow


def build_transformation_df(target_tables, validated_transformations):
    all_transformations = []
    
    for table, columns in target_tables.items():
        for column in columns['name']:
            all_transformations.append({
                'Output Table': table,
                'Output Column': column,
                'SQL Transformation': ''
            })
    
    for transform in validated_transformations:
        sql_clause = transform['sql_transform'] if transform['sql_transform'] else ''
        if not sql_clause:
            if len(transform['input_cols']) == 1:
                sql_clause = f"\"{transform['input_cols'][0]}\""
            else:
                concat_cols = ', '.join(f'"{col}"' for col in transform['input_cols'])
                sql_clause = f"CONCAT({concat_cols})"
        
        existing_entry = next((item for item in all_transformations 
                               if item['Output Table'] == transform['output_table'] 
                               and item['Output Column'] == transform['output_col']), None)
        
        if existing_entry:
            existing_entry['SQL Transformation'] = sql_clause
        else:
            all_transformations.append({
                'Output Table': transform['output_table'],
                'Output Column': transform['output_col'],
                'SQL Transformation': sql_clause
            })
    
    return pd.DataFrame(all_transformations)

def execute_transformations(con, transformation_df):
    result = {}
    for table in transformation_df['Output Table'].unique():
        table_transformations = transformation_df[transformation_df['Output Table'] == table]
        
        clauses = []
        columns = []
        for _, row in table_transformations.iterrows():
            columns.append(row['Output Column'])
            if row['SQL Transformation']:
                clauses.append(f"{row['SQL Transformation']} AS \"{row['Output Column']}\"")
        
        if clauses:
            query = f"SELECT {', '.join(clauses)} FROM input_table"
            df = con.execute(query).df()
            for col in columns:
                if col not in df.columns:
                    df[col] = None
            result[table] = df[columns]
        else:
            result[table] = pd.DataFrame(columns=columns)
    
    return result


def create_mapping_from_df(mapping_df):
    mapping = []
    
    grouped = mapping_df.groupby(['Input Column', 'Description'])
    
    for (input_col, description), group in grouped:
        outputs = []
        for _, row in group.iterrows():
            if row['Output Table'] and row['Output Column']:
                outputs.append({
                    'output_table': row['Output Table'],
                    'output_col': row['Output Column']
                })
        
        item = {
            'input_col': input_col,
            'description': description,
            'outputs': outputs
        }
        mapping.append(item)
    
    return mapping


class RefineTransformationNode(Node):
    default_name = 'Refine Transformation'
    default_description = 'This node refines the column mapping and transformations based on a natural language query.'

    def extract(self, item):
        con = self.para['con']
        target_tables = self.para['target_tables']
        mapping_df = self.para['mapping_df'].copy(deep=True)
        transformation_df = self.para['transformation_df'].copy(deep=True)
        query = self.para['query']

        mapping = create_mapping_from_df(mapping_df)

        sample_query = "SELECT * FROM input_table LIMIT 100"
        sample_df = con.execute(sample_query).df()
        sample_df = sample_df.applymap(truncate_cell)

        return {
            'target_tables': target_tables,
            'mapping': mapping,
            'transformation_df': transformation_df,
            'sample_df': sample_df,
            'query': query
        }

    def run(self, extract_output, use_cache=True):
        target_tables = extract_output['target_tables']
        mapping = extract_output['mapping']
        transformation_df = extract_output['transformation_df']
        sample_df = extract_output['sample_df']
        query = extract_output['query']
        con = self.para['con']

        target_tables_str = ""
        for table_name, table_df in target_tables.items():
            target_tables_str += f"Table Name: {table_name}\n"
            target_tables_str += table_df.to_csv(index=False)
            target_tables_str += "\n"

        template = f"""You are an expert in data mapping and SQL transformations. Given the following information:

## Sample of the input table (QUOTE_NONNUMERIC):
{sample_df.to_csv(index=False, quoting=2)}

## Target tables and their columns:
{target_tables_str}

## Current column mapping:
{yaml.dump(mapping, default_flow_style=False)}

## Current transformations:
{transformation_df.to_csv(index=False)}

Example transformations:
{duckdb_cocoon_hint}

## USER REQUEST:
{query}
## END OF USER REQUEST

Please suggest updates to the column mapping and transformations based on the user's query. 
If the user's query is not possible in SQL or not related to the data transformation, you can choose to not update and politely respond.
Respond in YAML format as follows:

```yaml
mapping_updates:
  - input_col: [input column name]
    description: [description of the input column]
    outputs:
      - output_table: [target table name]
        output_col: [target column name]
transformation_updates:
  - output_table: [target table name]
    output_col: [target column name]
    sql_transform: |
      [SQL transformation clause or "" if not transformed]
response: >-
    Sure! I have mapped the columns and transformed the data based on your request. 
```

Only include columns that need to be updated or added. For unchanged columns, do not include them in the response."""

        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)

        yml_code = extract_yaml_code(response['choices'][0]['message']["content"])
        updates = yaml.safe_load(yml_code)

        for update in updates.get('mapping_updates', []):
            if update['input_col'] not in sample_df.columns:
                raise ValueError(f"Input column '{update['input_col']}' does not exist.")
            
            for output in update.get('outputs', []):
                output_table = output.get('output_table')
                if output_table and output_table.strip():
                    if output_table not in target_tables:
                        raise ValueError(f"Output table '{output_table}' does not exist.")
                else:
                    output['output_table'] = None
                    output['output_col'] = None

        for update in updates.get('transformation_updates', []):
            if update['output_table'] not in target_tables:
                raise ValueError(f"Output table '{update['output_table']}' does not exist.")
            if update['sql_transform'] is not None and update['sql_transform'].strip() == '""':
                update['sql_transform'] = ''
            if update['sql_transform'] is None or update['sql_transform'].strip() == '':
                update['sql_transform'] = ""
            elif update['sql_transform'].strip():
                try:
                    con.execute(f"SELECT {update['sql_transform']} AS \"{update['output_col']}\" FROM input_table LIMIT 1")
                except Exception as e:
                    print(f"SELECT {update['sql_transform']} AS \"{update['output_col']}\" FROM input_table LIMIT 1")
                    raise ValueError(f"SQL error for {update['output_table']}.{update['output_col']}: {str(e)}")

        return updates
    
    def run_but_fail(self, extract_output, use_cache=True):
        return {
            'mapping_updates': [],
            'transformation_updates': [],
            'response': "Sorry, there is something wrong with the request. Please try again."
        }

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        updates = run_output
        mapping = extract_output['mapping']
        transformation_df = extract_output['transformation_df']

        affected_input_cols = []
        affected_output_cols = []

        for update in updates.get('mapping_updates', []):
            existing_item = next((item for item in mapping if item['input_col'] == update['input_col']), None)
            if existing_item:
                existing_item['description'] = update['description']
                existing_item['outputs'] = update['outputs']
            else:
                mapping.append(update)
            
            affected_input_cols.append(update['input_col'])
            for output in update['outputs']:
                affected_output_cols.append([output['output_table'], output['output_col']])

        target_tables = extract_output['target_tables']
        for update in updates.get('transformation_updates', []):
            if update['sql_transform'].strip() == '':
                if update['output_table'] in target_tables and update['output_col'] not in target_tables[update['output_table']]['name'].values:
                    transformation_df = transformation_df[
                        ~((transformation_df['Output Table'] == update['output_table']) & 
                          (transformation_df['Output Column'] == update['output_col']))
                    ]
                    continue

            idx = transformation_df[(transformation_df['Output Table'] == update['output_table']) & 
                                    (transformation_df['Output Column'] == update['output_col'])].index
            if len(idx) > 0:
                transformation_df.loc[idx, 'SQL Transformation'] = update['sql_transform']
            else:
                new_row = pd.DataFrame([{
                    'Output Table': update['output_table'],
                    'Output Column': update['output_col'],
                    'SQL Transformation': update['sql_transform']
                }])
                transformation_df = pd.concat([transformation_df, new_row], ignore_index=True)
            
            affected_output_cols.append([update['output_table'], update['output_col']])

        mapping_df = create_mapping_df(mapping)

        return callback({
            'mapping': mapping,
            'mapping_df': mapping_df,
            'transformation_df': transformation_df,
            'response': updates['response'],
            'affected_input_cols': list(set(affected_input_cols)),
            'affected_output_cols': list(map(list, set(map(tuple, affected_output_cols))))
        })
        
def create_refine_transformation_workflow(para=None, output=None):
    if para is None:
        para = {}
    refine_workflow = Workflow("Refine Transformation Workflow",
                               item={},
                               description="A workflow to refine column mapping and transformations",
                               para=para,
                               output=output)
    
    refine_workflow.add_to_leaf(RefineTransformationNode(output=output))
    return refine_workflow


class MultiCatalogStreamlitAgent:
    def __init__(self, data_catalogs, debug=False, track_cost=False):
        self.data_catalogs = data_catalogs
        self.conversation_history = []
        self.internal_chat = []
        self.step_count = 0
        self.debug = debug
        self.explored_item_ids = set()
        self.track_cost = track_cost
        self.session_cost = 0 if track_cost else None

    def generate_initial_prompt(self, user_question):
        catalog_contexts = []
        for idx, catalog in enumerate(self.data_catalogs, 1):
            catalog_context = catalog.generate_text_summary(idx=idx)
            catalog_contexts.append(f"Catalog {idx}: '{catalog.get_name()}'\n{catalog_context}")
        
        all_catalog_contexts = "\n\n".join(catalog_contexts)
        
        prompt = f"""{all_catalog_contexts}

## General Principles
1. Prioritize SQL-based answers using existing items in the catalogs.
2. Be evidence-based, referencing `item_name` and ```sql codes``` frequently.
3. Explore related items for context.
4. Keep responses concise.
5. Persist in exploration if information seems insufficient.

## Syntax Guidelines
- Reference models: `model_name`
- SQL code blocks:
  ```sql
  SELECT ...
  ```
- New model creation:
  ```sql
  -- model_name: 'new_model_name'
  SELECT ...
  ```

## Editing Existing Models
- Highlight changes only:
  ```sql
  -- model_name: 'existing_model_name'
  -- ... existing code ...
  {{ edit_1 }}
  -- ... existing code ...
  {{ edit_2 }}
  -- ... existing code ...
  ```
- Skip unchanged parts (start/end of file).
- Rewrite entire file only if explicitly requested.
- Consider downstream effects when editing.
- Propose edits for affected downstream models.

Note that you can only modify sql. If asked to modify other, politely reject.

## Your actions (one at a time):
1. Explore the catalogs to answer the user's question. 
---cocoon_start---
reason: >-
    The most relevant items are ...
action: explore
items:
  - catalog_id: 1
    item_ids: [<list of up to 10 item ids to explore; preferably include related items for better context>]
  - catalog_id: 2
    item_ids: [<list of up to 10 item ids to explore; preferably include related items for better context>]
---cocoon_end---

2. Answer the question based on the current context (or explain why you can't).
---cocoon_start---
reason: >-
    I'm going to answer the user's question. I have explored all the relevant items / I still need to explore ...[action becomes explore]
action: conclude
answer: |
    <reference `item_name` and ```sql codes```>
---cocoon_end---

Now answer the question: "{user_question}"
Your action:"""
        self.print_message("user", prompt)
        return prompt

    def generate_follow_up(self, user_question):
        return f'User follow-up question: "{user_question}"\nFor explore action, don\'t include item ids already explored.\nYour action:'

    def process_user_question(self, user_question):
        if not self.conversation_history:
            initial_prompt = self.generate_initial_prompt(user_question)
            self.conversation_history = [{"role": "user", "content": initial_prompt}]
        else:
            follow_up_prompt = self.generate_follow_up(user_question)
            self.conversation_history.append({"role": "user", "content": follow_up_prompt})
        
        self.internal_chat.append(("user", [("markdown", user_question)]))

        max_rounds = 10
        for _ in range(max_rounds):
            for attempt in range(5):
                try:
                    use_cache = (attempt == 0)
                    if self.track_cost:
                        response, cost = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1, return_cost=True, use_cache=use_cache)
                        self.session_cost += cost
                    else:
                        response = call_llm_chat(self.conversation_history, temperature=0.1, top_p=0.1, use_cache=use_cache)
                    
                    llm_message = response['choices'][0]['message']
                    
                    yaml_content = extract_special_code(llm_message["content"])
                    decision = yaml.safe_load(yaml_content)

                    if decision['action'] in ['explore', 'conclude']:
                        self.conversation_history.append(llm_message)
                        self.print_message(llm_message["role"], llm_message["content"])
                        
                        if decision['action'] == 'explore':
                            self.explore_items(decision.get('reason', 'No reason provided'), decision.get('items', []))
                        else:
                            self.conclude(decision)
                            return decision
                        
                        break
                except Exception as e:
                    if cocoon_main_setting.get('DEBUG_MODE', False):
                        raise
                    
                    if attempt == 4:
                        decision = {
                            "action": "conclude",
                            "answer": "There seems to be some issue processing your request. Could you please rephrase your question or provide more details?"
                        }
                        self.conclude(decision)
                        return decision
                    
                    time.sleep(5)
                    continue

        return {"action": "conclude", "answer": "Max rounds reached without a conclusion. Please try rephrasing your question."}

    def explore_items(self, reason, items):
        exploration_results = []
        newly_explored_items = []
        for item in items:
            catalog_id = item['catalog_id']
            catalog = self.data_catalogs[catalog_id - 1]
            catalog_results = []
            for item_id in item['item_ids']:
                item_name = catalog.get_item_name_by_id(item_id)
                full_item_id = f"{catalog_id}.{item_id}"
                if full_item_id in self.explored_item_ids:
                    catalog_results.append(f"{item_id}. {item_name}:\nAlready explored before")
                else:
                    item_summary_text = catalog.generate_item_text_summary(item_id=item_id)
                    catalog_results.append(f"{item_id}. {item_name}\n{item_summary_text}")
                    self.explored_item_ids.add(full_item_id)
                    newly_explored_items.append(full_item_id)
            
            if catalog_results:
                catalog_name = catalog.get_name()
                catalog_results_text = "\n\n".join(catalog_results)
                exploration_results.append(f"# Catalog {catalog_id}: {catalog_name}\n\n{catalog_results_text}")

        combined_results = "\n\n".join(exploration_results)
        follow_up_prompt = f"""Exploration results:
{combined_results}

Your next action (Explore or Conclude):"""
        self.print_message("user", follow_up_prompt)
        self.conversation_history.append({"role": "user", "content": follow_up_prompt})

        self.internal_chat.append(("Cocoon", [
            ("markdown", f"**RAG Data Pipeline...**\n\n{reason}"),
            ("cocoon", items)
        ]))

    def conclude(self, decision):
        content = decision.get('answer', '')
        self.internal_chat.append(("assistant", [("markdown", content)]))

    def print_message(self, role, content):
        if self.debug:
            self.step_count += 1
            print(f"\n--- Step {self.step_count}: {role.capitalize()} Message ---\n")
            print(content)
            print("\n--- End of Message ---\n")

    def get_session_cost(self):
        if self.track_cost:
            return f"${self.session_cost:.4f}"
        else:
            return "not enabled"

    def display_chat_history(self):
        for role, content_list in self.internal_chat:
            with st.chat_message(role):
                for content_type, content in content_list:
                    if content_type == 'markdown':
                        st.markdown(content, unsafe_allow_html=True)
                    elif content_type == 'html':
                        st.components.v1.html(content, height=300, scrolling=True)
                    elif content_type == 'sql':
                        st.code(content, language="sql")
                    elif content_type == 'yml':
                        st.markdown(
                            f"""
                            <div style="max-height: 500px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;">
                            <pre><code class="language-yaml">{content}</code></pre>
                            </div>
                            """,
                            unsafe_allow_html=True
                        )
                    elif content_type == 'cocoon':
                        self.display_cocoon_content(content)
                    else:
                        st.text(content)
        
        if self.track_cost:
            st.markdown(f"<small>Cost estimation: {self.get_session_cost()}</small>", unsafe_allow_html=True)

    def display_cocoon_content(self, items):
        for item in items:
            catalog_id = item['catalog_id']
            catalog = self.data_catalogs[catalog_id - 1]
            st.subheader(f"Catalog {catalog_id}: {catalog.get_name()}")
            catalog.create_data_selectbox(model_ids=item['item_ids'])


class AnalyzeModelUpstreamsSingleNode(Node):
    default_name = 'Analyze Single Model Upstreams'
    default_description = 'This node analyzes the SQL of a single model in the DBT lineage to determine upstream dependencies.'

    def extract(self, item):
        self.input_item = item
        dbt_lineage = item['dbt_lineage']
        model_name = self.para['model_name']
        
        model_info = dbt_lineage.conn.execute("""
            SELECT raw_code, compiled_code
            FROM model
            WHERE model_name = ?
        """, (model_name,)).fetchone()
        
        if model_info and (model_info[1] or model_info[0]):
            sql = model_info[1] if model_info[1] else model_info[0]
        else:
            sql = None
        
        return {'model_name': model_name, 'sql': sql}

    def run(self, extract_output, use_cache=True):
        model_name = extract_output['model_name']
        sql = extract_output['sql']
        
        display(HTML(f"Working on model: {model_name}"))
        
        if sql is None:
            return {
                'model_name': model_name,
                'upstream_models': {
                    'existing': [],
                    'new': []
                }
            }
        
        dbt_lineage = self.input_item['dbt_lineage']
        catalog_context = dbt_lineage.generate_text_summary()
        
        template = f"""{catalog_context}

Analyze the following model and its SQL to determine its upstream dependencies:

Model Name: {model_name}
SQL:
```sql
{sql}
```

Please provide the upstream models for this model. If the upstream is an existing model in the lineage, provide its name. If it's a new model referenced in the SQL but not in the lineage, provide its exact name as referenced in the SQL.

Respond in the following YAML format:
```yaml
model_name: {model_name}
upstream_models:
  # Skip this field if no existing upstream models
  existing:
    - existing model id as integer
  # Skip this field if no new upstream models
  new:
    - >-
      extract model name from SQL as string
```"""
        
        messages = [{"role": "user", "content": template}]
        response = call_llm_chat(messages, temperature=0.1, top_p=0.1, use_cache=use_cache)
        messages.append(response['choices'][0]['message'])
        self.messages.append(messages)
        
        yml_code = extract_yaml_code(response['choices'][0]['message']["content"])
        summary = yaml.load(yml_code, Loader=yaml.SafeLoader)

        dbt_lineage = self.input_item['dbt_lineage']
        existing_model_ids = set(row[0] for row in dbt_lineage.conn.execute("""
            SELECT new_rowid
            FROM ordered_models
            ORDER BY new_rowid
        """).fetchall())

        if 'upstream_models' not in summary or not isinstance(summary['upstream_models'], dict):
            summary['upstream_models'] = {}

        if 'existing' not in summary['upstream_models'] or summary['upstream_models']['existing'] is None:
            summary['upstream_models']['existing'] = []
        
        reported_existing = [model_id for model_id in summary['upstream_models']['existing']
                             if isinstance(model_id, int)]

        non_existent = [model_id for model_id in reported_existing if model_id not in existing_model_ids]
        if non_existent:
            raise ValueError(f"The following reported 'existing' model IDs do not exist in the database: {', '.join(map(str, non_existent))}")

        summary['upstream_models']['existing'] = reported_existing

        if 'new' not in summary['upstream_models'] or summary['upstream_models']['new'] is None:
            summary['upstream_models']['new'] = []

        summary['upstream_models']['new'] = list(set(
            model for model in summary['upstream_models']['new']
            if model and model.strip()
        ))

        return summary

    def postprocess(self, run_output, callback, viewer=False, extract_output=None):
        dbt_lineage = self.input_item['dbt_lineage']
        model_name = self.para['model_name']
        
        existing_upstream_names = []
        for existing_upstream_id in run_output['upstream_models'].get('existing', []):
            existing_upstream_name = dbt_lineage.get_item_name_by_id(existing_upstream_id)
            if existing_upstream_name != model_name:
                existing_upstream_names.append(existing_upstream_name)
        
        for existing_upstream_name in existing_upstream_names:
            dbt_lineage.add_model_lineage(existing_upstream_name, model_name)    
        
        for new_upstream in run_output['upstream_models'].get('new', []):
            if new_upstream not in dbt_lineage.conn.execute("SELECT model_name FROM model").fetchall():
                dbt_lineage.add_or_update_model({
                    'model_name': new_upstream
                })
            dbt_lineage.add_model_lineage(new_upstream, model_name)
        
        return callback(run_output)

class AnalyzeModelUpstreamsMultipleNode(MultipleNode):
    default_name = 'Analyze Model Upstreams'
    default_description = 'This node analyzes the SQL of all models in the DBT lineage to determine upstream dependencies.'

    def construct_node(self, element_name, idx=0, total=0):
        para = self.para.copy()
        para["model_name"] = element_name
        para["idx"] = idx
        para["total"] = total
        node = AnalyzeModelUpstreamsSingleNode(para=para, id_para="model_name")
        node.inherit(self)
        return node

    def extract(self, item):
        self.input_item = item
        dbt_lineage = item['dbt_lineage']
        
        dbt_lineage.invalidate_lineage_cache()
        models = dbt_lineage.get_ordered_models()
        
        filtered_models = []
        for model in models:
            model_name = model[0]
            
            raw_code = dbt_lineage.conn.execute("""
                SELECT raw_code
                FROM model
                WHERE model_name = ?
            """, (model_name,)).fetchone()
            
            if not raw_code or not raw_code[0]:
                continue
            
            upstream_models = dbt_lineage.conn.execute("""
                SELECT source_model_name
                FROM model_lineage
                WHERE target_model_name = ?
            """, (model_name,)).fetchall()
            
            if upstream_models:
                continue
            
            filtered_models.append(model)
        
        self.elements = [model[0] for model in filtered_models]
        self.nodes = {element: self.construct_node(element, idx, len(self.elements))
                      for idx, element in enumerate(self.elements)}

def create_dbt_lineage_analysis_workflow(dbt_lineage, output=None):
    workflow = Workflow("DBT Lineage Analysis Workflow",
                        item={'dbt_lineage': dbt_lineage},
                        description="A workflow to analyze DBT lineage and determine model dependencies",
                        output=output)
    
    workflow.add_to_leaf(AnalyzeModelUpstreamsMultipleNode(output=output))
    return workflow