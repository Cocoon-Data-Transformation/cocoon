import openai
import os
import pandas as pd
import json
import re
import pandas as pd
import numpy as np
import sys
import traceback
import networkx as nx
import matplotlib.pyplot as plt
from ipywidgets import *
import ipywidgets as widgets
from IPython.display import *
from graphviz import Digraph
import base64
from pygments import highlight
from pygments.lexers import PythonLexer
from pygments.formatters import Terminal256Formatter
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns
import plotly.graph_objects as go
import pandas as pd
import ast
import faiss  
from tqdm import tqdm
import base64
from io import BytesIO
import heapq
import hashlib

if 'OPENAI_API_TYPE' in os.environ:
    openai.api_type = os.environ['OPENAI_API_TYPE']

if 'OPENAI_API_BASE' in os.environ:
    openai.api_base = os.environ['OPENAI_API_BASE']

if 'OPENAI_API_KEY' in os.environ:
    openai.api_key = os.environ['OPENAI_API_KEY']

if 'OPENAI_API_VERSION' in os.environ:
    openai.api_version = os.environ['OPENAI_API_VERSION']

if 'OPENAI_GPT4_ENGINE' in os.environ:
    openai.gpt4_engine = os.environ['OPENAI_GPT4_ENGINE']

if 'OPENAI_EMBED_ENGINE' in os.environ:
    openai.embed_engine = os.environ['OPENAI_EMBED_ENGINE']


LOG_MESSAGE_HISTORY = False

cocoon_icon_64 = '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'

def call_embed(input_string):

    if openai.api_type == 'azure':
        response = openai.Embedding.create(input=input_string, 
            engine=openai.embed_engine)
        return response
    
    elif openai.api_type == 'open_ai':
        response = openai.Embedding.create(
            model="text-embedding-ada-002",
            input=input_string
        )
        return response


def call_gpt4(messages, temperature=0.1, top_p=0.1):


    if openai.api_type == 'azure':
        response = openai.ChatCompletion.create(
            engine = openai.gpt4_engine,
            temperature=temperature,
            top_p=top_p,
            messages=messages
        )
        return response

    elif openai.api_type == 'open_ai':
        
        response = openai.ChatCompletion.create(
            model="gpt-4-1106-preview",
            temperature=temperature,
            top_p=top_p,
            messages=messages
        )

        return response

    else:
        raise ValueError(f"openai.api_type is {openai.api_type}, but it should be 'azure' or 'openai'.")


def get_source_nodes_ids(nodes, edges, node):
    source_nodes = []

    for edge in edges:
        if edge[1] == node:
            source_nodes.append(edge[0])

    source_node_indices = [nodes.index(n) for n in source_nodes]

    return source_node_indices


class MinHashLSH:
    def __init__(self, num_hashes, num_buckets):
        self.num_hashes = num_hashes
        self.num_buckets = num_buckets
        self.hash_functions = [self._hash_func(i) for i in range(num_hashes)]
        self.hash_table = {i: set() for i in range(num_buckets)}

    def _hash_func(self, seed):
        def hash_func(x):
            hash_object = hashlib.md5((str(seed) + str(x)).encode())
            return int(hash_object.hexdigest(), 16)
        return hash_func

    def _min_hash(self, document):
        min_hash_values = [float('inf')] * self.num_hashes
        for i, hash_func in enumerate(self.hash_functions):
            for word in document:
                hash_value = hash_func(word)
                min_hash_values[i] = min(min_hash_values[i], hash_value)
        return min_hash_values

    def index_document(self, document, doc_id):
        min_hash_values = self._min_hash(document)
        for i, value in enumerate(min_hash_values):
            bucket = (i, value % self.num_buckets)
            self.hash_table[bucket].add(doc_id)

    def query_document(self, query_document):
        min_hash_values = self._min_hash(query_document)
        candidate_sets = [self.hash_table[(i, value % self.num_buckets)] for i, value in enumerate(min_hash_values)]
        intersection_set = set.intersection(*candidate_sets)
        return intersection_set

def wrap_in_scrollable_div(html_code, width='100%'):
    scrollable_div = f'<div style="width: {width}; overflow: auto;">{html_code}</div>'

    return scrollable_div


def wrap_in_iframe(html_code, width=800, height=400):

    escaped_html = html_code.replace("&", "&amp;").replace("<", "&lt;").replace(">", "&gt;").replace("\"", "&quot;").replace("'", "&#039;")

    iframe_code = f"<iframe srcdoc=\"{escaped_html}\" width=\"{width}\" height=\"{height}\" frameborder=\"0\"></iframe>"

    return iframe_code

def plot_missing_values_html(df):
    missing_percent = df.isnull().mean() * 100
    missing_percent = missing_percent[missing_percent > 0]

    if len(missing_percent) == 0:
        return

    plot_width = max(4, len(missing_percent) * 0.5)
    sns.set(style="whitegrid")

    plt.figure(figsize=(plot_width, 3), dpi=100)
    bar_plot = sns.barplot(x=missing_percent.index, y=missing_percent, color="skyblue")

    for index, value in enumerate(missing_percent):
        bar_plot.text(index, value, f'{value:.2f}%', color='black', ha="center", fontsize=8)

    plt.title('Missing Values %', fontsize=8)
    plt.ylabel('%', fontsize=10)
    plt.xticks(rotation=45, fontsize=6)
    plt.yticks(fontsize=8)
    plt.tight_layout()

    buffer = BytesIO()
    plt.savefig(buffer, format='png')
    plt.close()
    buffer.seek(0)
    image_png = buffer.getvalue()

    image_base64 = base64.b64encode(image_png).decode('utf-8')
    html_str = f'<img src="data:image/png;base64,{image_base64}"/>'

    return html_str


def get_tree_html(data):
    def process_data(data):
        keys = list(data.keys())
        if len(keys) == 0:
            return {}

        key = keys[0]
        main_data = data[key]

        children = []
        for sub_key, sub_value in main_data.items():
            if isinstance(sub_value, list):
                items_str = ','.join(map(str, sub_value))

                final_str = '[' + items_str + ']'

                leaf_node = [{"name": final_str}]
                children.append({"name": sub_key, "children": leaf_node})
            else:
                children.append(process_data({sub_key: sub_value}))

        return {"name": key, "children": children}



    def count_leaf_nodes(node):
        """
        Count the number of leaf nodes in a tree represented by nested dictionaries.

        Args:
        node (dict): The tree node, which could be the root of the tree or any sub-node within the tree.

        Returns:
        int: The total number of leaf nodes in the tree from the given node down.
        """
        if not isinstance(node, dict) or 'children' not in node or not node['children']:
            return 1

        leaf_count = 0
        for child in node['children']:
            leaf_count += count_leaf_nodes(child)

        return leaf_count

    def max_path_length(data, ch_size, gap_size):
        def calculate_length(path):
            return sum(len(step) for step in path) * ch_size + (len(path) - 1) * gap_size

        def traverse(node, path=[]):
            if isinstance(node, dict):
                for key, value in node.items():
                    yield from traverse(value, path + [key])
            elif isinstance(node, list):
                for item in node:
                    yield calculate_length(path + [item])

        return max(traverse(data))

    processed_data = process_data(data)

    number_of_leaves = count_leaf_nodes(processed_data)

    html_content = """<div id="tree" style="overflow: scroll;"></div>
<script src="https://d3js.org/d3.v6.min.js"></script>
<style>
    .link {
        fill: none;
        stroke: #555;
        stroke-opacity: 0.4;
        stroke-width: 1.5px;
    }

    .node {
        cursor: pointer;
    }

    .node circle {
        fill: #999;
        stroke: steelblue;
        stroke-width: 1.5px;
    }

    .node text {
        font: 12px sans-serif;
        fill: #555;
    }
</style>
<script>
    const inputData = {'name': 'Patient',
     'children': [{'name': 'Personal Information',
       'children': [{'name': 'Identification',
         'children': [{'name': '[Id,SSN,DRIVERS,PASSPORT]'}]},
        {'name': 'Name',
         'children': [{'name': '[PREFIX,FIRST,LAST,SUFFIX,MAIDEN]'}]},
        {'name': 'Demographics',
         'children': [{'name': '[BIRTHDATE,DEATHDATE,MARITAL,RACE,ETHNICITY,GENDER,BIRTHPLACE]'}]}]},
      {'name': 'Location Information',
       'children': [{'name': 'Address',
         'children': [{'name': '[ADDRESS,CITY,STATE,COUNTY,ZIP]'}]},
        {'name': 'Geolocation', 'children': [{'name': '[LAT,LON,FIPS]'}]}]},
      {'name': 'Healthcare Information',
       'children': [{'name': '[HEALTHCARE_EXPENSES,HEALTHCARE_COVERAGE]'}]},
      {'name': 'Financial Information', 'children': [{'name': '[INCOME]'}]}]};


    // Usage:
    const structuredData = {{data}};  // directly send the inputData without accessing 'Person'


    const treeLayout = d3.tree().size([{{height1}}, {{width1}}]);

    const root = d3.hierarchy(structuredData);
    root.x0 = 0;
    root.y0 = 0;

    // root.children.forEach(collapse);

    // function collapse(d) {
    //     if(d.children) {
    //         d._children = d.children;
    //         d._children.forEach(collapse);
    //         d.children = null;
    //     }
    // }

    const svg = d3.select('#tree').append('svg')
        .attr('width',  {{width2}})
        .attr('height', {{height2}});

    const g = svg.append('g')
        .attr('transform', 'translate(50,50)');

    update(root);

    function update(source) {
        const duration = 750;

        // Re-compute the tree layout
        treeLayout(root);

        // Determine the leftmost node's y-position to adjust the tree positioning
        let leftmostNode = root;
        root.each(d => {
            if (d.y < leftmostNode.y) {
                leftmostNode = d;
            }
        });
        const shiftX = -leftmostNode.y + 50;

        const nodes = root.descendants();
        const links = root.links();

        const node = g.selectAll('.node')
            .data(nodes, d => d.id || (d.id = Math.random()));

        const nodeEnter = node.enter().append('g')
            .attr('class', 'node')
            .attr('transform', d => `translate(${source.y0 + shiftX},${source.x0})`)
            .on('click', click);

        nodeEnter.append('circle')
            .attr('r', 1e-6)
            .style('fill', d => d._children ? 'lightsteelblue' : '#fff');

        nodeEnter.append('text')
            .attr('dy', '0.35em')
            .attr('x', d => d.children || d._children ? -13 : 13)
            .style('text-anchor', d => d.children || d._children ? 'end' : 'start')
            .text(d => d.data.name);

        const nodeUpdate = nodeEnter.merge(node);

        nodeUpdate.transition()
            .duration(duration)
            .attr('transform', d => `translate(${d.y + shiftX},${d.x})`);

        nodeUpdate.select('circle')
            .attr('r', 5)
            .style('fill', d => d._children ? 'lightsteelblue' : '#fff');

        const nodeExit = node.exit().transition()
            .duration(duration)
            .attr('transform', d => `translate(${source.y + shiftX},${source.x})`)
            .remove();

        nodeExit.select('circle')
            .attr('r', 1e-6);

        nodeExit.select('text')
            .style('fill-opacity', 1e-6);

        const link = g.selectAll('.link')
            .data(links, d => d.target.id);

        const linkEnter = link.enter().insert('path', 'g')
            .attr('class', 'link')
            .attr('d', d => {
                const o = { x: source.x0, y: source.y0 };
                return diagonal(o, o);
            });

        const linkUpdate = linkEnter.merge(link);

        linkUpdate.transition()
            .duration(duration)
            .attr('d', d => diagonal(d.source, d.target));

        link.exit().transition()
            .duration(duration)
            .attr('d', d => {
                const o = { x: source.x, y: source.y };
                return diagonal(o, o);
            })
            .remove();


        // Update the positional attributes of each node for the next transition.
        nodes.forEach(d => {
            d.x0 = d.x;
            d.y0 = d.y;
        });

        function diagonal(s, d) {
            const path = `M ${s.y + shiftX} ${s.x}
                C ${(s.y + d.y) / 2 + shiftX} ${s.x},
                ${(s.y + d.y) / 2 + shiftX} ${d.x},
                ${d.y + shiftX} ${d.x}`;

            return path;
        }

    }

    function click(event, d) {
        if (d.children) {
            d._children = d.children;
            d.children = null;
        } else {
            d.children = d._children;
            d._children = null;
        }
        update(d);
    }

</script>"""

    processed_data_str = json.dumps(processed_data, indent=4)

    height1 = number_of_leaves * 30
    height2 = number_of_leaves * 40 + 100
    
    html_content_updated = html_content.replace('{{data}}', processed_data_str)
    html_content_updated = html_content_updated.replace('{{height1}}', str(height1))
    html_content_updated = html_content_updated.replace('{{height2}}', str(height2))
    html_content_updated = html_content_updated.replace('{{width1}}', str(max_path_length(data, 5, 20)))
    html_content_updated = html_content_updated.replace('{{width2}}', str(max_path_length(data, 10, 50) + 200))

    return html_content_updated




def get_rename(meanings):

    column_meanings = '\n'.join([f"  \"{m['column']}\": {m['meaning']}" for m in meanings])

    template = f"""{column_meanings}

    Analyze the column names. The final result as a json file:

    ```json
    [{{
    "column": "column_name" (case sensitive),
    "ambiguous": Given the meaning, is the column name ambguous? Note that it's fine for the name to be domain specific.
    "rename": "" (empty string if not ambiguous)
    }},...]```"""

    messages = [{"role": "user", "content": template}]

    response = call_gpt4(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("---------------------")

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_code = replace_newline(json_code)
    json_code = json.loads(json_code)
            
    def verify_rename(meanings, json_code):

        meanings_columns = set([m['column'] for m in meanings])
        json_code_columns = set([j['column'] for j in json_code])
        if meanings_columns != json_code_columns:
            raise ValueError(f"The set of column names in the meanings and json_code are different: {meanings_columns} vs {json_code_columns}") 

    verify_rename(meanings, json_code)

    return json_code, messages

def topological_sort(nodes, edges):
    from collections import defaultdict

    graph = defaultdict(list)
    for edge in edges:
        graph[edge[0]].append(edge[1])

    visited = set()
    recursion_stack = set()
    result = []

    def dfs(node):
        if node in recursion_stack:
            raise ValueError("Graph is not a DAG (contains a cycle)")
        if node in visited:
            return
        recursion_stack.add(node)
        for neighbor in graph[node]:
            dfs(neighbor)
        result.append(node)
        visited.add(node)
        recursion_stack.remove(node)

    for node in nodes:
        if node not in visited:
            dfs(node)

    return result[::-1]


def show_progress(max_value):
    progress = widgets.IntProgress(
        value=1,
        min=0,
        max=max_value+1,  
        step=1,
        description='',
        bar_style='',
        orientation='horizontal'
    )
    
    display(progress)
    return progress


def give_title(task):
    template = f"""Rename the below task into a title that's as short as possible:
{task}"""

    messages = [{"role": "user", "content": template}]

    response = call_gpt4(messages, temperature=0.1, top_p=0.1)

    title = response['choices'][0]['message']['content']
    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)
    
    for message in messages:
        write_log(message['content'])
        write_log("---------------------")

    return title, messages


def classify_unusual_values(col, unique_values, reason):

    template = f"""Issue: In the '{col}' column, some values are unusual.
Data Values: {unique_values}
Reason: {reason}
Goal: Classify unusual and normal values.

Return the results in json format:
```json
{{
    "reasoning": "The unusual values are ...",
    "unusual_values": ["unusual value",...], (could be empty)
    "normal_values": ["normal value",...] (could be empty)
}}```"""

    messages = [{"role": "user", "content": template}]

    response = call_gpt4(messages, temperature=0.1, top_p=0.1)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("---------------------")


    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_code = replace_newline(json_code)
    json_code = json.loads(json_code)


    def verify_result(json_code, unique_values):

        unique_values = [str(v) for v in unique_values]

        assert json_code['reasoning'] is not None
        assert json_code['unusual_values'] is not None
        assert json_code['normal_values'] is not None

        assert isinstance(json_code['reasoning'], str)
        assert isinstance(json_code['unusual_values'], list)
        assert isinstance(json_code['normal_values'], list)

        assert len(set(json_code['unusual_values']).intersection(set(json_code['normal_values']))) == 0

        json_codes_all = json_code['unusual_values'] + json_code['normal_values']
        json_codes_all = [str(v) for v in json_codes_all]

        assert len(set(unique_values).difference(set(json_codes_all))) == 0 

    verify_result(json_code, unique_values)

    return json_code

def find_regex_pattern(col, result):

    template = f"""Issue: In the '{col}' column, some values are unusual.
Unusual Values: {result['unusual_values']}
Normal Values: {result['normal_values']}

Goal: Identify if there is regular expression that
(1) matches all the normal values, and
(2) does not match any of the unusual values.

There could be multiple regex patterns that satisfy the goal. 
Please understand the meaning of the column, and find one general regex pattern.

Return the results in json format:
```json
{{
"reasoning": "The patterns in the normal values are ...",
"exists_regex": true/false,
"regex": "..."
}}```"""

    messages = [{"role": "user", "content": template}]

    response = call_gpt4(messages, temperature=0.1, top_p=0.1)


    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    for message in messages:
        write_log(message['content'])
        write_log("---------------------")

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    json_code = replace_newline(json_code)
    json_code = json.loads(json_code)



    def verify_json(json_code):
        fields = ['reasoning', 'exists_regex', 'regex']
        for field in fields:
            if field not in json_code:
                raise ValueError(f"Missing field '{field}' in json code.")

        if not isinstance(json_code['reasoning'], str):
            raise ValueError(f"Field 'reasoning' should be string.")
        if not isinstance(json_code['exists_regex'], bool):
            raise ValueError(f"Field 'exists_regex' should be boolean.")
        if not isinstance(json_code['regex'], str):
            raise ValueError(f"Field 'regex' should be string.")

    verify_json(json_code)
    
    if json_code['exists_regex']:

        def verify_regex(regex, normal_values, unusual_values):
            regex = re.compile(regex)
            for value in normal_values:
                if not regex.match(value):
                    raise ValueError(f"Regex does not match normal value: {value}")
            for value in unusual_values:
                if regex.match(value):
                    raise ValueError(f"Regex matches unusual value: {value}")
                
        verify_regex(json_code["regex"], result['normal_values'], result['unusual_values'])

    return json_code

def replace_newline(input_string):
    parts = input_string.split('"')

    for i in range(len(parts)):
        if i % 2 == 1:
            parts[i] = parts[i].replace("\n", "\\n")
            characters = ["d", "t", "b", "r", "f", "D", "w", "W", "s", "S", "B", ".", "(", ")", "[", "]", "{", "}", "|", "?", "*", "+", "^", "$"]
            for character in characters:
                parts[i] = parts[i].replace(f"\\\\{character}", f"\\{character}").replace(f"\\{character}", f"\\\\{character}")

    modified_string = '"'.join(parts)

    return modified_string


def write_log(message: str):
    pass

def display_workflow(nodes, edges):
    dot = Digraph(format='png')

    node_style = {
        'style': 'filled',
        'fillcolor': '#DAE8FC',
        'color': '#6C8EBF',
        'shape': 'box',
        'fontname': 'Helvetica',
        'fontsize': '11',
        'fontcolor': '#2E4057',
        'margin': '0.2,0.05',
        'height': '0.5',
        'width': '0.75'
    }

    edge_style = {
        'color': '#6C8EBF',
        'arrowsize': '0.5',
        'penwidth': '1.0'
    }

    for node in nodes:
        dot.node(node, node, **node_style)

    for tail, head in edges:
        dot.edge(tail, head, **edge_style)

    png_image = dot.pipe()
    display(Image(png_image))

def print_history(history):
    for chat in history:
        print(chat['content'])
        print("---------------------")

def visualize_graph(tables, edges):
    G = nx.Graph()
    G.add_nodes_from(tables)
    G.add_edges_from(edges)

    pos = nx.spring_layout(G)

    nx.draw_networkx_nodes(G, pos, node_color='skyblue',node_size=1200, edgecolors='darkblue', linewidths=1)

    nx.draw_networkx_edges(G, pos, edge_color='darkblue', width=1)

    node_labels = {node:node for node in G.nodes()}
    nx.draw_networkx_labels(G, pos, node_labels, font_size=6)

    edge_labels = nx.get_edge_attributes(G, 'label')
    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='darkblue')

    plt.axis('off')
    plt.gca().set_facecolor('lightgrey')
    plt.gca().grid(which='major', color='white', linestyle='-', linewidth=0.5)
    plt.show()


def plot_distribution(df, column_name):
    """
    This function takes a pandas DataFrame and a column name as input and plots a suitable 
    visualization with labels. For numeric data, it plots a histogram with at most 10 bins. 
    For categorical data, it plots a horizontal bar chart for the top 10 categories, with all others 
    grouped into an 'Other Values' category.
    """
    if column_name not in df.columns:
        print(f"Column '{column_name}' not found in the DataFrame.")
        return

    if pd.api.types.is_numeric_dtype(df[column_name]):
        plt.figure(figsize=(4, 2))
        hist_plot = sns.histplot(df[column_name], bins=min(10, len(df[column_name].dropna().unique())), 
                                 kde=False, color="skyblue")

        hist_plot.xaxis.set_major_formatter(ticker.EngFormatter())

        for p in hist_plot.patches:
            height = p.get_height()
            plt.text(p.get_x() + p.get_width() / 2., height, f'{int(height)}', ha='center', va='bottom', fontsize=8)

        plt.ylabel('Frequency', fontsize=8)
        plt.xlabel('')
    else:
        top_categories = df[column_name].value_counts().head(10)
        others_count = df[column_name].value_counts()[10:].sum()
        
        if others_count > 0:
            other_values_series = pd.Series([others_count], index=['Other Values'])
            top_categories = pd.concat([top_categories, other_values_series])
            
        plt.figure(figsize=(4, 2))
        bar_plot = sns.barplot(y=top_categories.index, x=top_categories, color="skyblue")

        if 'Other Values' in top_categories:
            bar_plot.patches[-1].set_facecolor('orange')

        for index, value in enumerate(top_categories):
            bar_plot.text(value, index, f'{value}', color='black', va='center', fontsize=8)
        
        plt.xlabel('Frequency', fontsize=8)
        plt.ylabel('')

    plt.xticks(fontsize=8)
    plt.yticks(fontsize=8)
    plt.tight_layout()
    plt.show()

def display_html_iframe(html_content, width, height):

    encoded_html = base64.b64encode(html_content.encode()).decode()

    data_uri = f"data:text/html;base64,{encoded_html}"
    
    display(IFrame(src=data_uri, width=width, height=height))


def get_detailed_error_info():
    """
    Retrieve detailed information about the most recent exception caught by an except clause.
    Specifically, it fetches the exact line and explanation of the error within the exec block.
    """
    exc_type, exc_value, exc_traceback = sys.exc_info()

    tb_list = traceback.format_exception(exc_type, exc_value, exc_traceback)

    for i, line in enumerate(tb_list):
        if "exec(" in line:
            tb_list = tb_list[i+1:]
            break

    detailed_error_info = ''.join(tb_list)

    return detailed_error_info

def extract_json_code_safe(s):
    s_stripped = s.strip()
    if (s_stripped.startswith('{') and s_stripped.endswith('}')) or \
       (s_stripped.startswith('[') and s_stripped.endswith(']')):
        return s_stripped

    json_code =  extract_json_code(s_stripped)

    if json_code is not None:
        return json_code

    start_brace = s_stripped.find('{')
    start_bracket = s_stripped.find('[')
    start_index = start_brace if start_bracket == -1 else (start_bracket if start_brace == -1 else min(start_brace, start_bracket))

    if start_index == -1:
        return None

    end_brace = s_stripped.rfind('}')
    end_bracket = s_stripped.rfind(']')
    end_index = end_brace if end_bracket == -1 else (end_bracket if end_brace == -1 else max(end_brace, end_bracket))

    if end_index == -1:
        return None

    potential_json = s_stripped[start_index:end_index + 1]
    return potential_json

def extract_json_code(s):
    import re
    pattern = r"```json(.*?)```"
    match = re.search(pattern, s, re.DOTALL)
    return match.group(1).strip() if match else None

def extract_python_code(s):
    import re
    pattern = r"```python(.*?)```"
    match = re.search(pattern, s, re.DOTALL)
    return match.group(1).strip() if match else None

def collect_df_statistics(df, sample_distinct=20):
    stats = {}

    for col in df.columns:
        dtype = df[col].dtype
        null_percentage = round(df[col].isnull().mean() * 100, 2)

        if np.issubdtype(dtype, np.number):
            unique_values = df[col].nunique()

            hist, bin_edges = np.histogram(df[col].dropna(), bins=sample_distinct)
            histogram = {
                f"[{bin_edges[i]:.2f} - {bin_edges[i + 1]:.2f}]": hist[i] for i in range(sample_distinct)
            }

            stats[col] = {
                "dtype": str(dtype),
                "null_percentage": null_percentage,
                "unique_values": unique_values,
                "histogram": histogram
            }

        else:
            top_values = df[col].value_counts().head(sample_distinct).to_dict()
            other_count = df[col].nunique() - len(top_values)
            if other_count > 0:
                top_values["other"] = other_count

            stats[col] = {
                "dtype": str(dtype),
                "null_percentage": null_percentage,
                "histogram": top_values,
                "unique_values": df[col].nunique()
            }

    return stats

def describe_column(stats, col_name, mention_missing=False, k=5):
    if not col_name in stats:
        raise ValueError(f"Column {col_name} not found in stats.")

    column_stats = stats[col_name]
    description = []

    if pd.api.types.is_numeric_dtype(column_stats["dtype"]) and not pd.api.types.is_bool_dtype(column_stats["dtype"]):
        description.append(f"'{col_name}': numerical")
        min_val = min([float(i.split(' - ')[0][1:]) for i in column_stats["histogram"].keys()])
        max_val = max([float(i.split(' - ')[1][:-1]) for i in column_stats["histogram"].keys()])
        
        description.append(f" with range [{min_val}, {max_val}]")
        

    else:
        description.append(f"'{col_name}': categorical ")
        num_unique = column_stats['unique_values']
        description.append(f"with {num_unique} unique value{'s' if num_unique > 1 else ''}")
        
        histogram = column_stats["histogram"]
        top_k = 	sorted(histogram.items(), key=lambda x: x[1], reverse=True)[:k+1]
        top_k_values = ["'" + str(k[0]) + "'" for k in top_k if k[0] != "other"]
        
        if k >= column_stats['unique_values']:
            description.append(f" {', '.join(top_k_values)}.")
        else:
            description.append(f". E.g., {', '.join(top_k_values)}...")

    if mention_missing:
        null_percentage = column_stats['null_percentage']
        if null_percentage > 0:
            description.append(f"% Missing: {null_percentage}%")
            description.append(f"Contains missing values.")
        else:
            description.append(f"No missing values.")
    
    return "".join(description)

def describe_missing_values(stats, df, threshold=50):
    description = ""
    for col_name in df.columns:
        if stats[col_name]['null_percentage'] > threshold:
            description +=  f"{col_name}: {stats[col_name]['null_percentage']}% missing values\n"
    return description

def describe_df_in_natural_language(df, table_name, num_rows_to_show, num_cols_to_show=None):
    num_rows = len(df)
    num_columns = len(df.columns)
    if table_name != "" and table_name is not None:
        description = f"The table '{table_name}' has {num_rows} rows and {num_columns} columns.\n"
    else:
        description = f"The table has {num_rows} rows and {num_columns} columns.\n"

    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)

    if num_cols_to_show is None:
        num_cols_to_show = num_columns

    if num_rows_to_show > 0:
        first_rows_not_null = df[df.notnull().any(axis=1)]

        first_rows = first_rows_not_null.iloc[:num_rows_to_show, :num_cols_to_show]

        first_rows_str = first_rows.to_csv(index=False, quoting=2) 
        description += f"Here are the first {num_rows_to_show} rows:\n{first_rows_str}"
    else:
        description += f"Here are the columns:\n{df.columns.to_list()}"
    
    return description

def replace_asterisks_with_tags(text):
    """
    Replaces all occurrences of words enclosed in double asterisks with the same words enclosed in <u></u> tags.
    """
    import re

    pattern = r'\*\*(.*?)\*\*'
    replaced_text = re.sub(pattern, r'<u>\1</u>', text)

    return replaced_text


def get_table_summary(main_entity, table_sample):
    """
    Generates a summary of the table.
    """
    template = f"""The table is about recording {main_entity}.
    {table_sample} 

    - Task: Summarize the table attributes in a few sentences.
    -  Highlight: Include and highlight all attributes as **attribute** (capital sensitive). DON'T highlight non-attributes.
    -  Clarity: Use simple, clear, and concise language throughout.
    -  Structure: Start with the big picture, then explain how detailed attributes are related
    Example: The table is about ... at **Time**, in **Location**..."""

    messages = [{"role": "user", "content": template}]

    response = call_gpt4(messages, temperature=0.1, top_p=0.1)

    summary = response['choices'][0]['message']['content']
    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summary, messages

def find_target_table(source_table_description):
    template = f"""You have a source table. All its attributes are highlighted in **bold**.
{source_table_description}

You have a target database, with tables highlighted in **bold**.
The database primarily focuses on healthcare data, structured around several interconnected entities. The central entity is the **PATIENT** table, which contains details about individuals receiving medical care. Their healthcare journey is tracked through the **VISIT_OCCURRENCE** table, which records each visit to a healthcare facility. The **CONDITION_OCCURRENCE** table details any diagnosed conditions during these visits, while the **DRUG_EXPOSURE** table captures information on medications prescribed to the patients.
Procedures performed are logged in the **PROCEDURE_OCCURRENCE** table, and any medical devices used are listed in the **DEVICE** table. The **MEASUREMENT** table records various clinical measurements taken, and the **OBSERVATION** table notes any other relevant clinical observations.
In cases where a patient passes away, the **DEATH** table provides information on the mortality. The **SPECIMEN** table tracks biological samples collected for analysis, and the **COST** table details the financial aspects of the healthcare services.
The **LOCATION**, **CARE_SITE**, and **PROVIDER** tables offer contextual data, respectively detailing the geographical locations, healthcare facilities, and medical professionals involved in patient care. Lastly, the **PAYER_PLAN_PERIOD** table provides information on the patients' insurance coverage details and durations.

Goal: transform source table to target tables.
First repeat source table attributes and reason if they can be mapped to target tables
Then, concisely summarize the targt tables (case sensitive) that can be directly mapped to, and high level reasons (not detailed attributes) in json:
```json
{{  "target table": "xxx concepts in the source matches ...",
...
}}
```"""
    messages = [{"role": "user", "content": template}]
    response = call_gpt4(messages, temperature=0.1, top_p=0.1)

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    summry = json.loads(json_code)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summry, messages

def decide_one_one(source_table_description, source_table_sample, target_table_description, target_table_sample, transform_reason):
    template = f"""Source table:
{source_table_description}
{source_table_sample}
Target table:
{target_table_description}
{target_table_sample}
The source can be transformed to the target table:
{transform_reason}

Task: Understand the Row Mapping. Is it 1:1 or not?
Example of NOT 1:1: 
a. Source row is about student and teacher relationship. Target row in target table is Person. So one row in source table is mapped to two rows in target table.
b. Source row is about one person info (e.g., height, weight, age). Target row in target table is person's all info. So multiple rows in source table is mapped to one row in target table.
Steps:
1. repeat what each row in the source and target table is about.
2. conclude whether the mapping is 1:1 or not in json:
```json
{{  "1:1": true/false,
    "reason": "why 1:1 or not 1:1"
}}
```"""
    messages = [{"role": "user", "content": template}]

    response = call_gpt4(messages, temperature=0.1, top_p=0.95)

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    summry = json.loads(json_code)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summry, messages

def get_concept_mapping(source_table_description, source_table_sample, target_table_description, target_table_sample, transform_reason):
    template = f"""Source table:
{source_table_description}
{source_table_sample}
Target table:
{target_table_description}
{target_table_sample}
The source can be transformed to the target table:
{transform_reason}

Task: Plan for Column Mapping
First repeat each column meaning and find all columns they can be mapped to in target tables.
Then group the similar mapping, and how to map in json:
```json
[  {{"source_columns": ["day", "month", "year"] (case sensitive), 
     "target_columns": ["date"],
     "reason": "Concatenate day, month, year to date"
     }},
]
```"""
    messages = [{"role": "user", "content": template}]

    response = call_gpt4(messages, temperature=0.1, top_p=0.95)

    json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
    summry = json.loads(json_code)

    assistant_message = response['choices'][0]['message']
    messages.append(assistant_message)

    return summry, messages

def write_code_and_debug(key, source_attributes, source_table_description, target_attributes, df, target_table):

    target_attributes = "\n".join(f"{idx + 1}. {target_attribute}: {attributes_description[target_table][target_attribute]}" for idx, target_attribute in enumerate(target_attributes))

    template =  f"""Transform task: Given Source Table, tansform it to Target Table. 

Source table:
{source_table_description}

Target table needs columns:
{target_attributes}

Do the following:
1. First reason about which target columns are transformable, and how to transform. 
2. Then fill in the python function, with detailed comments. Don't change the function name, first line and the return clause.
If no column is transformable, return an empty dataframe.
```python
def transform(input_df):
    output_df = pd.DataFrame()
    ...
    return output_df
```"""
    messages = [{"role": "user", "content": template}]
    
    response = call_gpt4(messages, temperature=0.1, top_p=0.1)
    
    python_code = extract_python_code(response['choices'][0]['message']['content'])

    messages = messages + [response['choices'][0]['message']]

    detailed_error_info = None

    max_tries = 2

    while max_tries > 0:
        max_tries -= 1

        detailed_error_info = None

        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(python_code, globals())
            temp_target_df = transform(df)
        except Exception: 
            detailed_error_info = get_detailed_error_info()

        if detailed_error_info is None:
            return python_code, messages


        error_message = f"""There is a bug in the code: {detailed_error_info}.
First, study the error message and point out the problem.
Then, fix the bug and return the codes in the following format:
```python
def transform(input_df):
    output_df = pd.DataFrame()
    ...
    return output_df
```"""
        messages.append({"role": "user", "content": error_message})

        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        python_code = extract_python_code(response['choices'][0]['message']['content'])
        messages.append(response['choices'][0]['message'])
    
    for message in messages:
        print(message['content'])
        print("---------------------")

    raise Exception("The code is not correct. Please try again.")


def escape_json_string(s):
    s = s.replace("\\", "\\\\")

    s = s.replace("\\\\n", "\\n")
    s = s.replace("\\\\\"", "\\\"")

    return s

def load_template(filename, use_template=True, **kwargs):
    with open(filename, 'r') as file:
        template = file.read()
        if use_template:
            return eval(template, {}, kwargs)
        else:
            return template

def process_dataframe(df, instruction_name, **kwargs):
    message = {
        "role": "user",
        "content": load_template(instruction_name, **kwargs) + 
                   load_template('python_template.txt', use_template=False)
    }
    write_log(message['content'])
    write_log("-----------------------------------")
    
    messages_history = [message]

    max_turns = 10

    while len(messages_history) < max_turns:
        
        response = call_gpt4(messages, temperature=0.1, top_p=0.1)


        assistant_message = response['choices'][0]['message']
        messages_history.append(assistant_message)
        write_log(assistant_message['content'])
        write_log("-----------------------------------")

        processed_string = escape_json_string(assistant_message["content"])
        action_content = json.loads(processed_string)
        action_name = action_content["Action"]["Name"]
        
        if action_name == "Code":
            eda_code = action_content["Action"]["Content"]
            
            exec(eda_code, globals())
            eda_result = code(df)
            
            eda_message = {
                "role": "user",
                "content": f"The EDA codes run successfully:\n\n{eda_result}\n\nPlease keep respond in the required format"
            }
            messages_history.append(eda_message)
            write_log(eda_message['content'])
            write_log("-----------------------------------")

        elif action_name == "Decide":
            continue

        elif action_name == "Conclude":
            return messages_history
        
        else:
            raise Exception("Unknown action")


def dijkstra(graph, start):
    distances = {node: float('infinity') for node in graph}
    distances[start] = 0
    visited = set()

    priority_queue = [(0, start)]

    while priority_queue:
        current_distance, current_node = heapq.heappop(priority_queue)

        if current_node in visited:
            continue

        visited.add(current_node)

        for neighbor, weight in graph[current_node].items():
            distance = current_distance + weight

            if distance < distances[neighbor]:
                distances[neighbor] = distance
                heapq.heappush(priority_queue, (distance, neighbor))

    return distances

def prim(graph):
    min_spanning_tree = []
    visited = set()

    start_node = next(iter(graph))
    visited.add(start_node)

    edge_heap = [(weight, start_node, neighbor) for neighbor, weight in graph[start_node].items()]
    heapq.heapify(edge_heap)

    while edge_heap:
        weight, current_node, neighbor = heapq.heappop(edge_heap)

        if neighbor in visited:
            continue

        min_spanning_tree.append((current_node, neighbor, weight))

        visited.add(neighbor)

        for next_neighbor, next_weight in graph[neighbor].items():
            if next_neighbor not in visited:
                heapq.heappush(edge_heap, (next_weight, neighbor, next_neighbor))

    return min_spanning_tree

def build_kmp_table(pattern):
    m = len(pattern)
    kmp_table = [0] * m
    j = 0

    for i in range(1, m):
        while j > 0 and pattern[i] != pattern[j]:
            j = kmp_table[j - 1]

        if pattern[i] == pattern[j]:
            j += 1

        kmp_table[i] = j

    return kmp_table

def kmp_search(text, pattern):
    n = len(text)
    m = len(pattern)
    kmp_table = build_kmp_table(pattern)
    matches = []

    i, j = 0, 0
    while i < n:
        if pattern[j] == text[i]:
            i += 1
            j += 1

            if j == m:
                matches.append(i - j)
                j = kmp_table[j - 1]
        else:
            if j != 0:
                j = kmp_table[j - 1]
            else:
                i += 1

    return matches

class DocumentedData:
    def __init__(self, df: pd.DataFrame, table_name = None, log_file_path='data_log.txt', display_html=True):
        if not isinstance(df, pd.DataFrame):
            raise ValueError("Input must be a pandas DataFrame.")
        
        self.table_name = table_name
        self.original_df = df.copy()
        self.df = df
        self.document = {}
        self.stats = collect_df_statistics(df)

        idx = 1
        column_desc = ""
        for col in df.columns:
            column_desc += f"{idx}. " + describe_column(self.stats, col) + "\n"
            idx += 1

        self.column_description = column_desc

        self.log_file_path = log_file_path

        self.display_html = display_html
        self.viewer = None

    def describe_missing_values(self,threshold=50):
        description = ""
        first = True
        for col_name in self.df.columns:
            if self.stats[col_name]['null_percentage'] > threshold:
                if first:
                    description += "Percentage of missing values:\n"
                    first = False
                description +=  f"{col_name}: {self.stats[col_name]['null_percentage']}%\n"
        if first:
            description += "None of the columns have missing values."
        return description

    def complete(self):
        clear_output(wait=True)
        print(f"""Congratulation! The document is complete. 

{BOLD}What's next?{END}
1. Use Cleaner  to clean the data.
2. Use Standardizer  to standardize the data.
3. Use Transformer  to transform the data.      
...

And more to come! """)

    def write_log(self, message: str):
        self.log_file = open(self.log_file_path, 'a')
        self.log_file.write(message + '\n')
        self.log_file.close()

    def start_document(self, viewer=None):
        next_step = self.get_main_entity

        self.viewer = viewer
        
        if self.viewer is  None:
            mode_selector = widgets.RadioButtons(
                options=[
                    ('Table Viewer : Im not familiar with the table. Please provide your best guess.', 'Table Viewer'),
                    ('Table Provider : I am ready to provide detailed information about the table.', 'Table Provider')
                ],
                description='Documentation Mode:',
                layout={'width': 'max-content'},
                style={'description_width': 'initial'}
            )


            submit_button = widgets.Button(description="Submit")

            def on_submit(b):
                selected_mode = mode_selector.value
                if selected_mode == 'Table Viewer':
                    self.viewer = True
                else:
                    self.viewer = False

                clear_output(wait=True)

                if selected_mode == 'Table Viewer':
                    print("We got it. Sit back and grab a coffee. It will be done in a few minutes. ")
                next_step()


            submit_button.on_click(on_submit)

            display(mode_selector, submit_button)

        else:
            next_step()

    def save_as_html(self, file_path):
        html_content = self.generate_html()
        with open(file_path, 'w') as file:
            file.write(html_content)

    def generate_html(self):
        middle_html = self.generate_html_source()
        full_html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Cocoon Documentation</title>
    <style>
    /* CSS for the overall appearance: fonts, spacing, etc. */
    body {{
        font-family: "Arial";
        margin: 40px;
        background-color: #d9ead3;  /* Matching the light green background of the icon */
    }}

    .container {{
        max-width: 900px;  /* Setting a maximum width for the content */
        margin: 0 auto;   /* Centering the container */
        background-color: #ffffff; /* White background for the main content */
        padding: 20px;
        border-radius: 5px; /* Rounded corners */
        box-shadow: 0 0 10px rgba(0,0,0,0.1); /* A subtle shadow to lift the container */
    }}

    h1 {{
        font-size: 24px;
        color: #274e13; /* Dark green color matching the icon */
        padding-bottom: 10px;
        margin-bottom: 20px;
    }}

    h2 {{
        font-size: 20px;
        margin-top: 20px;
        margin-bottom: 15px;
    }}

    ul, ol {{
        padding-left: 20px;
    }}

    li {{
        margin-bottom: 10px;    }}

    p {{        margin-bottom: 20px;
        text-align: justify; /* To align the text on both sides */
    }}

    /* CSS specific to the table elements */
    table {{
        font-family: Arial, sans-serif;
        border-collapse: collapse; /* Ensures there are no double borders */
        width: 100%;
        table-layout: auto;
    }}

    th, td {{
        border: 1px solid #dddddd; /* Light grey borders */
        text-align: left;
        padding: 8px; /* Make text not touch the borders */
    }}

    th {{
        background-color: #b6d7a8; /* Light green background matching the icon */
    }}

    tr:nth-child(even) {{
        background-color: #d9ead3; /* Light green background matching the icon */
    }}

    tr:hover {{
        background-color: #c9d9b3; /* A slightly darker green for hover effect */
    }}
    </style>
</head>
<body>
    <div class="container">
        {middle_html}
    </div>
</body>
</html>"""
        return full_html

    def generate_html_source(self):
        html_content_updated = f'<div style="display: flex; align-items: center;">' \
                    f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
                    f'<div style="margin: 0; padding: 0;">' \
                    f'<h1 style="margin: 0; padding: 0;">Table Documentation</h1>' \
                    f'<p style="margin: 0; padding: 0;">Powered by cocoon</p>' \
                    f'</div>' \
                    f'</div><hr>'

        html_content_updated += '<h2> Table Sample (first 5 rows):</h2>'
        html_content_updated += wrap_in_scrollable_div(self.df[:5].to_html(), width='800')
        html_content_updated += '<hr>'

        if "main_entity" in self.document and "table_summary" in self.document and "column_grouping" in self.document:
            main_entity = self.document['main_entity']['summary']
            reason = self.document['table_summary']['summary']
            reason = replace_asterisks_with_tags(reason)
            html_content_updated += '<h2> Table Summary:</h2>'
            html_content_updated += f'<b> The table is mainly about: </b>{main_entity}<br><br>'
            html_content_updated += f'<b>Details</b>: {reason}<br>'
            html_content_updated += '<br><b> Attribute Hierarchy</b><br>'

            group_data = self.document['column_grouping']['summary']
            html_content_updated += wrap_in_iframe(get_tree_html(group_data))
            html_content_updated += '<hr>'

        if "missing_value" in self.document:
            if any(self.df.isnull().mean() > 0):
                html_content_updated += f'<h2> Missing Values </h2>'
                html_content_updated += plot_missing_values_html(self.df)
                html_content_updated += '<br>'
                html_content_updated += self.generate_missing_values_report()
                html_content_updated += '<hr>'

        if "unusual" in self.document:
            exist_unusual = False

            for key in self.document['unusual']:
                value = self.document['unusual'][key]['summary']['Unusualness']
                if value:
                    exist_unusual = True
                    break
            
            if exist_unusual:
                html_content_updated +=f'<h2> Unusual Values </h2>'
                html_content_updated += self.generate_unusual_values_report()
                html_content_updated += '<hr>'

        return html_content_updated

    def display_document(self):
        currnet_seq = 1

        html_content_updated = f'<div style="display: flex; align-items: center;">' \
                    f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
                    f'<div style="margin: 0; padding: 0;">' \
                    f'<h1 style="margin: 0; padding: 0;">Table Documentation</h1>' \
                    f'<p style="margin: 0; padding: 0;">Powered by cocoon</p>' \
                    f'</div>' \
                    f'</div><hr>'

        html_content_updated += '<h2> Table Sample (first 5 rows):</h2>'
        html_content_updated += self.df[:5].to_html()
        html_content_updated += '<hr>'
        display(HTML(html_content_updated))

        if "main_entity" in self.document and "table_summary" in self.document and "column_grouping" in self.document:
            main_entity = self.document['main_entity']['summary']
            reason = self.document['table_summary']['summary']
            reason = replace_asterisks_with_tags(reason)
            html_content_updated = '<h2> Table Summary:</h2>'
            html_content_updated += f'<b> The table is mainly about: </b>{main_entity}<br>'
            html_content_updated += f'<b>Details</b>: {reason}<br>'
            html_content_updated += '<b> Attribute Hierarchy</b>'
            display(HTML(html_content_updated))
            group_data = self.document['column_grouping']['summary']
            self.display_tree(group_data)
            display(HTML('<hr>'))

        if "missing_value" in self.document:
            if any(self.df.isnull().mean() > 0):
                display(HTML(f'<h2> Missing Values </h2>'))
                self.plot_missing_values_compact()
                self.generate_html_missing_values_report()
                display(HTML('<hr>'))

        if "unusual" in self.document:
            exist_unusual = False

            for key in self.document['unusual']:
                value = self.document['unusual'][key]['summary']['Unusualness']
                if value:
                    exist_unusual = True
                    break
            
            if exist_unusual:
                display(HTML(f'<h2> Unusual Values </h2>'))
                self.generate_html_unusual_values_report()
                display(HTML('<hr>'))

    def generate_html_missing_values_report(self):

        html_output = self.generate_missing_values_report()

        display(HTML(html_output))

    def generate_missing_values_report(self):
        html_output = "<b>Reasons for Missing Values </b> <br><ul>"
        missing_values_dict = self.document['missing_value']
        for column, details in missing_values_dict.items():
            
            if 'summary' in details:
                html_output += f"<li><strong>{column}</strong>: "
                html_output += details['summary']
                html_output += "</li>"

        html_output += "</ul>"
        return html_output

    def generate_unusual_values_report(self):
        unusual_values_dict = self.document['unusual']
        html_output = ""
        for column, details in unusual_values_dict.items():
            
            if details['summary']['Unusualness']:
                html_output += f"<li><strong>{column}</strong>: "
                html_output += details['summary']['Examples']
                html_output += f" <strong>Explanation</strong>: "
                html_output += details['summary']['Explanation']
                html_output += "</li>"

        html_output += "</ul>"
        return html_output


    def generate_html_unusual_values_report(self):
        
        html_output = self.generate_unusual_values_report()
        display(HTML(html_output))

    def plot_missing_values_compact(self):
        """
        This function takes a pandas DataFrame as input and plots a compact bar chart showing
        the percentage of missing values for each column that has missing values.
        """
        df = self.df

        missing_percent = df.isnull().mean() * 100

        missing_percent = missing_percent[missing_percent > 0]
        
        if len(missing_percent) == 0:
            return

        plot_width = max(4, len(missing_percent) * 0.5)

        sns.set(style="whitegrid")    

        plt.figure(figsize=(plot_width, 3), dpi=100)
        bar_plot = sns.barplot(x=missing_percent.index, y=missing_percent, color="skyblue")

        for index, value in enumerate(missing_percent):
            bar_plot.text(index, value, f'{value:.2f}%', color='black', ha="center", fontsize=8)

        plt.title('Missing Values %', fontsize=8)
        plt.ylabel('%', fontsize=10)
        plt.xticks(rotation=45, fontsize=6)
        plt.yticks(fontsize=8)
        plt.tight_layout()
        plt.show()

    def get_column_warnings(self, col):
        warnings = []
        if "consistency" in self.document:
            if col in self.document["consistency"]:
                summary = self.document["consistency"][col]["summary"]
                if summary["Inconsitencies"] :
                    warnings.append({"type": "Value Consistency", 
                                    "explanation": "There are inconsistent values: " + summary["Examples"],
                                    "solution": ["Proceed with the transformation as is",
                                                "(not implemented) Clean the inconsistent values",]})
        
        if "missing_value" in self.document:
            if col in self.document["missing_value"]:
                if self.document["missing_value"][col]:
                    warnings.append({"type": "Missing Value", 
                                    "explanation": "There are missing values: " + self.document["missing_value"][col]["summary"][0],
                                    "solution": ["Proceed with the transformation as is",
                                                "Remove the rows with missing values",
                                                "(not implemented) Clean the inconsistent values",]})    
        
        if "unusual" in self.document:
            if col in self.document["unusual"]:
                summary  = self.document["unusual"][col]["summary"]
                if summary["Unusualness"]:
                    warnings.append({"type": "Unusual Value",
                                        "explanation": "There are unusual values: " + summary["Examples"],
                                        "solution": ["Proceed with the transformation as is",
                                                    "(not implemented) Clean the unusual values",]})

        return warnings
    
    def get_sample_text(self, sample_cols=None, sample_size=2, col_size=None):
        if sample_cols is None:
            sample_cols = self.df.columns
        return describe_df_in_natural_language(self.df[sample_cols], self.table_name, sample_size, num_cols_to_show=col_size)   

    def get_basic_description(self, sample_size=2, cols = None, sample_cols=None):

        if cols is None:
            cols = self.df.columns

        table_sample = self.get_sample_text(sample_cols=sample_cols, sample_size=sample_size)
        
        column_desc = ""

        idx=1
        for col in cols:
            column_desc += f"{idx}. " + describe_column(self.stats, col) + "\n"
            idx += 1

        return table_sample + "\n\nColumn details:\n" + column_desc

    def show_progress(self, max_value):
        progress = widgets.IntProgress(
            value=1,
            min=0,
            max=max_value+1,  
            step=1,
            description='',
            bar_style='',
            orientation='horizontal'
        )
        
        display(progress)
        return progress

    def display_tree(self, data):

        if self.display_html:
            html_content_updated = get_tree_html(data)

            display_html_iframe(html_content_updated, width=800, height=400)
        else:

            import plotly.graph_objects as go

            def extract_hierarchy(data, parent_name='', hierarchy=None):
                if hierarchy is None:
                    hierarchy = {'names': [], 'parents': []}
                
                for name, children in data.items():
                    hierarchy['names'].append(name)
                    hierarchy['parents'].append(parent_name)
                    if isinstance(children, dict):
                        extract_hierarchy(children, parent_name=name, hierarchy=hierarchy)
                    elif isinstance(children, list):
                        for child in children:
                            hierarchy['names'].append(child)
                            hierarchy['parents'].append(name)
                return hierarchy

            hierarchy = extract_hierarchy(data)

            ids = list(range(1, len(hierarchy['names']) + 1))
            name_to_id = {name: id for id, name in zip(ids, hierarchy['names'])}
            parent_ids = [name_to_id[parent] if parent in name_to_id else '' for parent in hierarchy['parents']]

            fig = go.Figure(go.Treemap(
                ids=ids,
                labels=hierarchy['names'],
                parents=parent_ids
            ))

            fig.update_layout(width=600, height=400, margin = dict(t=0, l=0, r=0, b=0))

            fig.show()

    def get_table_summary(self, overwrite=False, once=False):
        next_step = self.get_column_grouping
        if "table_summary" not in self.document:
            self.document["table_summary"] = {}
        else:
            if self.document["table_summary"] and not overwrite:
                write_log("Warning: table_summary already exists in the document.")
                if not once:
                    next_step()
                return

        print(" Generating table summary based on renamed attributes...")

        progress = self.show_progress(1)

        main_entity = self.document["main_entity"]["summary"]
        table_sample = self.get_sample_text()

        max_trials = 3

        while max_trials > 0:
            try:
                summary, messages = get_table_summary(main_entity, table_sample)
                progress.value += 1

                for message in messages:
                    write_log(message['content'])
                    write_log("-----------------------------------")

                def extract_words_in_asterisks(text):
                    """
                    Extracts and returns all words enclosed in double asterisks in the given text.
                    """
                    import re

                    pattern = r'\*\*(.*?)\*\*'

                    matches = re.findall(pattern, text)

                    return matches
                    
                def check_sets_equality(list1, list2):
                    """
                    Checks if two lists have the same set of elements, regardless of order.
                    """
                    if not set(list1) == set(list2):
                        raise ValueError(f"""The two lists do not have the same set of elements.
                List 1: {sorted(list1)} 
                List 2: {sorted(list2)}""")

                columns = self.df.columns.to_list()

                check_sets_equality(extract_words_in_asterisks(summary), columns)

                break

            except Exception as e:
                write_log(f"Error: {e}")
                write_log("-----------------------------------")
                max_trials -= 1
                if max_trials == 0:
                    raise e
                continue

        self.document["table_summary"]["summary"] = summary
        if LOG_MESSAGE_HISTORY:
            self.document["table_summary"]["history"] = messages


        html = f"<b>Table Summary</b><br>{replace_asterisks_with_tags(summary)}"
        display(HTML(html))

        def on_button_clicked(b):
            clear_output(wait=True)
            print("Submission received.")
            next_step()

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='',
            tooltip='Click to submit',
            icon='check'
        )

        submit_button.on_click(on_button_clicked)

        display(submit_button)
        
        if self.viewer:
            on_button_clicked(submit_button)

    def get_column_grouping(self, overwrite=False, once=False):
        next_step = self.check_missing_all
        if "column_grouping" not in self.document:
            self.document["column_grouping"] = {}
        else:
            if self.document["column_grouping"] and not overwrite:
                write_log("Warning: column_grouping already exists in the document.")
                if not once:
                    next_step()
                return

        print(" Building concept map...")

        progress = self.show_progress(1)

        meanings = self.document["column_meaning"]["summary"]

        column_meanings = '\n'.join([f"- {m['column']}: {m['meaning']}" for m in meanings])

        main_entity = self.document["main_entity"]["summary"]

        sample = self.get_sample_text()

        template = f"""{sample}

{column_meanings}

This table is about {main_entity}. The goal is to build a mind map. Do the following step-by-step:

1. Discuss, at a high level, what are the different aspects of the main entities in this table.
2. Recursively group the attributes based on inherent entity association, not conceptual similarity.
    E.g., for [student name, student grade, teacher grade], group by student and teacher, not by name.
   Avoid groups with too many/few subgroups. 
3. Cluster related attributes into final leaf group, e.g.,
    'Name': ['First Name', 'Last Name']
    'Birth Date': ['Birth Day', 'Birth Month', 'Birth Year']
    'Residence Address': ['Street', 'City', 'State', 'Zip Code']   
    Attributes are Case Sensitive!
3. Conclude with the final result as a multi-level JSON. Make sure all attributes are included.

```json
{{
    "{main_entity}":
        {{
        "Sub group": {{
        "sub-sub group": ["attribute1", "attribute2", ...],
        }},
    }}
}}
```"""
        
        def extract_attributes(json_var):
            attributes = []
            
            def traverse(element):
                if isinstance(element, dict):
                    for value in element.values():
                        traverse(value)
                        
                elif isinstance(element, list):
                    for item in element:
                        if isinstance(item, str):
                            attributes.append(item)
                        
                        else:
                            traverse(item)
                            

            traverse(json_var)
            
            return attributes

        def validate_attributes(attributes, reference_attributes):
            error_messages = []

            seen_attributes = set()
            duplicates = set()
            for attribute in attributes:
                if attribute in seen_attributes:
                    duplicates.add(attribute)
                seen_attributes.add(attribute)
            
            if duplicates:
                error_messages.append("Duplicate attributes: " + ', '.join(duplicates))

            attributes_set = set(attributes)
            reference_set = set(reference_attributes)

            extra_attributes = attributes_set - reference_set
            if extra_attributes:
                error_messages.append("Extra attributes: " + ', '.join(extra_attributes))

            missing_attributes = reference_set - attributes_set
            if missing_attributes:
                error_messages.append("Missing attributes: " + ', '.join(missing_attributes))

            return '\n'.join(error_messages)

        

        def build_concept_map_and_verify(messages):
            
            number_of_trials = 3

            for messgae in messages:
                write_log(messgae['content'])
                write_log("-----------------------------------")

            while number_of_trials > 0:
                response = call_gpt4(messages, temperature=0.1, top_p=0.1)
                
                write_log(response['choices'][0]['message']['content'])
                write_log("-----------------------------------")

                assistant_message = response['choices'][0]['message']
                json_code = extract_json_code_safe(assistant_message['content'])
                json_code = json_code.replace('\'', '\"')
                json_var = json.loads(json_code)
                attributes = extract_attributes(json_var)

                messages.append(assistant_message)

                error = validate_attributes(attributes, self.df.columns)

                if error!= '':
                    error_message = {
                        "role": "user",
                        "content": f"{error}\nPlease correct your answer and return the json in the required format."
                    }
                    messages.append(error_message)
                    write_log(error_message['content'])
                    write_log("-----------------------------------")

                    number_of_trials -= 1
                
                else:
                    self.document["column_grouping"]["summary"] = json_var
                    if LOG_MESSAGE_HISTORY:
                        if "history" not in self.document["column_grouping"]:
                            self.document["column_grouping"]["history"] = messages
                        else:
                            self.document["column_grouping"]["history"] += messages
                    break


        messages =[ {"role": "user", "content": template}]
        build_concept_map_and_verify(messages)
        data = self.document["column_grouping"]["summary"]
        
        

        progress.value += 1
        
        clear_output(wait=True)
        
        self.display_tree(data)

        def create_widgets_for_column_grouping():
            def on_value_change(change):
                if change['new'] == 'No':
                    feedback_container.layout.display = ''
                    text_area.disabled = False
                else:
                    feedback_container.layout.display = 'none'
                    text_area.disabled = True

            accuracy_question_label = widgets.Label(value='Is the mind map accurate?')

            accuracy_check = widgets.RadioButtons(
                options=['Yes', 'No'],
                description='',
                disabled=False
            )

            label = widgets.Label(value='If not accurate, how to fix it?')

            text_area = widgets.Textarea(
                value='',
                placeholder='Type here',
                description='',
                disabled=True
            )

            feedback_container = widgets.VBox([label, text_area], layout=Layout(display='none'))

            submit_button = widgets.Button(
                description='Submit',
                disabled=False,
                button_style='',
                tooltip='Click to submit',
                icon='check'
            )

            accuracy_check.observe(on_value_change, names='value')

            def on_button_clicked(b):
                if accuracy_check.value == 'No':
                    if text_area.value == '':
                        print("\033[91mPlease enter the information\033[0m.")
                        return
                    feedback = text_area.value

                    print(" Refining concept map...")
                    progress = self.show_progress(1)

                    data = self.document["column_grouping"]["summary"]

                    messages =[ {"role": "user", "content": template}]
                    messages.append({"role": "system", "content": "```json\n"+str(data)+"\n```"})
                    messages.append({"role": "user", 
                                    "content": f"""{feedback} Please refine the json and return the result within ```json``` block."""})
                    
                    build_concept_map_and_verify(messages)
                    clear_output(wait=True)
                    data = self.document["column_grouping"]["summary"]
                    self.display_tree(data)
                    accuracy_question = create_widgets_for_column_grouping()
                    display(accuracy_question)
                    
                else:
                    clear_output(wait=True)
                    print("Submission received.")
                    next_step()

            submit_button.on_click(on_button_clicked)

            accuracy_question = widgets.VBox([accuracy_question_label, accuracy_check, feedback_container, submit_button])

            return accuracy_question, on_button_clicked

        accuracy_question, on_button_clicked = create_widgets_for_column_grouping()

        display(accuracy_question)

        if self.viewer:
            on_button_clicked(None)


    def execute_rename_column(self):
        next_step = self.get_table_summary
        rename_summary = self.document["rename_column"]["summary"]
        temp_name_mapping = {}
        final_name_mapping = {}

        self.name_mapping = {}
        
        

        for item in rename_summary:
            if item['rename'] != '':
                print(f"Renaming column {item['column']} to {item['rename']}")
                self.name_mapping[item['column']] = item['rename']
            else:
                self.name_mapping[item['column']] = item['column']

        self.rename_step = ColumnRename(sample_df = self.df[:2], rename_map = self.name_mapping )

        self.df = self.rename_step.run_codes(self.df)




        self.stats = collect_df_statistics(self.df)
        
        next_step()

    def generate_pipeline(self):
        source = SourceStep(self)
        edges = [(source.name, self.rename_step.name)]
        pipeline = TransformationPipeline(steps = [source, self.rename_step], edges=edges)
        return pipeline

    def rename_column(self, overwrite=False, once=False):
        next_step = self.execute_rename_column
        if "rename_column" not in self.document:
            self.document["rename_column"] = {}
        else:
            if self.document["rename_column"] and not overwrite:
                write_log("Warning: rename_column already exists in the document.")
                if not once:
                    next_step()
                return

        print(" Renaming the columns...")
        progress = self.show_progress(1)

        meanings = self.document["column_meaning"]["summary"]

        max_trials = 3

        while max_trials > 0:
            try:
                json_code, messages = get_rename(meanings)
                progress.value += 1
                break
                
            except Exception as e:
                write_log(f"Error: {e}")
                write_log("-----------------------------------")
                max_trials -= 1
                if max_trials == 0:
                    raise e
                continue






        

        self.document["rename_column"]["summary"] = json_code
        if LOG_MESSAGE_HISTORY:
            self.document["rename_column"]["history"] = messages

        next_step()                                    

    def get_column_meaning(self, overwrite=False, once=False):
        next_step = self.rename_column
        if "column_meaning" not in self.document:
            self.document["column_meaning"] = {}
        else:
            if self.document["column_meaning"] and not overwrite:
                write_log("Warning: column_meaning already exists in the document.")
                if not once:
                    next_step()
                return
        
        print(" Understanding the columns...")
        progress = self.show_progress(1)

        main_entity = self.document["main_entity"]["summary"]
        basic_description = self.get_basic_description()

        template = f"""{basic_description}

This table is about {main_entity}. The goal is study the high-level column meaning.
Please use simple words to describe the possible (potentially ambiguous) meanings.
Don't include ambiguous meanings that are nuanced or detailed properties.

Example for good ambiguous meanings: 
column "age" can mean "age of the person" or "age of the company".

Example for bad ambiguous meanings:
column "age" can mean "chronological age in years" or "age in months".
This is bad because both, at a high level, mean the same thing. The difference is only in the detailed calculation.

Return the final result as a json file:

```json
[{{
   "column": "column_name",
   "meaning": "your best guess on the general meaning",
   "ambiguous": ["other possible general meanings"] 
               (empty list if not ambiguous. don't include meanings whose difference is only in the details),
}},...]
```"""
        
        max_trials = 3

        while max_trials > 0:
            try:
                messages = [{"role": "user", "content": template}]

                response = call_gpt4(messages, temperature=0.1, top_p=0.1)

                progress.value += 1

                write_log(template)
                write_log("-----------------------------------")
                write_log(response['choices'][0]['message']['content'])
                
                json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
                data = json.loads(json_code)

                messages.append(response['choices'][0]['message'])

                def check_column_complete(data):
                    if len(data) != len(self.df.columns):
                        raise Exception("Not all columns are covered in the column meaning.")

                    for item in data:
                        if item['column'] not in self.df.columns:
                            raise Exception(f"Column {item['column']} does not exist in the table.")
                
                check_column_complete(data)

                self.document["column_meaning"]["summary"] = data
                if LOG_MESSAGE_HISTORY:
                    self.document["column_meaning"]["history"] = messages

                break
            
            except Exception as e:
                write_log(f"Error: {e}")
                write_log("-----------------------------------")
                max_trials -= 1
                if max_trials == 0:
                    raise e
                continue

        clear_output(wait=True)

        def radio_change_handler(change):
            instance = change['owner']
            if instance.value == 'Other':
                instance.text_area.layout.display = ''
            else:
                instance.text_area.layout.display = 'none'

        container = widgets.VBox()

        column_to_radio = {}

        for item in data:
            if item['ambiguous']:
                label_text = f"<b>{item['column']}</b>: <span style='color: red;'>(This column has ambiguous interpretations)</span>"
            else:
                label_text = f"<b>{item['column']}</b>:"

            label = widgets.HTML(
                value=label_text, 
                layout=widgets.Layout(margin='0 0 10px 0')
            )
            
            options = [item['meaning']] +  item['ambiguous'] + ['Other']
            radio = widgets.RadioButtons(
                options=options,
                value=item['meaning'],
                layout=widgets.Layout(width='80%', align_items='flex-start')
            )
            
            text_area = widgets.Textarea(
                value='',
                placeholder='Please provide the meaning of the column.',
                description='',
                disabled=False,
                layout=widgets.Layout(display='none', width='100%')
            )
            
            radio.text_area = text_area
            column_to_radio[item['column']] = radio
            radio.observe(radio_change_handler, names='value')
            
            container.children += (label, radio, text_area)

        def submit_callback(btn):
            error_items = []
            
            for item in data:
                radio = column_to_radio[item['column']] 
                if radio.value == 'Other' and not radio.text_area.value.strip():
                    error_items.append(item)
            
            if error_items:
                clear_output(wait=True)
                display(container, submit_btn)
                for item in error_items:
                    print(f"\033[91m{item['column']} meaning can't be empty.\033[0m")
            else:
                clear_output(wait=True)
                feedback_data = []

                for item in data:
                    radio = column_to_radio[item['column']] 
                    
                    if radio.value == 'Other':
                        feedback_data.append({'column': item['column'], 'meaning': radio.text_area.value})
                    else:
                        feedback_data.append({'column': item['column'], 'meaning': radio.value})
                self.document["column_meaning"]["summary"] = feedback_data
                if LOG_MESSAGE_HISTORY:
                    self.document["column_meaning"]["history"].append({"role": "user",
                                                                    "content": feedback_data})
                print("Submission received.")
                next_step()
                                                                          

                        
        submit_btn = widgets.Button(
            description="Submit",
            button_style='',
            tooltip='Submit',
            icon=''
        )

        submit_btn.on_click(submit_callback)

        display(container, submit_btn)

        if self.viewer:
            submit_callback(submit_btn)

    
    def get_main_entity(self, overwrite=False, once=False):

        next_step = self.get_column_meaning
        
        if "main_entity" not in self.document:
            self.document["main_entity"] = {}
        else:
            if self.document["main_entity"] and not overwrite:
                write_log("Warning: main_entity already exists in the document.")
                if not once:
                    next_step()
                return

        print(" Understanding the table...")
        progress = self.show_progress(1)

        basic_description = self.get_basic_description()

        template = f"""{basic_description}

Identify the main entity this table is mainly recording. 
-   Entity is tangible or conceptual thing that can exist or be conceptualized individually.
    Example of Entity: Person, Course, Location, Time, School, Job
-   Entity is not property, or aspect. Infer the main underlying entities. 
    Example of Non-entity: Height, Weight, Information, Name, Finance
    For height, the main underlying entity is "People"
-   Entity shall be a clear single phrase. Don't use / to combine entities.

1. Start by reasoning what the table is about, and the candidate main entities.
   If there are multiple candidate main entities, discuss if there is a main relationship entity that can be used to group them.
2. Conclude by listing the main entity. 

Now respond in the following format:
```json
{{
    "reasoning": "The table is about ...",
    "main entity": "..."
}}
```"""

        messages = [{"role": "user", "content": template}]
        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        progress.value += 1
        
        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])
        
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        main_entity = json_code['main entity']

        messages.append(response['choices'][0]['message'])
        self.document["main_entity"]["summary"] = main_entity
        self.document["main_entity"]["reasoning"] = json_code['reasoning']
        if LOG_MESSAGE_HISTORY:
            self.document["main_entity"]["history"] = messages

        clear_output(wait=True)
        reason = json_code['reasoning']
        print(f'\033[1mThe table is mainly talking about: \033[0m{main_entity}')
        print(f'\033[1mDetails\033[0m: {reason}')

        accuracy_question_label = widgets.Label(value='Is the above information accurate?')

        accuracy_check = widgets.RadioButtons(
            options=['Yes', 'No'],
            description='',
            disabled=False
        )

        label = widgets.Label(value='If not accurate, the table is mainly talking about:')

        text_area = widgets.Textarea(
            value='',
            placeholder='Type here',
            description='',
            disabled=True
        )

        feedback_container = widgets.VBox([label, text_area], layout=Layout(display='none'))

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='',
            tooltip='Click to submit',
            icon='check'
        )

        def on_button_clicked(b):
            clear_output(wait=True)
            if accuracy_check.value == 'No':
                if text_area.value == '':
                    display(accuracy_question)
                    print("\033[91mPlease enter the information\033[0m.")
                    return
                corrected_entity = text_area.value
                
                print(f"Corrected information received. This table is mainly about {corrected_entity}")
                self.document["main_entity"]["summary"] = corrected_entity
                if LOG_MESSAGE_HISTORY:
                    self.document["main_entity"]["history"].append({"role": "user", 
                                                                "content": template})
            else:
                print("Submission received.")
                next_step()

        def on_value_change(change):
            if change['new'] == 'No':
                feedback_container.layout.display = ''
                text_area.disabled = False
            else:
                feedback_container.layout.display = 'none'
                text_area.disabled = True

        accuracy_check.observe(on_value_change, names='value')

        submit_button.on_click(on_button_clicked)

        accuracy_question = widgets.VBox([accuracy_question_label, accuracy_check, feedback_container, submit_button])

        display(accuracy_question)

        if self.viewer:
            on_button_clicked(submit_button)


    def document_all(self):
        self.get_main_entity()
        self.get_column_grouping()
        self.check_consistency_all()
        self.check_pattern_all()
        self.check_missing_all()
        self.check_unusual_all()

    def check_missing_all(self):
        next_step = self.check_unusual_all
        print(" Checking the missing values...")
        progress = self.show_progress(len(self.df.columns))

        for col in self.df.columns:
            self.check_missing(col)

            progress.value += 1

        ambiguous_missing = {}
        
        for col in self.document["missing_value"]:
            if "summary" in self.document["missing_value"][col]:
                summary = self.document["missing_value"][col]["summary"]
                if isinstance(summary, list):
                    ambiguous_missing[col] = summary

        if not ambiguous_missing:
            next_step()
            return

        clear_output(wait=True)
        print("The following columns have missing values: ")

        def radio_change_handler(change):
            instance = change['owner']
            if instance.value == 'Other':
                instance.text_area.layout.display = ''
            else:
                instance.text_area.layout.display = 'none'

        container = widgets.VBox()


        col_to_radio = {}

        for item in ambiguous_missing:
            
            label_text = f"<b>{item}</b>"

            label = widgets.HTML(value=label_text)

            reasons = ambiguous_missing[item]
            options = [f"{reason['class']}: {reason['explanation']}" for reason in reasons] + ['Unclear','Other']

            radio = widgets.RadioButtons(
                options=options,
                value=options[0],
                layout=widgets.Layout(width='80%', align_items='flex-start')
            )
            
            text_area = widgets.Textarea(
                value='',
                placeholder='Please provide the reason for missing values.',
                description='',
                disabled=False,
                layout=widgets.Layout(display='none', width='100%')
            )
            
            radio.text_area = text_area
            col_to_radio[item] = radio
            radio.observe(radio_change_handler, names='value')
            
            item_container = widgets.VBox([label, radio, text_area])
            container.children += (item_container,)

        def submit_callback(btn):
            error_items = []
            
            for col in col_to_radio:
                radio = col_to_radio[col]
                if radio.value == 'Other' and not radio.text_area.value.strip():
                    error_items.append(col)
            
            if error_items:
                for col in error_items:
                    print(f"\033[91m{col} reason can't be empty.\033[0m")
            else:
                clear_output(wait=True)

                for col in col_to_radio:
                    radio = col_to_radio[col]
                    
                    if radio.value == 'Other':
                        self.document["missing_value"][col]["summary"] = radio.text_area.value
                        if LOG_MESSAGE_HISTORY:
                            self.document["missing_value"][col]["history"].append({"role": "user",
                                                                                            "content": radio.text_area.value})
                    else:
                        self.document["missing_value"][col]["summary"] = radio.value
                        if LOG_MESSAGE_HISTORY:
                            self.document["missing_value"][col]["history"].append({"role": "user",
                                                                                     "content": radio.value})
                
                print("Submission received.")
                next_step()
                                                                          

                        
        submit_btn = widgets.Button(
            description="Submit",
            button_style='',
            tooltip='Submit',
            icon=''
        )

        submit_btn.on_click(submit_callback)

        display(container, submit_btn)

        if self.viewer:
            submit_callback(submit_btn)

        
        

    def check_missing(self, column: str):
        if column not in self.df.columns:
            raise ValueError(f"Column {column} does not exist in the DataFrame.")
        
        if "missing_value" not in self.document:
            self.document["missing_value"] = {}

        if column not in self.document["missing_value"]:
            self.document["missing_value"][column] = {}
        else:
            if self.document["missing_value"][column]:
                write_log(f"Warning: {column} already exists in the document.")
                return
        
        nan_rows = self.df[self.df[column].isna()]
        non_nan_rows = self.df.dropna(subset=[column])

        nan_sample = nan_rows.head(3)
        non_nan_sample = non_nan_rows.head(3)

        if len(nan_sample) == 0:
            write_log(f"Warning: {column} does not have missing values.")
            return

        nan_sample_str = nan_sample.to_string(index=False)
        non_nan_sample_str = ""
        if len(non_nan_sample) > 0:
            non_nan_sample_str = f"Sample of data without missing values:\n{non_nan_sample.to_string(index=False)}"
        else:
            non_nan_sample_str = "The whole column has missing values."

        main_entity = self.document["main_entity"]["summary"]

        template = f"""In a table about {main_entity}, {column} has missing value.

Sample of data with missing values:
{nan_sample_str}

{non_nan_sample_str}

There are general 5 classes of missing values:
    Not Collected: Information wasn't gathered due to oversight, resource limitations (e.g., certain tests or measures could not be performed), or it was deemed unnecessary at the time of collection.
    Not Applicable: Certain questions or fields do not apply to the individual/entity being measured. For example, a question about "spouse's occupation" wouldn't apply to someone who is unmarried.
    Non-Response: The subject chose not to provide information or ignored the request. This is common in surveys and certain types of observational research.
    Damaged Data: Information was originally collected but later became unavailable or corrupted due to issues in data storage, transfer, or processing.
    Censorship: for sensitive attribute, the data can be masked for privacy

Now, please provide a short list of at most 2 most likely reasons, ordered by likelihood, in the following format: 
```json
[{{"class": "The above 5 classes, or Other",
   "explanation": "Short explanation of the reason in 5 words",}}...]
```"""


        messages = [{"role": "user", "content": template}]
        response = call_gpt4(messages, temperature=0.1, top_p=0.1)
        
        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])
        
        messages.append(response['choices'][0]['message'])
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        self.document["missing_value"][column]["summary"] = json_code
        if LOG_MESSAGE_HISTORY:
            self.document["missing_value"][column]["history"] = messages


    def check_unusual_all(self):
        next_step = self.complete

        column_meanings = self.document['column_meaning']['summary']
        unusual_columns = {}

        print(" Checking the unusual values...")
        progress = self.show_progress(len(self.df.columns))

        for column_meaning in column_meanings:
            col = column_meaning['column']
            col = self.name_mapping[col]
            meaning = column_meaning['meaning']

            self.check_unusual(col, meaning)

            result = self.document['unusual'][col]['summary']

            if result['Unusualness'] and 'Explanation' not in result:
                unusual_columns[col] = result

            progress.value += 1

        if not unusual_columns:
            next_step()
            return

        clear_output(wait=True)
        print("The following columns have unusual values. Please provide the explanation as much as possible.")

        def radio_change_handler(change):
            instance = change['owner']
            if instance.value == 'Explanation:':
                instance.text_area.layout.display = ''
            else:
                instance.text_area.layout.display = 'none'

        container = widgets.VBox()


        col_to_radio = {}

        for item in unusual_columns:

            reasons = unusual_columns[item]
            
            label_text = f"<b>{item}</b>: {reasons['Examples']}"

            label = widgets.HTML(value=label_text)

            options = ['Unclear', 'Explanation:']

            radio = widgets.RadioButtons(
                options=options,
                value=options[0],
                layout=widgets.Layout(width='80%', align_items='flex-start')
            )
            
            text_area = widgets.Textarea(
                value='',
                placeholder='Please provide the reason for unusual values.',
                description='',
                disabled=False,
                layout=widgets.Layout(display='none', width='100%')
            )
            
            radio.text_area = text_area
            col_to_radio[item] = radio
            radio.observe(radio_change_handler, names='value')
            
            item_container = widgets.VBox([label, radio, text_area])
            container.children += (item_container,)

        def submit_callback(btn):
            error_items = []
            
            for col in col_to_radio:
                radio = col_to_radio[col]
                if radio.value == 'Explanation:' and not radio.text_area.value.strip():
                    error_items.append(col)
            
            if error_items:
                for col in error_items:
                    print(f"\033[91m{col} explanation can't be empty.\033[0m")
            else:
                clear_output(wait=True)

                for col in col_to_radio:
                    radio = col_to_radio[col]
                    
                    if radio.value == 'Explanation:':
                        self.document["unusual"][col]["summary"]["Explanation"] = radio.text_area.value
                        if LOG_MESSAGE_HISTORY:
                            self.document["unusual"][col]["history"].append({"role": "user",
                                                                                            "content": radio.text_area.value})
                    else:
                        self.document["unusual"][col]["summary"]["Explanation"] = "Unclear"
                
                print("Submission received.")
                next_step()
                                                                          

                        
        submit_btn = widgets.Button(
            description="Submit",
            button_style='',
            tooltip='Submit',
            icon=''
        )

        submit_btn.on_click(submit_callback)

        display(container, submit_btn)

        if self.viewer:
            submit_callback(submit_btn)



    def check_unusual(self, col, meaning):
        if col not in self.df.columns:
            raise ValueError(f"Column {col} does not exist in the DataFrame.")
        
        if "unusual" not in self.document:
            self.document["unusual"] = {}

        if col not in self.document["unusual"]:
            self.document["unusual"][col] = {}
        else:
            if self.document["unusual"][col]:
                write_log(f"Warning: {col} unusual already exists in the document.")
                return

        unique_values = self.df[col].dropna().unique()

        values_string = f"The actual data have {len(unique_values)} unique values: "

        def construct_string_with_limit(base_string, values, char_limit):
            final_string = base_string

            included_values = []

            at_least_one = False

            for value in values:
                str_value = f"'{str(value)}'"

                if not at_least_one or len(final_string) + len(str_value) + len(included_values) * len(", ") < char_limit:
                    included_values.append(str_value)
                    at_least_one = True
                else:
                    break

            values_part = ", ".join(included_values)

            if len(included_values) < len(values):
                values_part += "..."

            final_string += values_part

            return final_string

        char_limit = 300
        values_string = construct_string_with_limit(values_string, unique_values, char_limit)

        import datetime
        today = datetime.date.today()

        messages = [{"role": "user", "content": f"The column '{col}' is about: {meaning}. Guess in 10 words how the values usually look like."}]

        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        messages.append(response['choices'][0]['message'])
        messages.append({"role": "user", 
        "content": f"""{values_string}

        Review if there are any unusual values. Look out for:
        1. Values too large/small that are inconsistent with the context.
        E.g., age 999 or -5.
        Outlier is fine as long as it falls in a reasonable range, e.g., person age 120 is fine.
        2. Patterns that don't align with the nature of the data.
        E.g., age 10.12
        3. Special characters that don't fit within the real-world domain.
        E.g., age X21b 

        Be careful about date as your knowledge of date is not updated. Today is {today}.

        Follow below step by step:
        1. Summarize the values. Reason if it is unusual.
        2. Conclude with the following dict:

        ```json
        {{
            "Unusualness": true/false,
            "Examples": "There are ... E.g, ..." (concise in < 10 words, empty if not unusual) 
        }}
        ```"""})

        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        messages.append(response['choices'][0]['message'])

        for message in messages:  
            write_log(message['content'])
            write_log("-----------------------------------")
        
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)
        self.document["unusual"][col]["summary"] = json_code
        if LOG_MESSAGE_HISTORY:
            self.document["unusual"][col]["history"] = messages

    def check_consistency_all(self):
        for col in self.df.columns:
            self.check_consistency(col)

    def check_consistency(self, col: str):
        if col not in self.df.columns:
            raise ValueError(f"Column {col} does not exist in the DataFrame.")
        
        if "consistency" not in self.document:
            self.document["consistency"] = {}
        
        if col not in self.document["consistency"]:
            self.document["consistency"][col] = {}
        else:
            if self.document["consistency"][col]:
                write_log(f"Warning: {col} already exists in the document.")
                return
        
        result = process_dataframe(self.df, 'consistency.txt', col_name=col)

        processed_string = escape_json_string(result[-1]["content"])
        content = json.loads(processed_string)['Action']['Content'] 

        if isinstance(content, str):
            self.document["consistency"][col]["summary"] = json.loads(content)
        else:
            self.document["consistency"][col]["summary"] = content
        if LOG_MESSAGE_HISTORY:
            self.document["consistency"][col]["history"] = result

    def check_pattern_all(self):
        for col in self.df.columns:
            self.check_pattern(col)

    def check_pattern(self, col: str):
        if col not in self.df.columns:
            raise ValueError(f"Column {col} does not exist in the DataFrame.")
        
        if pd.api.types.is_numeric_dtype(self.df[col].dtype):
            write_log(f"Warning: {col} is a numeric column. Skipping...")
            return


        if "pattern" not in self.document:
            self.document["pattern"] = {}

        if col not in self.document["pattern"]:
            self.document["pattern"][col] = {}
        else:
            if self.document["pattern"][col]:
                write_log(f"Warning: {col} already exists in the document.")
                return
        
        result = process_dataframe(self.df, 'pattern.txt', col_name=col)

        processed_string = escape_json_string(result[-1]["content"])
        content = json.loads(processed_string)['Action']['Content'] 

        if isinstance(content, str):
            self.document["pattern"][col]["summary"] = json.loads(content)
        else:
            self.document["pattern"][col]["summary"] = content
        if LOG_MESSAGE_HISTORY:
            self.document["pattern"][col]["history"] = result


    def write_document_to_disk(self, filepath: str):
        with open(filepath, 'w') as file:
            json.dump(self.document, file)

    def read_document_from_disk(self, filepath: str):
        with open(filepath, 'r') as file:
            self.document = json.load(file)
        self.start_document(viewer=True)

    def __repr__(self):
        self.display_document()
        return ""


BOLD = '\033[1m'
ITALIC = '\033[3m'
END = '\033[0m'

def get_value_from_path(data, path):
    """Recursively extract value from nested dict using the given path."""
    
    if not isinstance(data, dict):
        return data

    if not path:
        return data
    return get_value_from_path(data[path[0]], path[1:])

target_concepts = {
    "PATIENT": {
        "Identity": {
            "Gender": ["gender_concept_id","gender_source_value"],
            "Race": ["race_concept_id","race_source_value"],
            "Ethnicity": ["ethnicity_concept_id","ethnicity_source_value"]
        },
        "Birth Details": {
            "Date of Birth": ["year_of_birth","month_of_birth","day_of_birth","birth_datetime"]
        },
        "Residence": {
            "Address": ["address","city","state","zip","county","location_source_value"],
            "Coordinates": ["latitude","longitude"]
        },
        "Identifier": ["person_id"]
    },
    "CONDITION_OCCURRENCE": {
        "Condition Information": {
            "Basic Condition Details": ["condition_occurrence_id", "condition_source_description_value", "condition_source_concept_id"],
            "Condition Duration": ["condition_start_date", "condition_start_datetime", "condition_end_date", "condition_end_datetime"],
            "Condition Status": ["stop_reason", "condition_status_source_value"]
        },
        "Patient Information": ["person_id"],
        "Provider Information": ["provider_id"],
        "Visit Information": {
            "Visit Identification": ["visit_occurrence_id", "visit_detail_id"],
            "Visit Context": ["condition_type_source"]
        }
    }

}

table_description = {
    "PATIENT": "The Patients table focuses on demographic and general information about individual patients. Key attributes include **person_id**, a unique identifier for each patient, and **gender_concept_id**, which records the patient's gender. The **year_of_birth**, **month_of_birth**, and **day_of_birth** attributes collectively provide the patient's date of birth. Other critical attributes are **race_concept_id** and **ethnicity_concept_id**, capturing the patient's race and ethnicity respectively. The **location_id** links to the patient's geographical information, while **provider_id** and **care_site_id** associate the patient with healthcare providers and care sites. Additionally, the **person_source_value**, **gender_source_value**, **gender_source_concept_id**, **race_source_value**, **race_source_concept_id**, **ethnicity_source_value**, and **ethnicity_source_concept_id** provide source-specific details for each corresponding attribute.",
    "VISIT_OCCURRENCE": "The VISIT_OCCURRENCE table documents detailed information about patient visits to healthcare facilities. It includes attributes such as **visit_occurrence_id** as a unique identifier for each visit, **person_id** linking to the patient's record, and **visit_concept_id** that classifies the type of visit (e.g., outpatient, inpatient). The table also tracks the **visit_start_date** and **visit_end_date** to specify the duration of the visit. Attributes like **visit_type_concept_id** offer insights into the context of the data collection, while **provider_id** links to the healthcare provider involved. Additionally, **care_site_id** associates the visit with a specific location, and **visit_source_value** and **visit_source_concept_id** capture information from original data sources. The table can also include **admitting_source_concept_id** and **discharge_to_concept_id** to detail patient transitions into and out of the care setting.", 
    "CONDITION_OCCURRENCE": "The CONDITION_OCCURRENCE table primarily focuses on recording patient conditions, typically diagnosed by a healthcare provider. It contains key attributes such as **person_id**, which uniquely identifies a patient, and **condition_concept_id**, referencing the specific condition diagnosed. The attribute **condition_start_date** indicates when the condition was first observed, while **condition_end_date** reflects when it was resolved or ceased. Additionally, **condition_type_concept_id** describes the context or source of the diagnosis, such as a hospital visit. The **stop_reason** attribute provides insight into why a condition was considered resolved or ended. Another crucial attribute, **provider_id**, identifies the healthcare provider responsible for the diagnosis, and **visit_occurrence_id** links the condition to a specific patient visit. The table also includes **condition_source_value**, a textual representation of the condition from the original source data, and **condition_source_concept_id** for mapping to a standardized concept. Lastly, **condition_status_concept_id** gives further details about the status of the condition, like whether it's an active diagnosis or a historical record.", 
    "DRUG_EXPOSURE": "The DRUG_EXPOSURE table captures information about a patient's exposure to a drug. The primary attribute, **DRUG_EXPOSURE_ID**, uniquely identifies each drug exposure event. **PERSON_ID** links to the individual patient, while **DRUG_CONCEPT_ID** identifies the specific drug. **DRUG_EXPOSURE_START_DATE** and **DRUG_EXPOSURE_END_DATE** denote the period of drug exposure. Dosage and frequency are detailed through **DOSE_UNIT_CONCEPT_ID** and **QUANTITY**. The table also includes **ROUTE_CONCEPT_ID** to specify the drug administration route, and **PROVIDER_ID** to identify the healthcare provider. **VISIT_OCCURRENCE_ID** links the drug exposure to a specific patient visit, and **DRUG_TYPE_CONCEPT_ID** categorizes the context of drug exposure, like prescription or inpatient administration.", 
    "PROCEDURE_OCCURRENCE": "The PROCEDURE_OCCURRENCE table records details of clinical procedures performed on patients. At its core, this table includes the **procedure_concept_id**, which identifies the specific type of procedure carried out. The **person_id** attribute links the procedure to a specific patient. The **procedure_date** and **procedure_datetime** attributes specify when the procedure was performed. The **procedure_type_concept_id** defines the context or source of data entry. Other important attributes include **quantity**, representing the number of times the procedure was performed, and **provider_id**, identifying the healthcare provider who performed the procedure. The **visit_occurrence_id** links the procedure to the patient visit during which it was performed, and **modifier_concept_id** provides additional details or modifications to the procedure. Finally, **procedure_source_value** and **procedure_source_concept_id** offer source-specific codes for the procedure, and **qualifier_concept_id** gives further context or qualifiers related to the procedure.", 
    "DEVICE": "The DEVICE table focuses on medical device usage in patient care. Key attributes include **device_id**, a unique identifier for each device record, and **person_id**, linking the device to an individual. **device_concept_id** represents the specific type of device. **device_exposure_start_date** and **device_exposure_end_date** define the usage period. **device_type_concept_id** describes the context of the device usage. **provider_id** identifies the healthcare provider involved, while **visit_occurrence_id** connects the device usage to a specific visit. Additional attributes like **device_source_value** and **quantity** provide extra details about the device and its usage.", 
    "MEASUREMENT": "The MEASUREMENT table captures quantitative data and observations about a patient's health status. It includes key attributes like **measurement_id**, a unique identifier for each measurement, and **person_id**, linking the measurement to a specific patient. The **measurement_concept_id** identifies what was measured, such as blood pressure or glucose level. The **measurement_date** and **measurement_datetime** record when the measurement was taken. The **measurement_type_concept_id** indicates the nature of the measurement, like lab result or vital sign. The **operator_concept_id** represents how the measurement was taken, for example, by a device or a clinician. The **value_as_number**, **value_as_concept_id**, and **unit_concept_id** provide the result and unit of the measurement. **range_low** and **range_high** offer reference ranges for the measurement value. **provider_id** and **visit_occurrence_id** link the measurement to the provider and the visit during which it was taken. Additional details such as **measurement_source_value**, **measurement_source_concept_id**, and **unit_source_value** capture the original source information. Lastly, **value_source_value** records the measurement result as it was originally represented.", 
    "OBSERVATION": "The OBSERVATION captures patient observations or measurements that are not diagnoses, procedures, or drug prescriptions. This table includes **person_id** to identify the patient, **observation_id** as a unique identifier for the observation, and **observation_concept_id** to specify the type of observation. The **observation_date** and **observation_datetime** record when the observation was made. **observation_type_concept_id** classifies the observation source, such as from a survey or a lab result. **value_as_number**, **value_as_string**, **value_as_concept_id**, and **unit_concept_id** describe the observation result in various formats. **qualifier_concept_id** and **associated_provider_id** give additional context to the observation, while **visit_occurrence_id** and **visit_detail_id** link the observation to specific visits. **obs_event_field_concept_id** captures the field from the source data where the observation originated, and **observation_source_value**, **observation_source_concept_id**, **unit_source_value**, and **qualifier_source_value** provide source-specific information. **observation_event_id**, **obs_event_field_concept_id**, and **value_as_datetime** offer further details on the observation event.", 
    "DEATH": "The DEATH table is specifically designed to capture information about patient deaths. It contains several key attributes that provide details about the circumstances and documentation of a patient's death. The primary attribute is **person_id**, which uniquely identifies the deceased individual within the database. Another crucial attribute is **death_date**, specifying the exact date of death. The **death_datetime** attribute can offer more precise timing if available. The **death_type_concept_id** is used to indicate the source or method of death determination, like a death certificate or autopsy report. The **cause_concept_id** and **cause_source_value** attributes are used to record the cause of death, either as a standard concept or as a raw value from the data source, respectively. Additionally, the **cause_source_concept_id** represents a standardized concept that corresponds to the source value for the cause of death.", 
    "SPECIMEN": "The SPECIMEN table stores detailed information about biological specimens collected from patients for diagnostic, treatment, or research purposes. This table includes several attributes that provide comprehensive data about each specimen. Key attributes include **specimen_id**, which is a unique identifier for each specimen, and **person_id**, linking the specimen to a specific individual in the database. The **specimen_concept_id** and **specimen_type_concept_id** describe the type of specimen and the method of its collection, respectively. **specimen_date** and **specimen_datetime** capture the date and time of specimen collection. **quantity** reflects the amount of specimen collected, and **unit_concept_id** indicates the unit of measurement. The **anatomic_site_concept_id** specifies the body site from where the specimen was taken, and **disease_status_concept_id** provides information about the disease status associated with the specimen. **specimen_source_id** and **visit_occurrence_id** are used to link the specimen to other relevant data in the database, such as the visit during which the specimen was collected. The **specimen_source_value**, **anatomic_site_source_value**, and **disease_status_source_value** attributes store the original values from the source data for mapping purposes.", 
    "COST": "The COST table stores financial information related to healthcare services provided to patients. This table captures various cost-related details associated with healthcare encounters, procedures, and medications. Key attributes include **cost_event_id** and **cost_domain_id**, which identify the event and its domain (like drug or procedure) associated with the cost. The **currency_concept_id** specifies the currency of the cost. There's a focus on the specific nature of costs with attributes like **paid_copay**, **paid_coinsurance**, **paid_toward_deductible**, and **paid_by_payer**, detailing different payment components. The **amount_allowed** and **revenue_code_concept_id** provide information on allowable amounts and revenue codes.", 
    "LOCATION": "The LOCATION table stores information about physical locations relevant to healthcare data. This table typically includes attributes such as **location_id**, which serves as a unique identifier for each location. It also contains **address_1** and **address_2** for detailed street addresses, **city** and **state** for regional identification, and **zip** for postal codes. Furthermore, the table includes **county**, **location_source_value**, and **country** to provide a comprehensive geographic context.", 
    "CARE_SITE": "The CARE_SITE table represents information related to healthcare facilities where patient care is provided. It includes the **care_site_id**, a unique identifier for each care site. The **care_site_name** provides the name of the care site, and the **place_of_service_concept_id** links to a standardized concept identifying the type of care site (e.g., hospital, clinic). The **location_id** associates the care site with a physical location, and the **care_site_source_value** captures the original source data for the care site as it appears in the source system. Additionally, the **place_of_service_source_value** is the source code used in the source data to identify the type of care site.", 
    "PROVIDER": "The PROVIDER tablecaptures detailed information about healthcare providers. It primarily focuses on the background and characteristics of individual providers. Key attributes include **provider_id**, serving as a unique identifier for each provider. The table also contains **provider_name**, which records the name of the provider. To classify the type of provider, there's **provider_type**, while **specialty_concept_id** links to their medical specialty. The **care_site_id** associates providers with their place of practice. Additionally, attributes like **gender_concept_id**, **year_of_birth**, and **provider_source_value** offer demographic and source-specific details about the providers.", 
    "PAYER_PLAN_PERIOD": "The PAYER_PLAN_PERIOD table stores information about the periods of time a person is covered by a particular payer or plan. It captures details about the insurance or payer coverage for an individual. The table includes attributes such as **PAYER_PLAN_PERIOD_ID**, which is a unique identifier for each record. **PERSON_ID** links the record to a specific individual. The coverage period is defined by **PAYER_PLAN_PERIOD_START_DATE** and **PAYER_PLAN_PERIOD_END_DATE**, indicating the start and end of the coverage period. **PAYER_SOURCE_VALUE** and **PLAN_SOURCE_VALUE** provide information about the payer and plan from the source data. There are also attributes for standard concepts: **PAYER_CONCEPT_ID** and **PLAN_CONCEPT_ID**, along with their associated source values and source concept IDs (**PAYER_SOURCE_CONCEPT_ID** and **PLAN_SOURCE_CONCEPT_ID**). Lastly, **FAMILY_SOURCE_VALUE** may contain additional information about the family or group plan, if applicable."
}


table_samples = {
    "PATIENT": 
"""	person_id	gender_concept_id	year_of_birth	month_of_birth	day_of_birth	birth_datetime	race_concept_id	ethnicity_concept_id	location_id	provider_id	care_site_id	person_source_value	gender_source_value	gender_source_concept_id	race_source_value	race_source_concept_id	ethnicity_source_value	ethnicity_source_concept_id
0	1	8507	1985	5	20	1985-05-20	8527	38003564	1	1	1	12345	M	8507	White	8527	Not Hispanic or Latino	38003564
1	2	8532	1990	8	10	1990-08-10	8516	38003563	2	2	2	67890	F	8532	Black or African American	8516	Hispanic or Latino	38003563""",
    
    "VISIT_OCCURRENCE": 
"""visit_occurrence_id	person_id	visit_concept_id	visit_start_date	visit_start_datetime	visit_end_date	visit_end_datetime	visit_type_concept_id	provider_id	care_site_id	visit_source_value	visit_source_concept_id	admitting_source_concept_id	admitting_source_value	discharge_to_concept_id	discharge_to_source_value	preceding_visit_occurrence_id
0	123456	101	9201	2023-01-10	2023-01-10 08:00:00	2023-01-12	2023-01-12 15:00:00	2001	501	601	VS123	3001	4001	AS123	5001	DS123	111
1	789012	102	9202	2023-02-15	2023-02-15 09:30:00	2023-02-16	2023-02-16 10:30:00	2002	502	602	VS456	3002	4002	AS456	5002	DS456	222""",
    
    "CONDITION_OCCURRENCE":
"""condition_occurrence_id	person_id	condition_concept_id	condition_start_date	condition_start_datetime	condition_end_date	condition_end_datetime	condition_type_concept_id	stop_reason	provider_id	visit_occurrence_id	visit_detail_id	condition_source_value	condition_source_concept_id	condition_status_concept_id	condition_status_source_value
0	1	101	201826	2021-01-10	2021-01-10 08:30:00	2021-01-20	2021-01-20 17:00:00	32019	Resolved	150	200	300	Diabetes	401	501	Active
1	2	102	31967	2021-02-15	2021-02-15 09:45:00	2021-02-25	2021-02-25 16:30:00	32020	Improved	151	201	301	Hypertension	402	502	Controlled""",
    
    "DRUG_EXPOSURE": 
"""drug_exposure_id	person_id	drug_concept_id	drug_exposure_start_date	drug_exposure_end_date	verbatim_end_date	drug_type_concept_id	stop_reason	refills	quantity	days_supply	sig	route_concept_id	lot_number	provider_id	visit_occurrence_id	drug_source_value	drug_source_concept_id	route_source_value	dose_unit_source_value
0	1	1001	456789	2023-01-01	2023-01-10	2023-01-10	38000177	Completed course	0	10	10	Take 1 tablet daily	0	LOT1001	12345	111	DrugA	0	Oral	mg
1	2	1002	987654	2023-02-15	2023-03-01	2023-03-01	38000177	Adverse reaction	1	30	15	Take 2 tablets twice daily	0	LOT1002	67890	222	DrugB	0	Oral	mg""",
    
    "PROCEDURE_OCCURRENCE":
"""   procedure_occurrence_id  person_id  procedure_concept_id procedure_date  \
0                        1        101               2100001     2023-01-15   
1                        2        102               2100002     2023-02-20   
    procedure_datetime  procedure_type_concept_id  modifier_concept_id  \
0  2023-01-15 10:00:00                   38000275                    0   
1  2023-02-20 14:30:00                   38000275                    0   

   quantity  provider_id  visit_occurrence_id  visit_detail_id  \
0         1          501                  701                0   
1         2          502                  702                0   

   procedure_source_concept_id procedure_source_value  qualifier_concept_id  \
0                      2100001                  PROC1                     0   
1                      2100002                  PROC2                     0   

  qualifier_source_value  procedure_cost  
0                   None           200.0  
1                   None           450.0  """,
    "DEVICE": 
"""device_id	person_id	device_exposure_start_date	device_exposure_start_datetime	device_exposure_end_date	device_exposure_end_datetime	device_concept_id	device_type_concept_id	unique_device_id	quantity	provider_id	visit_occurrence_id	device_source_value	device_source_concept_id
0	1001	2001	2023-01-15	2023-01-15 08:00:00	2023-01-20	2023-01-20 18:00:00	3001	4001	UD1001	1	5001	6001	DeviceA	7001
1	1002	2002	2023-02-20	2023-02-20 09:30:00	2023-02-25	2023-02-25 16:30:00	3002	4002	UD1002	2	5002	6002	DeviceB	7002""",
    
    "MEASUREMENT": """measurement_id	person_id	measurement_concept_id	measurement_date	measurement_datetime	measurement_type_concept_id	operator_concept_id	value_as_number	value_as_concept_id	unit_concept_id	range_low	range_high	provider_id	visit_occurrence_id	measurement_source_value	measurement_source_concept_id	unit_source_value	value_source_value
1001	2001	3001	2023-01-15	2023-01-15 08:00:00	4001	5001	5.6	6001	7001	4.5	8.0	8001	9001	BP_SYS	10001	mmHg	120
1002	2002	3002	2023-01-16	2023-01-16 09:30:00	4002	5002	7.8	6002	7002	6.0	9.5	8002	9002	BP_DIA	10002	mmHg	80""",
    
    "OBSERVATION": 
"""observation_id  person_id  observation_concept_id observation_date  \
0            1001        123                    3001       2023-01-01   
1            1002        456                    3002       2023-01-02   

  observation_datetime  observation_type_concept_id  value_as_number  \
0  2023-01-01 08:00:00                         2001             98.6   
1  2023-01-02 09:30:00                         2002             99.5   

  value_as_string  value_as_concept_id  qualifier_concept_id  unit_concept_id  \
0          Normal                 4001                  5001             6001   
1        Elevated                 4002                  5002             6002   

   provider_id  visit_occurrence_id observation_source_value  \
0         7001                 8001           Blood Pressure   
1         7002                 8002               Heart Rate   

   observation_source_concept_id unit_source_value qualifier_source_value  
0                           9001              mmHg                Resting  
1                           9002         beats/min         After Exercise  """,
    
    "DEATH": 
""" person_id  death_date      death_datetime  death_type_concept_id  cause_concept_id cause_source_value  cause_source_concept_id
    123456  2023-05-15  2023-05-15 14:30:00               38003565             50115              I21.9                  4323456
    789012  2023-06-20  2023-06-20 08:45:00               38003566            433146              C50.9                  7654321""",
    
    "SPECIMEN": 
"""specimen_id	person_id	specimen_concept_id	specimen_type_concept_id	specimen_date	specimen_datetime	quantity	unit_concept_id	anatomic_site_concept_id	disease_status_concept_id	specimen_source_id	visit_occurrence_id	visit_detail_id	specimen_source_value	unit_source_value	anatomic_site_source_value	disease_status_source_value
101	2001	3001	4001	2023-01-15	2023-01-15 10:00:00	1.5	5001	6001	7001	A123	8001	9001	Blood Sample	ml	Arm	Healthy
102	2002	3002	4002	2023-01-20	2023-01-20 11:30:00	2.0	5002	6002	7002	B456	8002	9002	Tissue Sample	g	Liver	Diseased""",
    
    "COST": 
"""	cost_id	person_id	cost_event_id	cost_domain_id	currency_concept_id	total_charge	total_paid	payer_plan_period_id	amount_allowed	paid_by_payer	paid_by_patient	paid_patient_copay	paid_patient_coinsurance	paid_patient_deductible	paid_by_primary	paid_ingredient_cost	paid_dispensing_fee
0	1001	2001	3001	Drug	840	500.0	450.0	4001	450.0	400.0	50.0	25.0	15.0	10.0	400.0	300.0	20.0
1	1002	2002	3002	Procedure	840	1500.0	1400.0	4002	1400.0	1300.0	100.0	50.0	40.0	10.0	1300.0	1000.0	50.0""",
    
    "LOCATION": 
"""location_id	address_1	address_2	city	state	zip	county	location_source_value	latitude	longitude
0	1	123 Main St	Suite 100	Springfield	NY	12345	Hampden	L123	40.7128	-74.0060
1	2	456 Elm St	Apt 202	Riverdale	CA	67890	Archie	L456	34.0522	-118.2437""",
    
    "CARE_SITE": 
""" care_site_id   care_site_name  place_of_service_concept_id  location_id care_site_source_value place_of_service_source_value
1   City Hospital                        12345          101                  CS001                      Hospital
2    Rural Clinic                        67890          102                  CS002                        Clinic""",
    
    "PROVIDER": 
"""provider_id	provider_name	npi	dea	specialty_concept_id	care_site_id	year_of_birth	gender_concept_id	provider_source_value	specialty_source_value	specialty_source_concept_id	gender_source_value	gender_source_concept_id
0	101	Dr. Jane Smith	1234567890	AB1234567	111	10	1970	8507	P101	Cardiology	1001	F	1003
1	102	Dr. John Doe	0987654321	CD7654321	222	20	1980	8507	P102	Neurology	1002	M	1004""",
    
    "PAYER_PLAN_PERIOD": 
"""payer_plan_period_id	person_id	payer_plan_period_start_date	payer_plan_period_end_date	payer_concept_id	plan_concept_id	sponsor_concept_id	family_plan_concept_id	stop_reason	payer_source_value	payer_source_concept_id	plan_source_value	plan_source_concept_id	sponsor_source_value	sponsor_source_concept_id	family_plan_source_value	family_plan_source_concept_id
1001	123	2023-01-01	2023-12-31	2001	3001	4001	5001	End of contract	PayerA	6001	PlanA	7001	SponsorA	8001	FamilyPlanA	9001
1002	456	2023-06-01	2023-12-31	2002	3002	4002	5002	Change of employment	PayerB	6002	PlanB	7002	SponsorB	8002	FamilyPlanB	9002""",
}

attributes_description = {
    "PATIENT" :{
    "person_id": "the original id from the source data provided, otherwise it can be an autogenerated number. ",
    "year_of_birth": "as an integer",
    "month_of_birth": "as an integer",
    "day_of_birth": "as an integer",
    "birth_datetime": "If birth_datetime is not provided in the source, use the following logic to infer the date: If day_of_birth is null and month_of_birth is not null then use the first of the month in that year. If month_of_birth is null or if day_of_birth AND month_of_birth are both null and the person has records during their year of birth then use the date of the earliest record, otherwise use the 15th of June of that year. If time of birth is not given use midnight (00:00:0000).",
    "gender_concept_id": "{ 'FEMALE': 8532, 'MALE': 8507 }",
    "gender_source_value": "the original value in the source table",
    "person_source_value": "any identifier from the source data that identifies the person.",
    "race_concept_id": "{ 'American Indian or Alaska Native': 8657, 'Asian': 8515, 'Black': 8516, 'Native Hawaiian or Other Pacific Islander': 8557, 'White': 8527 }",
    "race_source_value": "the original value in the source table",
    "ethnicity_concept_id": "{ 'Hispanic or Latino': 38003564, 'Not Hispanic or Latino': 38003563 }",
    "ethnicity_source_value": "the original value in the source table",
    },
    "CONDITION_OCCURRENCE": {
    "condition_occurrence_id": "the original id column from the source data provided, otherwise it can be an autogenerated number. ",
    "person_id": "The PERSON_ID of the PERSON for whom the condition is recorded.",
    "condition_start_date": "the start date of the condition",
    "condition_start_datetime": "If a source does not specify datetime the convention is to set the time to midnight (00:00:0000)",
    "condition_end_date": "The end date of the condition",
    "condition_end_datetime": " the end date of the condition",
    "stop_reason": "The Stop Reason indicates why a Condition is no longer valid with respect to the purpose within the source data.",
    "visit_occurrence_id": "The visit during which the condition occurred.",
    "condition_source_description_value": "Source data representing the condition that occurred.",
    "condition_type_source": "the provenance of the Condition record, as in whether the condition was from an EHR system, insurance claim, registry, or other sources.",
    "condition_status_source_value": "the source data indicating when and how a diagnosis was given to a patients",
    },
    "DEATH" :{
    "death_date": "Date of death, use January 1st of the year if only year is known. E.g., 2020-01-01",
    "death_datetime": "Exact date and time of death, default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "cause_source_value": "Original cause of death from source data. E.g., 'Heart Attack'",
    "death_record_source": "Source of death record (e.g., death certificate, hospital record). E.g., 'Death Certificate'",
    },
    "DRUG_EXPOSURE" :{
    "drug_exposure_id": "Unique identifier for each drug exposure record. E.g., 101",
    "drug_exposure_start_date": "Start date of the drug exposure. E.g., 2022-03-15",
    "drug_exposure_start_datetime": "Exact start date and time of the drug exposure. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "drug_exposure_end_date": "End date of the drug exposure. E.g., 2022-04-14",
    "drug_exposure_end_datetime": "Exact end date and time of the drug exposure. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "verbatim_end_date": "Original end date as recorded in the source data. E.g., 2022-04-14",
    "stop_reason": "Reason the drug exposure was stopped. E.g., 'Completed treatment'",
    "refills": "Number of refills prescribed. E.g., 2",
    "quantity": "Quantity of the drug prescribed. E.g., 30 (tablets)",
    "days_supply": "Number of days the drug supply is expected to last. E.g., 30",
    "sig": "Directions for use as specified by the prescriber. E.g., 'Take 1 tablet daily'",
    "lot_number": "Lot number of the drug. E.g., 'LN123456'",
    "drug_source_value": "Original value of the drug as in the source data. E.g., 'Aspirin'",
    "route_source_value": "Original value of the route of administration in the source data. E.g., 'Oral'",
    "dose_unit_source_value": "Unit of the dose as in the source data. E.g., 'mg'",
    },
    "PROCEDURE_OCCURRENCE" :{
    "procedure_occurrence_id": "Unique identifier for the procedure record. E.g., 456789",
    "procedure_date": "Date when the procedure was performed. E.g., 2023-05-20",
    "procedure_datetime": "Exact date and time of the procedure. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "procedure_type_concept_id": "Concept ID indicating the type of procedure (e.g., surgical, diagnostic). E.g., 44818701 (for 'Surgical procedure')",
    "modifier_concept_id": "Concept ID for any modifiers related to the procedure. E.g., 2000000 ('Laparoscopic')",
    "quantity": "Number of times the procedure was performed. E.g., 1",
    "procedure_source_value": "Original value of the procedure as in the source data. E.g., 'Appendectomy'",
    "qualifier_source_value": "Additional details about the procedure from the source data. E.g., 'Laparoscopic'",
    },
    "DEVICE" :{
    "device_exposure_id": "Unique identifier for each device exposure record. E.g., 999888",
    "device_exposure_start_date": "Start date of the device exposure. E.g., 2023-01-15",
    "device_exposure_start_datetime": "Exact start date and time of the device exposure. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "device_exposure_end_date": "End date of the device exposure. E.g., 2023-06-15",
    "device_exposure_end_datetime": "Exact end date and time of the device exposure. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "quantity": "Number of devices used or exposed to. E.g., 1",
    "device_source_value": "Original value of the device as in the source data. E.g., 'Pacemaker'",
    },
    "MEASUREMENT" :{
    "measurement_id": "Unique identifier for the measurement record. E.g., 123456",
    "measurement_date": "Date when the measurement was taken. E.g., 2023-08-10",
    "measurement_datetime": "Exact date and time the measurement was taken. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "value_as_number": "Numerical value of the measurement. E.g., 120 (for systolic blood pressure)",
    "range_low": "Lower limit of the normal range for the measurement. E.g., 90 (for systolic blood pressure)",
    "range_high": "Upper limit of the normal range for the measurement. E.g., 120 (for systolic blood pressure)",
    "measurement_source_value": "Original value of the measurement as in the source data. E.g., 'Blood Pressure'",
    "unit_source_value": "Original unit of the measurement as in the source data. E.g., 'mmHg'",
    },
    "OBSERVATION" :{
    "observation_id": "Unique identifier for each observation record. E.g., 789123",
    "observation_date": "Date of the observation. E.g., 2023-02-15",
    "observation_datetime": "Exact date and time of the observation. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "value_as_number": "Numerical value of the observation if applicable. E.g., 10 (for 'Cigarettes per day')",
    "value_as_string": "Textual value of the observation if applicable. E.g., 'Non-smoker'",
    "observation_source_value": "Original value of the observation as in the source data. E.g., 'Smoking status'",
    "unit_source_value": "Original unit of the observation as in the source data. E.g., 'Cigarettes per day'",
    },
    "SPECIMEN" :{
    "specimen_id": "Unique identifier for each specimen record. E.g., 123456",
    "specimen_date": "Date when the specimen was collected. E.g., 2023-03-01",
    "specimen_datetime": "Exact date and time when the specimen was collected. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "quantity": "Quantity of the specimen collected. E.g., 10 (ml for blood)",
    "specimen_source_value": "Original value of the specimen as in the source data. E.g., 'Blood Sample'",
    "unit_source_value": "Original unit of the specimen quantity as in the source data. E.g., 'ml'",
    },
    "COST" :{
    "total_charge": "Total charge for the event. E.g., 200.00",
    "total_cost": "Total cost for the event. E.g., 150.00",
    "total_paid": "Total amount paid for the event. E.g., 150.00",
    "paid_by_payer": "Amount paid by the payer (e.g., insurance). E.g., 100.00",
    "paid_by_patient": "Amount paid by the patient. E.g., 50.00",
    "paid_patient_copay": "Patient's copay amount. E.g., 20.00",
    "paid_patient_coinsurance": "Patient's coinsurance amount. E.g., 10.00",
    "paid_patient_deductible": "Patient's deductible amount. E.g., 20.00",
    "paid_by_primary": "Amount paid by primary payer. E.g., 100.00",
    "paid_ingredient_cost": "Cost of the ingredient for a drug. E.g., 30.00",
    "paid_dispensing_fee": "Dispensing fee paid. E.g., 5.00",
    "amount_allowed": "Amount allowed by the payer. E.g., 150.00",
    "cost_source_value": "Original value of the cost as in the source data. E.g., 'Procedure cost'",
    },
    "LOCATION" :{
    "location_id": "Unique identifier for each location record. E.g., 123",
    "address_1": "First line of the address. E.g., '123 Main St'",
    "address_2": "Second line of the address (if applicable). E.g., 'Apt 4'",
    "city": "City of the location. E.g., 'New York'",
    "state": "State of the location. E.g., 'NY'",
    "zip": "Zip code of the location. E.g., '10001'",
    "county": "County of the location. E.g., 'New York County'",
    "location_source_value": "Original value of the location as in the source data. E.g., '123 Main St, Apt 4, New York, NY, 10001'",
    "latitude": "a float, must be between -90 and 90.",
    "longitude": "a float, must be between -180 and 180.",
    },
    "CARE_SITE" :{
    "care_site_id": "Unique identifier for each care site record. E.g., 456",
    "care_site_name": "Name of the care site. E.g., 'Main Street Medical Center'",
    "care_site_source_value": "Original value of the care site as in the source data. E.g., 'Main Street Medical Center, Outpatient'",
    "place_of_service_source_value": "Original place of service as in the source data. E.g., 'Outpatient Hospital'",
    },
    "PROVIDER" :{
    "provider_id": "Unique identifier for each provider record. E.g., 789",
    "provider_name": "Name of the provider. E.g., 'Dr. Jane Smith'",
    "NPI": "National Provider Identifier. E.g., '1234567890'",
    "DEA": "Drug Enforcement Administration registration number. E.g., 'AB1234567'",
    "year_of_birth": "Year of birth of the provider. E.g., 1970",
    "provider_source_value": "Original value of the provider as in the source data. E.g., 'Dr. Jane Smith'",
    "specialty_source_value": "Original specialty of the provider as in the source data. E.g., 'Cardiology'",
    "gender_source_value": "Original gender of the provider as in the source data. E.g., 'Female'",
    },
    "PAYER_PLAN_PERIOD" :{
    "payer_plan_period_id": "Unique identifier for each payer plan period record. E.g., 12345",
    "payer_plan_period_start_date": "Start date of the coverage period. E.g., 2023-01-01",
    "payer_plan_period_end_date": "End date of the coverage period. E.g., 2023-12-31",
    "stop_reason": "Reason for the end of the coverage period. E.g., 'Change of employment'",
    "payer_source_value": "Original value of the payer as in the source data. E.g., 'Medicare'",
    "plan_source_value": "Original value of the plan as in the source data. E.g., 'Medicare Part D'",
    "sponsor_source_value": "Original value of the sponsor as in the source data. E.g., 'Government'",
    },
    "VISIT_OCCURRENCE" :{
    "visit_occurrence_id": "Unique identifier for each visit record. E.g., 456789",
    "visit_start_date": "Start date of the visit. E.g., 2023-04-15",
    "visit_start_datetime": "Exact start date and time of the visit. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "visit_end_date": "End date of the visit. E.g., 2023-04-15",
    "visit_end_datetime": "Exact end date and time of the visit. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "visit_source_value": "Original value of the visit as in the source data. E.g., 'Outpatient Visit'",
    },
    "OBSERVATION_PERIOD" :{
    "observation_period_id": "Unique identifier for each observation period record. E.g., 112233",
    "observation_period_start_date": "Start date of the observation period. E.g., 2023-01-01",
    "observation_period_start_datetime": "Exact start date and time of the observation period. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    "observation_period_end_date": "End date of the observation period. E.g., 2023-12-31",
    "observation_period_end_datetime": "Exact end date and time of the observation period. Default to midnight if time unknown. E.g., 2020-01-01 00:00:00",
    }
}

cdm_tables = """1. PATIENT: Individual's healthcare identity hub, capturing demographics, contact information, and unique medical identifiers.
2. OBSERVATION_PERIOD: Clinical data timeframe, marking the start and end dates of health monitoring or treatment phases.
3. VISIT_OCCURRENCE: Healthcare interaction records, detailing visit dates, types (e.g., inpatient, outpatient), locations, and involved healthcare professionals.
4. CONDITION_OCCURRENCE: Medical condition logs, encompassing diagnosis dates, condition types, severity indicators, and observed symptoms.
5. DRUG_EXPOSURE: Medication intake documentation, including drug names, dosages, administration routes, prescribing dates, and treatment durations.
6. PROCEDURE_OCCURRENCE: Patient treatment actions, specifying procedure types, execution dates, purposes, and attending practitioners.
7. DEVICE: Medical device utilization specifics, identifying device types, application or implantation dates, functional purposes, and locations.
8. MEASUREMENT: Clinical test outcomes, containing types of tests, numerical or categorical results, units, and test dates.
9. OBSERVATION: Diverse health-related facts, sourced from examinations, questionnaires, or procedural outcomes, inclusive of non-clinical information. This is general, and please prefer to use MEASUREMENT or CONDITION_OCCURRENCE if possible.
10. DEATH: End-of-life data, specifying causes, dates, and circumstances surrounding a patient's death.
11. SPECIMEN: Biological sample records, denoting sample types, collection dates, handling processes, and preservation methods.
12. COST: Healthcare financials, enumerating costs associated with medical events like procedures, medications, visits, and equipment.
13. LOCATION: Geographical pinpointing, detailing physical addresses or coordinates of healthcare facilities or patient residences.
14. CARE_SITE: Health service points, defining types of facilities (e.g., hospitals, clinics), specializations, and operational scopes.
15. PROVIDER: Practitioner profiles, listing professionals' credentials, specialties, roles, and contact information.
16. PAYER_PLAN_PERIOD: Insurance coverage chronology, indicating enrollment spans, plan types, provided benefits, and payer information.
"""



class CDMTransformation:
    def __init__(self, doc_df, log_file_path='data_log.txt'):

        if isinstance(doc_df, DocumentedData):
            self.doc_df = doc_df
            self.pipeline = doc_df.generate_pipeline()
        elif isinstance(doc_df, DataCleaning):
            self.doc_df = doc_df.doc_df
            self.pipeline = doc_df.generate_pipeline()

        
        
        self.document = {}

        self.log_file_path = log_file_path
        database_description = """The database primarily focuses on healthcare data, structured around several interconnected entities. The central entity is the **PATIENT** table, which contains details about individuals receiving medical care. Their healthcare journey is tracked through the **VISIT_OCCURRENCE** table, which records each visit to a healthcare facility. The **CONDITION_OCCURRENCE** table details any diagnosed conditions during these visits, while the **DRUG_EXPOSURE** table captures information on medications prescribed to the patients.
Procedures performed are logged in the **PROCEDURE_OCCURRENCE** table, and any medical devices used are listed in the **DEVICE** table. The **MEASUREMENT** table records various clinical measurements taken, and the **OBSERVATION** table notes any other relevant clinical observations.
In cases where a patient passes away, the **DEATH** table provides information on the mortality. The **SPECIMEN** table tracks biological samples collected for analysis, and the **COST** table details the financial aspects of the healthcare services.
The **LOCATION**, **CARE_SITE**, and **PROVIDER** tables offer contextual data, respectively detailing the geographical locations, healthcare facilities, and medical professionals involved in patient care. Lastly, the **PAYER_PLAN_PERIOD** table provides information on the patients' insurance coverage details and durations."""
        self.database_description = database_description

        tables = [
            "PATIENT", "VISIT_OCCURRENCE", "CONDITION_OCCURRENCE", "DRUG_EXPOSURE", 
            "PROCEDURE_OCCURRENCE", "DEVICE", "MEASUREMENT", "OBSERVATION", 
            "DEATH", "SPECIMEN", "COST", "LOCATION", "CARE_SITE", "PROVIDER", 
            "PAYER_PLAN_PERIOD"
        ]
        self.tables = tables

        edges = [
            ("PATIENT", "VISIT_OCCURRENCE"),
            ("VISIT_OCCURRENCE", "CONDITION_OCCURRENCE"),
            ("VISIT_OCCURRENCE", "DRUG_EXPOSURE"),
            ("VISIT_OCCURRENCE", "PROCEDURE_OCCURRENCE"),
            ("VISIT_OCCURRENCE", "DEVICE"),
            ("VISIT_OCCURRENCE", "MEASUREMENT"),
            ("VISIT_OCCURRENCE", "OBSERVATION"),
            ("PATIENT", "DEATH"),
            ("VISIT_OCCURRENCE", "SPECIMEN"),
            ("VISIT_OCCURRENCE", "COST"),
            ("VISIT_OCCURRENCE", "LOCATION"),
            ("VISIT_OCCURRENCE", "CARE_SITE"),
            ("VISIT_OCCURRENCE", "PROVIDER"),
            ("PATIENT", "PAYER_PLAN_PERIOD")
        ]
        self.edges = edges
        

    def display_database(self):
        database_description = replace_asterisks_with_tags(self.database_description)
        display(HTML("<h1>OMOP CDM</h1>" + database_description))
        visualize_graph(self.tables, self.edges)

    def write_log(self, message: str):
        self.log_file = open(self.log_file_path, 'a')
        self.log_file.write(message + '\n')
        self.log_file.close()

    def complete(self):
        print("Congratulation! The transformation is complete. ")

    def write_document_to_disk(self, filepath: str):
        with open(filepath, 'w') as file:
            json.dump(self.document, file)

    def read_document_from_disk(self, filepath: str):
        with open(filepath, 'r') as file:
            self.document = json.load(file)

    def show_progress(self, max_value):
        progress = widgets.IntProgress(
            value=1,
            min=0,
            max=max_value+1,  
            step=1,
            description='',
            bar_style='',
            orientation='horizontal'
        )
        
        display(progress)
        return progress

    def start(self):
        self.get_main_table()

    def get_main_table(self):

        next_step = self.decide_main_table

        if "main_table" not in self.document:
            self.document["main_table"] = {}
        else:
            if self.document["main_table"]:
                write_log("Warning: main_table already exists in the document.")
                next_step()
                return

        print(" Identifying target tables to map to...")

        progress = self.show_progress(1)
        source_table_description = self.doc_df.document["table_summary"]["summary"]
        
        summary, messages = find_target_table(source_table_description)
        progress.value += 1

        for message in messages:
            write_log(message['content'])
            write_log("-----------------------------------")

        for table in summary:
            if table not in self.tables:
                raise ValueError(f"Table {table} does not exist in the CDM.")
        
        self.document["main_table"]["summary"] = summary
        if LOG_MESSAGE_HISTORY: 
            self.document["main_table"]["history"] = messages

        next_step()









        



    def concept_mapping(self, target_table):

        next_step = self.write_codes2

        if "concept_mapping" not in self.document:
            self.document["concept_mapping"] = {}
        
        if target_table in self.document["concept_mapping"]:
            write_log(f"Warning: {target_table} already exists in the document for concept_mapping.")
        else:
            print(" Identifying the concept mapping...")

            progress = self.show_progress(1)
            source_table_description = self.doc_df.document["table_summary"]["summary"]
            source_table_sample = self.doc_df.get_sample_text()
            target_table_description = table_description[target_table]
            target_table_sample = table_samples[target_table]
            transform_reason = self.document["main_table"]["summary"][target_table]
            
            summary, messages = get_concept_mapping(source_table_description, source_table_sample, target_table_description, target_table_sample, transform_reason)
            progress.value += 1

            for message in messages:
                write_log(message['content'])
                write_log("-----------------------------------")


            self.document["concept_mapping"][target_table] = {}
            self.document["concept_mapping"][target_table]["summary"] = summary
            if LOG_MESSAGE_HISTORY:
                self.document["concept_mapping"][target_table]["history"] = messages

        summary = self.document["concept_mapping"][target_table]["summary"]

        print(f""" {BOLD}Plan to map attributes from source to target table:{END}""")
        

        def display_mapping(summary):
            for mapping in summary:
                source_attributes = mapping["source_columns"]
                target_attributes = mapping["target_columns"]
                reason = mapping["reason"]
                print(f"{BOLD}{', '.join(source_attributes)}{END}")
                print(f"     Can be mapped to {BOLD}{', '.join(target_attributes)}{END}")
                print(f"    {ITALIC}{reason}{END}")

                has_warning = False
                for source_attribute in source_attributes:
                    warnings = self.doc_df.get_column_warnings(source_attribute)
                    if warnings:
                        has_warning = True
                        print(f"\n     {BOLD}{source_attribute}{END} has Data Quality Issues:")
                        for idx, warning in enumerate(warnings):
                            warning_type = warning["type"]
                            warning_explanation = warning["explanation"]
                            print(f"        {idx+1}. {BOLD}{warning_type}{END}: {ITALIC}{warning_explanation}{END}")
                if has_warning:
                    print(f"     It's recommended to first clean the data before transformation.")
                print()

        display_mapping(summary)

        submit_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='',
            tooltip='Click to submit',
        )

        def on_submit_button_clicked(b):
            clear_output(wait=True)
            next_step(target_table)
            

        submit_button.on_click(on_submit_button_clicked)

        display(submit_button)

        print(f"""\n Some attributes are not supported:""")
        print(f"""    1.  concept id: vocabulary standardization is under development""")
        print(f"""    2.  foreign key: table connection is under development""")
        print(f""" Please send a feature request if you want them!""")    
    
    def write_codes2(self, target_table):

        print(" Writing the codes...")

        next_step = self.complete

        concept_mapping = self.document["concept_mapping"][target_table]["summary"]

        progress = self.show_progress(len(concept_mapping))

        for mapping in concept_mapping:

            target_attributes = mapping["target_columns"]
            
            potential_attributes = attributes_description[target_table]
            
            target_attributes = [attr for attr in target_attributes if attr in potential_attributes]

            if not target_attributes:
                progress.value += 1
                continue

            source_attributes = mapping["source_columns"]

            key = str(target_attributes) + str(source_attributes)

            reason = mapping["reason"]

            progress.value += 1
            
            if "code_mapping" not in self.document:
                self.document["code_mapping"] = {}
            else:
                if key in self.document["code_mapping"]:
                    write_log(f"Warning: code_mapping for {key} already exists in the document.")
                    continue

            source_table_description  = self.doc_df.get_basic_description(sample_cols=source_attributes, cols=source_attributes)
            
            codes, messages = write_code_and_debug(key=key, 
                                                source_attributes=source_attributes, 
                                                source_table_description=source_table_description, 
                                                target_attributes=target_attributes, 
                                                df=self.doc_df.df,
                                                target_table=target_table)

            for message in messages:
                write_log(message['content'])
                write_log("-----------------------------------")

            self.document["code_mapping"][key] = {}
            self.document["code_mapping"][key]["summary"] = codes
            if LOG_MESSAGE_HISTORY:
                self.document["code_mapping"][key]["history"] = messages

        final_node = self.pipeline.find_final_node()
        final_node_idx = self.pipeline.nodes.index(final_node)

        transform_step_indices = []
        project_step_indices = []

        sample_df = self.pipeline.run_codes()[:4]

        for mapping in concept_mapping:
            target_attributes = mapping["target_columns"]

            potential_attributes = attributes_description[target_table]
            
            target_attributes = [attr for attr in target_attributes if attr in potential_attributes]

            if not target_attributes:
                continue

            source_attributes = mapping["source_columns"]

            key = str(target_attributes) + str(source_attributes)


            code = self.document["code_mapping"][key]["summary"]

            transform_step = TransformationStep(name = "Column Transformation",
                                                        explanation=f"""Map from source table {BOLD}{str(source_attributes)}{END} to target table {BOLD}{str(target_attributes)}{END}""", 
                                                        codes=code, 
                                                        sample_df=sample_df)
            
            
            step_index = self.pipeline.add_new_step(transform_step)
            transform_step_indices.append(step_index)

            project_step = ProjectionStep(name = "Column Projection",
                                          cols=target_attributes)

            step_index = self.pipeline.add_new_step(project_step)
            project_step_indices.append(step_index)


            

        concat_step = ConcatenateHorizontalStep()
        concat_step_idx = self.pipeline.add_new_step(concat_step)

        self.pipeline.append_id()

        for transform_step_idx in transform_step_indices:
            self.pipeline.add_edge_by_index(final_node_idx, transform_step_idx)

        for i in range(len(transform_step_indices)):
            self.pipeline.add_edge_by_index(transform_step_indices[i], 
                                            project_step_indices[i])
        
        for project_step_idx in project_step_indices:
            self.pipeline.add_edge_by_index(project_step_idx, concat_step_idx)

        self.pipeline.display()

    def print_codes(self):
        self.pipeline.print_codes()
    
    def run_codes(self):
        return self.pipeline.run_codes()

    def decide_one_one_table(self, target_table):

        next_step = self.concept_mapping

        if "one_to_one" not in self.document:
            self.document["one_to_one"] = {}

        if target_table in self.document["one_to_one"]:
            write_log(f"Warning: {target_table} already exists in the document for one_to_one.")
        else:
            print(" Identifying the row mapping...")

            progress = self.show_progress(1)


            source_table_description = self.doc_df.document["table_summary"]["summary"]
            source_table_sample = self.doc_df.get_sample_text()
            target_table_description = table_description[target_table]
            target_table_sample = table_samples[target_table]
            transform_reason = self.document["main_table"]["summary"][target_table]
            
            summary, messages = decide_one_one(source_table_description, source_table_sample, target_table_description, target_table_sample, transform_reason)
            progress.value += 1

            for message in messages:
                write_log(message['content'])
                write_log("-----------------------------------")

            if not isinstance(summary["1:1"], bool):
                raise ValueError(f"summary['1:1'] is not a boolean.")
            
            self.document["one_to_one"][target_table] = {}
            self.document["one_to_one"][target_table]["summary"] = summary
            if LOG_MESSAGE_HISTORY:
                self.document["one_to_one"][target_table]["history"] = messages

        summary = self.document["one_to_one"][target_table]["summary"]

        if summary["1:1"]:
            next_step(target_table)
        else:
            display(HTML(" <b>Source doesn't have 1-1 row mapping with Target:</b> " + summary["reason"]))
            print(" M-N row mapping is under development. Please send a feature request!")
        


    def decide_main_table(self):

        next_step = self.decide_one_one_table


        json_code = self.document["main_table"]["summary"]


        source_table_description = self.doc_df.document["table_summary"]["summary"]
        source_table_description = replace_asterisks_with_tags(source_table_description)

        display(HTML("<b>Source table</b> " + source_table_description))
                    

        if json_code:
            print(" Below are potential tables to transform to.")

            for key in json_code:
                print(f'    {BOLD}{key}{END}: {json_code[key]}')

            print(" Please choose one table to transform to.")

            radio_options = widgets.RadioButtons(
                options=list(json_code.keys()) + ['Manually Specify'],
                description='',
                disabled=False
            )
        else:
            print(f" It doesn't seem to be related to any table in common data model. \n Please manually specify the \033[1mmost\033[0m related table.")
            radio_options = widgets.RadioButtons(
                options=['Manually Specify'],
                description='',
                disabled=False
            )

        dropdown = widgets.Dropdown(
            options=self.tables,
            description='',
            disabled= (True if json_code else False),
            layout={'display': 'none' if json_code else ''}  
        )

        def on_radio_selection_change(change):
            if change['new'] == 'Manually Specify':
                dropdown.disabled = False
                dropdown.layout.display = ''
            else:
                dropdown.disabled = True
                dropdown.layout.display = 'none'

        radio_options.observe(on_radio_selection_change, names='value')

        submit_button = widgets.Button(
            description='Submit',
            disabled=False,
            button_style='',
            tooltip='Click to submit',
        )

        def on_submit_button_clicked(b):
            if radio_options.value == 'Manually Specify':
                main_table = dropdown.value
            else:
                main_table = radio_options.value

            clear_output(wait=True)

            next_step(main_table)
            

        submit_button.on_click(on_submit_button_clicked)

        container = widgets.VBox([radio_options, dropdown, submit_button])

        display(container)

    def write_codes(self):
        
        next_step = self.complete

        print(" Writing the codes...")

        return

        concept_mapping = self.document["concept_mapping"]
        
        source_concepts = self.doc_df.document["column_grouping"]["summary"]

        target_to_source = {}

        for key, value in concept_mapping.items():
            source_path = key.strip("[]").replace("'", "").split(", ")
            
            if "summary" in value:
                for summary_mapping in value["summary"]:
                    target_path = str(summary_mapping)
                    if target_path not in target_to_source:
                        target_to_source[target_path] = source_path
                    else:
                        target_to_source[target_path].append(source_path)

        progress = self.show_progress(len(target_to_source))

        for key, source_path in target_to_source.items():
            target_path = key.strip("[]").replace("'", "").split(", ")
            target_attributes = get_value_from_path(target_concepts, target_path)
            source_attributes = get_value_from_path(source_concepts, source_path)

            progress.value += 1
            
            if "code_mapping" not in self.document:
                self.document["code_mapping"] = {}
            else:
                if key in self.document["code_mapping"]:
                    write_log(f"Warning: code_mapping for {key} already exists in the document.")
                    continue
            
            self.write_code_single(key, source_attributes, target_attributes)

        self.target_df = pd.DataFrame()    

        for key, source_path in target_to_source.items():
            target_path = key.strip("[]").replace("'", "").split(", ")
            target_attributes = get_value_from_path(target_concepts, target_path)
            source_attributes = get_value_from_path(source_concepts, source_path)


            print(f"""Codes that map 
# from source table {BOLD}{str(source_path)}{END} ({ITALIC}{str(source_attributes)}{END})
# to target table {BOLD}{str(target_path)}{END} ({ITALIC}{str(target_attributes)}{END})""")

            code = self.document["code_mapping"][key]
            print()
            print("-" * 80) 
            print(highlight(code, PythonLexer(), Terminal256Formatter()))
            print("-" * 80) 
            print()

            
            exec(code, globals())
            temp_target_df = etl(self.doc_df.df)
            for col in temp_target_df.columns:
                self.target_df[col] = temp_target_df[col]

        
    
    def write_code_single(self, key, source_attributes, target_attributes):

        target_attributes = "\n".join(f"{idx + 1}. {target_attribute}: {attributes_description[target_attribute]}" for idx, target_attribute in enumerate(target_attributes))

        template =  f"""ETL task: Given Source Table, tansform it into Target Table with new columns. 

Source table:
{self.doc_df.get_basic_description(sample_cols=source_attributes, cols=source_attributes)}

The target table needs columns:
{target_attributes}

Do the following:
1. First reason about how to extract the columns
2. Then fill in the python function with detailed comments.
```python
def etl(source_df):
    target_df = pd.DataFrame()
    ...
    return target_df
```"""

        messages = [{"role": "user", "content": template}]

        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        python_code = extract_python_code(response['choices'][0]['message']['content'])

        detailed_error_info = None

        max_tries = 2

        while max_tries > 0:
            max_tries -= 1

            try:
                exec(python_code, globals())
                temp_target_df = etl(self.doc_df.df)
            except Exception: 
                detailed_error_info = get_detailed_error_info()

            if detailed_error_info is None:
                self.document["code_mapping"][key] = python_code
                return


            error_message = f"""There is a bug in the code: {detailed_error_info}.
First, study the error message and point out the problem.
Then, fix the bug and return the codes in the following format:
```python
def etl(source_df):
    target_df = pd.DataFrame()
    ...
    return target_df
```"""
            messages = [{"role": "user", "content":template},
            {"role": "assistant", "content": python_code},
            {"role": "user", "content": error_message},]

            response = call_gpt4(messages, temperature=0.1, top_p=0.1)

            write_log(error_message)
            write_log("-----------------------------------")
            write_log(response['choices'][0]['message']['content'])

            python_code = extract_python_code(response['choices'][0]['message']['content'])
    
        raise Exception("The code is not correct. Please try again.")


    def map_concept(self):

        next_step = self.write_codes

        print("Mapping the concepts...")

        target_concept = {self.document["main_table"]["summary"]["concept"]:
                            target_concepts[self.document["main_table"]["summary"]["concept"]]}

        def dict_to_descriptive_list(data_dict, parent_keys=None):
            if parent_keys is None:
                parent_keys = []

            descriptive_map = {}

            for key, value in data_dict.items():
                current_keys = parent_keys + [key]

                if isinstance(value, dict):
                    descriptive_map.update(dict_to_descriptive_list(value, current_keys))
                elif isinstance(value, list):
                    keys_path = '[' + ', '.join(current_keys) + ']'
                    
                    attributes = ', '.join(value)

                    description = f"{keys_path}, with {len(value)} attributes: {attributes}"

                    descriptive_map[str(current_keys)] = description

            return descriptive_map
        
        source_concept = dict_to_descriptive_list(self.doc_df.document["column_grouping"]["summary"])

        progress = self.show_progress(len(source_concept))

        for keys_path in source_concept:
            description = source_concept[keys_path]

            if "concept_mapping" not in self.document:
                self.document["concept_mapping"] = {}
            else:
                if keys_path in self.document["concept_mapping"]:
                    write_log(f"Warning: concept_mapping for {keys_path} already exists in the document.")
                    continue

            self.map_concept_single(keys_path, description, target_concept)

            progress.value += 1

        clear_output(wait=True)

        source_concepts = self.doc_df.document["column_grouping"]["summary"]

        concept_mapping = self.document["concept_mapping"]



        def display_mapping(concept_mapping, target_concepts, source_concepts):
            results = []

            for key, value in concept_mapping.items():
                source_path = key.strip("[]").replace("'", "").split(", ")
                
                source_attributes = get_value_from_path(source_concepts, source_path)
                
                print(f"{BOLD}{'->'.join(source_path)}{END} ({ITALIC}{', '.join(source_attributes)}{END})")
                if "summary" in value:
                    for summary_mapping in value["summary"]:
                        target_path = summary_mapping
                        target_attributes = get_value_from_path(target_concepts, target_path)
                        matching_source_attributes = ", ".join(summary_mapping[-3:])
                        print(f"    Can be mapped to {BOLD}{'->'.join(target_path)}{END} ({ITALIC}{', '.join(target_attributes)}{END}) attributes.")
                else:
                    print(f"    Can't be used for any attributes.")


        display_mapping(concept_mapping, target_concepts, source_concepts)
        
        submit_button = widgets.Button(
            description='Next',
            disabled=False,
            button_style='',
            tooltip='Click to submit',
        )

        def on_submit_button_clicked(b):
            
            clear_output(wait=True)

            next_step()
            

        submit_button.on_click(on_submit_button_clicked)

        display(submit_button)

    def map_concept_single(self, keys_path, description, target_concept):

        key_path_list = keys_path.strip("[]").replace("'", "").split(", ")

        def extract_paths(data, path=None, results=None):
            if path is None:
                path = []
            if results is None:
                results = []

            for key, value in data.items():
                new_path = path + [key]
                if isinstance(value, dict):
                    extract_paths(value, new_path, results)
                elif isinstance(value, list):
                    results.append(new_path)

            return results

        paths = extract_paths(target_concept)

        paths_str = "\n".join(f"{idx + 1}. {item}" for idx, item in enumerate(paths))

        template = f"""You have list of target concepts about {list(target_concept.keys())[0]}. Each concept is a list from category to specifics:
{paths_str}

You have a source table about: {description}. 
The goal is to transform from source to target.

Enumerate the target concepts that the source table can be potentially mapped to, in the following format (empty list if no relevant):
```json
[["{list(target_concept.keys())[0]}",...,"Leaf Category"], 
  ["{list(target_concept.keys())[0]}", ...]...]
```"""
        messages = [{"role": "user", "content": template}]

        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)


        
        if len(json_code) == 0:
            self.document["concept_mapping"][keys_path] = {}
            return


        
        result = "\n".join(f'{idx + 1}. {path}: {get_value_from_path(target_concept, path)}' for idx, path in enumerate(json_code))

        template = f"""You have list of target concepts about {list(target_concept.keys())[0]}. Each concept is a list from category to specifics:
{result}

You have a source table about: {description}. 
**Assumption: {list(target_concept.keys())[0]} is semantically similar or more general to {key_path_list[0]}!!**

Exclude target concepts that are obviously semantically different.
E.g., ["Person", "birth date"] and ["Patient, "death date"]
"birth date" and "death date", despite both are about date, are obviously different.
Sometimes, the difference is not obvious. E.g., "birth date" and "birth year" are the same.

Based on the assumption, exclude the conpets that are obviously different, but keep the ones that are unsure.
Return the remaining concepts in the following format (empty list if no).
```json
[["{list(target_concept.keys())[0]}",...,"Leaf Category"], 
  ["{list(target_concept.keys())[0]}", ...]...]
```"""

        messages = [{"role": "user", "content": template}]
        
        response = call_gpt4(messages, temperature=0.1, top_p=0.1)
        
        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)


        
        if len(json_code) == 0:
            self.document["concept_mapping"][keys_path] = {}
            return
        


        
        result = "\n".join(f'{idx + 1}. {path}: {get_value_from_path(target_concept, path)}' for idx, path in enumerate(json_code))

        template = f"""You have list of target concepts about {list(target_concept.keys())[0]}. Each concept is a list from category to specifics. The right side is its attributes:
{result}

You have a source table about: {description}. 
First, go through the attributes of target concept. Argue if any can be transformed from the source table.
E.g., "Student height" cannot be transformed from "Student weight" as they are different measurements.
E.g., "Student age" can be transformed from "Student birth date" by calculating the date difference.

Then, enumerate the target concept where there exists attributes can be transformed (empty list if no):
```json
[["{list(target_concept.keys())[0]}",...,"Leaf Category"], 
  ["{list(target_concept.keys())[0]}", ...]...]
```"""
        messages = [{"role": "user", "content": template}]
        
        response = call_gpt4(messages, temperature=0.1, top_p=0.1) 
        
        write_log(template)    
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])
        
        processed_string  = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = json.loads(processed_string)



        self.document["concept_mapping"][keys_path] = {}

        if len(json_code) == 0:    
            return
        
        self.document["concept_mapping"][keys_path]["summary"] = json_code
        

def check_functional_dependency(df, determinant, dependent):
    groups = df.groupby(list(determinant))[dependent].nunique()
    is_functionally_dependent = (groups == 1).all()
    return is_functionally_dependent

def combine_pipelines(pipelines):
    start_id = 1
    for pipeline in pipelines:
        start_id = pipeline.append_id(start_id)
    
    steps = []
    nodes = []
    edges = []

    for pipeline in pipelines:
        steps.extend(pipeline.steps)
        nodes.extend(pipeline.nodes)
        edges.extend(pipeline.edges)
    
    return TransformationPipeline(steps=steps, edges=edges, nodes=nodes)

class TransformationPipeline:

    def __init__(self, steps, edges, nodes=None):
        self.steps = steps
        if nodes is None:
            nodes = [step.name for step in steps]
        self.nodes = nodes
        self.edges = edges


    def get_step(self, node):
        return self.steps[self.nodes.index(node)]

    def append_id(self, start_id=1):
        id_mapping = {}
        new_nodes = []
        current_id = start_id

        for node in self.nodes:
            node_without_id = re.sub(r'^\d+\.\s*', '', node)

            new_node_name = f"{current_id}. {node_without_id}"
            id_mapping[node] = new_node_name
            new_nodes.append(new_node_name)
            current_id += 1

        self.nodes = new_nodes

        new_edges = []
        for source, target in self.edges:
            new_source = id_mapping.get(source, source)
            new_target = id_mapping.get(target, target)
            new_edges.append((new_source, new_target))

        self.edges = new_edges

        return current_id

    def validate_graph(self):
        graph = {node: [] for node in self.nodes}
        for source, target in self.edges:
            graph[source].append(target)

        visited = set()
        recursion_stack = set()

        def is_cyclic(node):
            if node in recursion_stack:
                return True
            if node in visited:
                return False

            visited.add(node)
            recursion_stack.add(node)

            for neighbour in graph[node]:
                if is_cyclic(neighbour):
                    return True

            recursion_stack.remove(node)
            return False

        for node in self.nodes:
            if node not in visited:
                if is_cyclic(node):
                    raise ValueError("The graph is cyclic")

        return

    def display_workflow(self):
        display_workflow(self.nodes, self.edges)

    def display(self):
        

        def create_widget(instances):
            dropdown = widgets.Dropdown(
                options=self.nodes,
                disabled=False,
            )

            button1 = widgets.Button(description="View")

            button2 = widgets.Button(description="Edit")

            def on_button_clicked(b):
                clear_output(wait=True)
                display_workflow(self.nodes, self.edges)
                display(dropdown)
                display(buttons)
                idx = self.nodes.index(dropdown.value)

                selected_instance = instances[idx]
                selected_instance.display()

            def on_button_clicked2(b):
                clear_output(wait=True)

                idx = self.nodes.index(dropdown.value)
                selected_instance = instances[idx]

                def call_back_display(step):
                    clear_output(wait=True)
                    self.display()

                selected_instance.edit_widget(callbackfunc=call_back_display)

            button1.on_click(on_button_clicked)
            button2.on_click(on_button_clicked2)

            display_workflow(self.nodes, self.edges)

            buttons = widgets.HBox([button1, button2])

            display(dropdown, buttons)


        create_widget(self.steps)


    def get_codes(self):
        sorted_nodes = topological_sort(self.nodes, self.edges)

        codes = "import pandas as pd\nimport numpy as np\n\n"

        for node in sorted_nodes:

            target_id = self.nodes.index(node)
            source_ids = get_source_nodes_ids(self.nodes, self.edges, node)

            step = self.steps[target_id]

            codes += step.get_codes(target_id=target_id, source_ids=source_ids)

        return codes

    
    def run_codes(self):

        sorted_nodes = topological_sort(self.nodes, self.edges)
        
        results = {}

        for node in sorted_nodes:

            target_id = self.nodes.index(node)
            source_ids = get_source_nodes_ids(self.nodes, self.edges, node)

            step = self.steps[target_id]

            source_dfs = [results[source_id] for source_id in source_ids]

            result = step.run_codes(dfs=source_dfs)

            if isinstance(result, pd.DataFrame):
                results[target_id] = result
            elif isinstance(result, str):
                write_log(result)
                raise ValueError(result)
            else:
                raise ValueError(f"Step {node} returns a {type(result)} instead of a pandas dataframe.")
        
        final_node = self.find_final_node()
        if final_node is None:
            return results
        else:
            final_node_id = self.nodes.index(final_node)
            return results[final_node_id]


    def print_codes(self):
        codes = self.get_codes()
        
        print(highlight(codes, PythonLexer(), Terminal256Formatter()))

    

    def find_final_node(self):

        graph = {node: [] for node in self.nodes}

        for source, target in self.edges:
            graph[source].append(target)

        final_node = None
        for node, neighbours in graph.items():
            if len(neighbours) == 0:
                if final_node is not None:
                    return None
                final_node = node

        if final_node is None:
            return None

        def can_reach_final(node, visited):
            if node == final_node:
                return True
            if node in visited:
                return False

            visited.add(node)
            for neighbour in graph[node]:
                if can_reach_final(neighbour, visited):
                    return True
            return False

        for node in self.nodes:
            if node != final_node and not can_reach_final(node, set()):
                return None

        return final_node

    def add_step(self, new_step, parent_node, relabel=True):

        if len(self.nodes) == 0 and parent_node is None:
            self.steps.append(new_step)
            self.nodes.append(new_step.name)
            return

        if parent_node not in self.nodes:
            raise ValueError(f"Node {parent_node} doesn't exist in the graph.")

        if new_step.name in self.nodes:
            if not relabel:
                raise ValueError(f"Node name {new_step.name} already exists in the graph.")
        
        self.steps.append(new_step)
        self.nodes.append(new_step.name)

        if relabel:
            parent_node_id = self.nodes.index(parent_node)
            self.append_id()
            self.edges.append((self.nodes[parent_node_id], self.nodes[-1]))
        else:
            self.edges.append((child_node, new_step.name))
        

    
    def add_step_right_after(self, new_step, parent_node, relabel=True):
        
        if parent_node not in self.nodes:
            raise ValueError(f"Node {parent_node} doesn't exist in the graph.")

        if new_step.name in self.nodes:
            if not relabel:
                raise ValueError(f"Node name {new_step.name} already exists in the graph.")
        
        self.steps.append(new_step)
        self.nodes.append(new_step.name)

        children_indices = []
        edges_to_remove = []
        for p_node, c_node in self.edges:
            if p_node == parent_node:
                children_indices.append(self.nodes.index(c_node))
                edges_to_remove.append((p_node, c_node))

        for edge in edges_to_remove:
            self.edges.remove(edge)

        if relabel:
            parent_node_id = self.nodes.index(parent_node)

            self.append_id()
            self.edges.append((self.nodes[parent_node_id], self.nodes[-1]))

            for idx in children_indices:
                self.edges.append((self.nodes[-1], self.nodes[idx]))
        else:

            self.edges.append((parent_node, new_step.name))

            for idx in children_indices:
                self.edges.append((self.nodes[-1], self.nodes[idx]))


    def add_step_to_final(self, new_step):

        final_node = self.find_final_node()
        if final_node is None:
            raise ValueError("The graph doesn't have a final node.")

        self.add_step(new_step, final_node)



    def add_edges(self, edges):
        self.edges.extend(edges)


    
    def add_new_step(self, new_step):
        self.steps.append(new_step)
        self.nodes.append(new_step.name)
        return len(self.nodes) - 1

    def add_edge_by_index(self, source_idx, target_idx):
        self.edges.append((self.nodes[source_idx], self.nodes[target_idx]))


    def start(self):
        pass

    def __repr__(self):
        self.display()
        return ""


class TransformationStep:

    def __init__(self, name = None, explanation="", codes="", sample_df=None):
        if name is None:
            self.name = "Transformation Task"
        else:
            self.name = name

        self.codes = codes
        self.explanation = explanation

        if sample_df is not None:
            self.sample_df = sample_df.copy()

        self.reason = ""
            

    def verify_input(self, df):
        if not isinstance(df, pd.DataFrame):
            raise ValueError("Input is not a pandas dataframe.")

    def verify_output(self, df):
        if not isinstance(df, pd.DataFrame):
            raise ValueError("Output is not a pandas dataframe.")


    def generate_codes(self, explanation=None):
        if explanation is None:
            explanation = self.explanation
        template = f"""Transformation task: Given input df, write python codes that transform and ouput df.
===
Input Df:
{self.sample_df.to_csv()}
===
Transformation Requirement
{explanation}

Do the following:
1. First reason about how to transform
2. Then fill in the python function, with detailed comments. 
DONT change the function name, first line and the return clause.

{{
    "reason": "To transform, we need to ...",
    "codes": "def transform(input_df):\\n    output_df = input_df.copy()\\n    ...\\n    return output_df"
}}
"""
        messages = [{"role": "user", "content": template}]
    
        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        write_log(template)
        write_log("-----------------------------------")
        write_log(response['choices'][0]['message']['content'])

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_code = replace_newline(json_code)
        json_code = json.loads(json_code)
        
        self.reason = json_code["reason"]
        self.codes = json_code["codes"]


    def run_codes(self, dfs, codes = None):

        if codes is None:
            codes = self.codes
        
        if isinstance(dfs, list):
            df = dfs[0]
        else:
            df = dfs

        input_df = df.copy()
        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(codes, globals())
            temp_target_df = transform(input_df) 
            self.verify_output(temp_target_df)
            return temp_target_df
        except Exception: 
            detailed_error_info = get_detailed_error_info()
            return detailed_error_info




    def edit_widget(self, callbackfunc=None):

        print("\033[91mRemember to save after editing!\033[0m")

        explanation_label = widgets.Label('Task:')
        explanation_text = widgets.Textarea(
            value = self.explanation if self.explanation != "" else "Transform ... For example ...",
            layout=widgets.Layout(width='95%', height='200px')
        )
        submit_button = widgets.Button(description="Submit Task")

        def on_submit_clicked(b):
            print("Generating codes...")
            self.generate_codes(explanation = explanation_text.value)

            codes_text.value = self.codes
            reason_label.value = self.reason
            print("Done")

        submit_button.on_click(on_submit_clicked)

        codes_label = widgets.Label('Codes:')
        codes_text = widgets.Textarea(
            value=self.codes,
            layout=widgets.Layout(width='95%', height='200px')
        )
        run_button = widgets.Button(description="Run Codes")
        reason_label = widgets.Label(layout=Layout(width='100%', overflow='auto', white_space='pre-wrap'))
        output_label = widgets.Label()

        def on_run_clicked(b):
            print("Running codes...")

            output = self.run_codes(dfs = self.sample_df, codes = codes_text.value)
            if isinstance(output, str):
                error_label.value = "<span style='color: red;'>" + output.replace("\n", "<br>") + "</span>"
                output_df_widget.value = ""
            elif isinstance(output, pd.DataFrame):
                error_label.value = ""
                output_df_widget.value = output.to_html(border=0)
            else:
                raise ValueError("Output is neither a string nor a pandas dataframe.")
            print("Done")
                

        run_button.on_click(on_run_clicked)


        panel_layout = Layout(width='400px')

        left_panel = widgets.VBox([explanation_label, explanation_text, submit_button], layout=panel_layout)
        right_panel = widgets.VBox([codes_label, codes_text, run_button, output_label], layout=panel_layout)
        display(widgets.HBox([left_panel, right_panel]))
        display(reason_label)

        input_df_label = widgets.Label('Input Table:')
        display(input_df_label)
        display(HTML(self.sample_df.to_html(border=0)))

        output_df_label = widgets.Label('Output Table:')
        output_df_widget = widgets.HTML(
            value='',
            placeholder='Output DataFrame will be shown here',
            description='',
        )
        
        error_label = widgets.HTML(layout=Layout(overflow='auto'))

        save_button = widgets.Button(description="Save the Step")

        def on_save_clicked(b):
            print("Saving ...")
            self.codes = codes_text.value
            self.explanation = explanation_text.value
            self.reason = reason_label.value
            callbackfunc(self)
            print("Done")

        save_button.on_click(on_save_clicked)

        display(widgets.VBox([
                                output_df_label, 
                                output_df_widget, 
                                error_label]))

        display(save_button)

        if self.codes != "":
            on_run_clicked(run_button)

    def get_sample_output(self):
        result = self.run_codes(self.sample_df)
        if isinstance(result, str):
            raise ValueError(f"The codes for step {self.name} is not correct: {result} \n Please edit the codes.")
        elif isinstance(result, pd.DataFrame):
            return result
        else:
            raise ValueError("Output is neither a string nor a pandas dataframe.")

    def display(self):
        print(f"{BOLD}{self.name}{END}: {self.explanation}")
        display(HTML(f"<hr>"))
        print(f"{BOLD}Codes{END}:")
        print(highlight(self.codes, PythonLexer(), Terminal256Formatter()))

        if hasattr(self, 'sample_df'):
            display(HTML(f"<hr><b>Example Input</b>: {self.sample_df.to_html()}"))
            
            if not hasattr(self, 'output_sample_df'):
                self.output_sample_df = self.run_codes(self.sample_df)
            
            display(HTML(f"<hr><b>Example Output</b>: {self.output_sample_df.to_html()}"))

    def __repr__(self):
        self.display()
        return ""

    def rename_based_on_explanation(self):
        title, message = give_title(self.explanation)
        self.name = title

    def get_codes(self, target_id=0, source_ids=[]):
        source_ids_str = ", ".join([f"df_{source_id}" for source_id in source_ids])
        return self.codes + "\n\n" + f"df_{target_id} = transform({source_ids_str})\n\n"





class MultiDFTransformationStep(TransformationStep):
    def __init__(self, name=None, explanation="", codes="", sample_dfs=None):
        super().__init__(name, explanation, codes)
        if sample_dfs is not None:
            self.sample_dfs = [df.copy() for df in sample_dfs]
        else:
            self.sample_dfs = []

    def run_codes(self, dfs):
        input_dfs = [df.copy() for df in dfs]
        try:
            if 'transform' in globals():
                del globals()['transform']
            exec(self.codes, globals())
            temp_target_df = transform(*input_dfs)
            return temp_target_df
        except Exception:
            detailed_error_info = get_detailed_error_info()
            return detailed_error_info
        
        

    def display(self):
        display(HTML(f"<b>{self.name}</b>: {self.explanation} <hr> <b>Codes</b>:"))
        print(highlight(self.codes, PythonLexer(), Terminal256Formatter()))

        if hasattr(self, 'sample_dfs'):
            for i, df in enumerate(self.sample_dfs):
                display(HTML(f"<hr><b>Example Input {i+1}</b>: {df.to_html()}"))

            if not hasattr(self, 'output_sample_dfs'):
                self.output_sample_dfs = self.run_codes(self.sample_dfs)

            for i, df in enumerate(self.output_sample_dfs):
                display(HTML(f"<hr><b>Example Output {i+1}</b>: {df.to_html()}"))
    
    def generate_codes(self):
        raise NotImplementedError("generate_codes() is not implemented for MultiDFTransformationStep.")


class ConcatenateHorizontalStep(MultiDFTransformationStep):
    def __init__(self, name=None, explanation="", codes="", sample_dfs=None):
        super().__init__(name, explanation, codes, sample_dfs)
        self.generate_concatenate_horizontal_codes()

        if name is None:
            self.name = "Concatenate Horizontal"

    def generate_concatenate_horizontal_codes(self):
        self.codes = """# Concatenate all dataframes horizontally
def transform(*dfs):
    return pd.concat(dfs, axis=1)"""
    

class SourceStep(TransformationStep):
    def __init__(self, doc_df, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.explanation = ""
        self.doc_df = doc_df
        self.name = doc_df.table_name

    def generate_codes(self):
        pass

    def run_codes(self, dfs):
        return self.doc_df.df

    def display(self):
        display(HTML(self.doc_df.original_df[:5].to_html()))

    def edit_widget(self, callbackfunc=None):
        self.display()
        print("\033[91mEdit Source File is under development!\033[0m")

        return_button = widgets.Button(description="Return")

        def on_return_clicked(b):
            clear_output(wait=True)
            callbackfunc(self)
        
        return_button.on_click(on_return_clicked)

        display(return_button)
    
    def get_codes(self, target_id=0, source_ids=[]):
        return f"df_{str(target_id)} = pd.read_csv(ADD_YOUR_SOURCE_FILE_PATH_HERE) \n\n"

class RemoveMissingValueStep(TransformationStep):

    def __init__(self, col, reason, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.col = col
        self.explanation = f"""Remove the rows with missing values in column {col}"""
        self.generate_remove_missing_value_codes()
        self.generate_missing_value_samples()

        if 'name' not in kwargs:
            self.name = f"Remove NULL for {col}"


    def generate_remove_missing_value_codes(self):
        self.codes = f"""# Remove the rows with missing values in column {self.col}
def transform(df):
    output_df = df.copy()
    output_df = output_df.dropna(subset=["{self.col}"])
    return output_df"""


    def generate_missing_value_samples(self):
        sample_df1 = self.sample_df[self.sample_df[self.col].notnull()][:2]
        sample_df2 = self.sample_df[self.sample_df[self.col].isnull()][:2]
        sample_df = pd.concat([sample_df1, sample_df2])
        self.sample_df = sample_df


class ProjectionStep(TransformationStep):

    def __init__(self, cols, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.cols = cols

        all_cols = ", ".join(self.cols)

        self.explanation = f"""Keep only column {all_cols}"""
        self.generate_projection_codes()

        if hasattr(self, 'sample_df'):
            self.generate_projection_samples()

        if 'name' not in kwargs:
            self.name = f"Project {all_cols}"

    def generate_projection_codes(self):
        all_cols = ', '.join([f'"{col}"' for col in self.cols])

        self.codes = f"""# Keep only specified columns
def transform(df):
    output_df = df.copy()
    output_df = output_df[[{all_cols}]]
    return output_df"""

    def generate_projection_samples(self):
        sample_df = self.sample_df[:4]
        self.sample_df = sample_df



class RegexTransformationStep(TransformationStep):

    def __init__(self, col, unique_values, reason, *args, **kwargs):

        super().__init__(*args, **kwargs)

        if 'name' not in kwargs:
            self.name = f"Clean {col}"

        self.explanation = f"""Remove the unusual values from column {col}: {reason}"""

        self.col = col
        self.unique_values = unique_values
        self.reason = reason

    def generate_regex_codes(self, explanation=None):
        if not hasattr(self, 'labeled_result'):
            raise ValueError("Please run the edit_widget() function first.")

        unusual_values = self.labeled_result["unusual_values"]
        normal_values = self.labeled_result["normal_values"]


        self.sample_df[self.col] = self.sample_df[self.col].astype(str)

        sample_df1 = self.sample_df[self.sample_df[self.col].isin(normal_values)][:2]
        sample_df2 = self.sample_df[self.sample_df[self.col].isin(unusual_values)][:2]
        sample_df = pd.concat([sample_df1, sample_df2])
        self.sample_df = sample_df
        
        print("Generating codes...")

        progress = show_progress(1)
        try:
            regex_result = find_regex_pattern(self.col, self.labeled_result)
        except Exception as e:
            write_log(e)
            regex_result = {"exists_regex": False}

        progress.value += 1

        if regex_result["exists_regex"]:
            regex_pattern = regex_result["regex"]
            self.codes = f"""# Clean column {self.col} by removing the unusual rows that don't match the regex pattern
def transform(df):
    output_df = df.copy()

    # Transform the column to string type
    output_df["{self.col}"] = output_df["{self.col}"].astype(str)

    output_df = output_df[output_df["{self.col}"].str.match(r"{regex_pattern}")]
    return output_df"""

            self.explanation = f"""Remove the unusual values from column {self.col} by removing the rows that don't match the regex pattern: {regex_pattern}"""
        
        else:
            unusual_values = self.labeled_result["unusual_values"]
            self.codes = f"""# Clean column {self.col} by removing the unusual rows with unusual values
def transform(df):
    output_df = df.copy()

    # Transform the column to string type
    output_df["{self.col}"] = output_df["{self.col}"].astype(str)

    output_df = output_df[~output_df["{self.col}"].isin({unusual_values})]
    return output_df"""

            self.explanation = f"""Remove the unusual values from column {self.col} by removing the rows with unusual values: {unusual_values}"""

    def parent_edit_widget(self, callbackfunc=None):
        super().edit_widget(callbackfunc)


    def edit_widget(self, callbackfunc=None):

        print("Analyzing the unusual values...")

        if not hasattr(self, 'labeled_result'):
            progress = show_progress(1)
            result = classify_unusual_values(self.col, self.unique_values, self.reason)
            self.labeled_result = result
            progress.value += 1

        def create_tabular_widgets_toggle(result):
            grid_items = []

            grid_items.append(widgets.Label(value=''))
            grid_items.append(widgets.Label(value='Keep (Normal)'))
            grid_items.append(widgets.Label(value='Remove (Unusual)'))

            checkbox_widgets = {}

            for email in result['normal_values'] + result['unusual_values']:
                is_normal = email in result['normal_values']
                
                label = widgets.Label(value=email)
                grid_items.append(label)

                keep_checkbox = widgets.Checkbox(value=is_normal)
                grid_items.append(keep_checkbox)

                remove_checkbox = widgets.Checkbox(value=not is_normal)
                grid_items.append(remove_checkbox)

                checkbox_widgets[email] = (keep_checkbox, remove_checkbox)

                keep_checkbox.observe(lambda change, email=email: on_checkbox_toggle(change, email, 'keep'), names='value')
                remove_checkbox.observe(lambda change, email=email: on_checkbox_toggle(change, email, 'remove'), names='value')

            def on_checkbox_toggle(change, email, action):
                if change['new']:
                    if action == 'keep':
                        checkbox_widgets[email][1].value = False
                    else:
                        checkbox_widgets[email][0].value = False

            grid_layout = widgets.Layout(grid_template_columns="repeat(3, 33%)",
                                        align_items='center')
            grid_box = widgets.GridBox(children=grid_items, layout=grid_layout)

            submit_button = widgets.Button(description="Submit")

            def on_submit_button_clicked(b):
                clear_output(wait=True)

                updated_result = {'unusual_values': [], 'normal_values': []}
                for email, (keep_checkbox, remove_checkbox) in checkbox_widgets.items():
                    if keep_checkbox.value:
                        updated_result['normal_values'].append(email)
                    else:
                        updated_result['unusual_values'].append(email)
                
                self.labeled_result = updated_result
                
                self.generate_regex_codes()

                print("Done")

                self.parent_edit_widget(callbackfunc)


            submit_button.on_click(on_submit_button_clicked)

            display(grid_box, submit_button)

        print("Please verify the unusual values...")

        create_tabular_widgets_toggle(self.labeled_result)



class ColumnRename(TransformationStep):

    def __init__(self, rename_map, *args, **kwargs):

        super().__init__(*args, **kwargs)

        if 'name' not in kwargs:
            self.name = "Rename Columns"

        self.rename_map = rename_map
        self.explanation = f"""Rename columns in the DataFrame based on: \n"""

        valid_renaming = {}

        for old_name, new_name in self.rename_map.items():
            if old_name != new_name:
                valid_renaming[old_name] = new_name

        rename_map_str = "{\n"
        rename_map_str += "\n".join([f"    '{old}': '{new}'," for old, new in valid_renaming.items()])
        rename_map_str += "\n    }"

        self.explanation += rename_map_str
        
        self.generate_rename_codes()

    def verify_input(self, df):
        super().verify_input(df)

        if len(self.rename_map) != len(set(self.rename_map.keys())):
            raise ValueError("Some old column names are duplicated.")
        
        if len(self.rename_map) != len(set(self.rename_map.values())):
            raise ValueError("Some new column names are duplicated.")

        for old_name in self.rename_map.keys():
            if old_name not in df.columns:
                raise ValueError(f"Column {old_name} does not exist in the DataFrame.")

    def generate_rename_codes(self):

        valid_renaming = {}
        
        for old_name, new_name in self.rename_map.items():
            if old_name != new_name:
                valid_renaming[old_name] = new_name

        rename_map_str = "{\n"
        rename_map_str += "\n".join([f"        '{old}': '{new}'," for old, new in valid_renaming.items()])
        rename_map_str += "\n    }"

        self.codes = f"""# Rename columns in the DataFrame
def transform(df):
    rename_map = {rename_map_str}
    df.rename(columns=rename_map, inplace=True)
    return df"""



        



class DocumentedDatabase:
    def __init__(self, doc_dfs):
        self.doc_dfs = doc_dfs
        
    def generate_pipeline(self):
        pipelines = []

        for doc_df in self.doc_dfs:
            pipelines.append(doc_df.generate_pipeline())

        final_pipeline = combine_pipelines(pipelines)
        return final_pipeline

class DataCleaning:
    def __init__(self, doc_df):
        self.doc_df = doc_df
        self.pipeline = doc_df.generate_pipeline()
    
    def __repr__(self):
        self.display()
        return ""

    def run_codes(self):
        return self.pipeline.run_codes()

    def display(self):
        
        self.pipeline.display_workflow()

        display(HTML(self.doc_df.original_df[:5].to_html()))

        attributes = []
        issues = {}

        document = self.doc_df.document

        if "missing_value" in document:
            issues["missing_value"] = []
            for attribute in document["missing_value"]:
                item = document["missing_value"][attribute]
                if item:
                    attributes.append(attribute)
                    issues["missing_value"].append((attribute, item["summary"]))

        if "unusual" in document:
            issues["unusual"] = []
            for attribute in document["unusual"]:
                item = document["unusual"][attribute]["summary"]
                if item["Unusualness"]:
                    attributes.append(attribute)
                    issues["unusual"].append((attribute, item["Examples"]))

        self.recommends_transformation(issues)

    def recommends_transformation(self, issues):

        def on_button_clicked_missing(b):
            clear_output(wait=True)
            self.create_remove_missing_value_step(b.attribute, b.issue)

        boxes = []
        for attribute, issue in issues["missing_value"]:
            label = widgets.HTML(value=f" <b>{attribute}</b> has missing values: {issue}")

            button = widgets.Button(description=f"Remove them")
            button.attribute = attribute
            button.issue = issue
            button.on_click(on_button_clicked_missing)

            box = widgets.VBox([label, button])
            boxes.append(box)

        def on_button_clicked(b):
            clear_output(wait=True)
            unique_values = self.doc_df.df[b.attribute].dropna().unique()[:20]
            self.create_regex_step(b.attribute, b.issue, unique_values)
            
        for attribute, issue in issues["unusual"]:
            label = widgets.HTML(value=f" <b>{attribute}</b> has unusual value: {issue}")

            button = widgets.Button(description=f"Remove them")
            button.attribute = attribute
            button.issue = issue
            button.on_click(on_button_clicked)

            box = widgets.VBox([label, button])
            boxes.append(box)

        label = widgets.HTML(value=f" Want to perform an ad hoc transformation?")

        def on_ad_hoc_clicked(b):
            clear_output(wait=True)
            self.create_ad_hoc_step()

        adhoc_button = widgets.Button(description="Ad hoc")
        adhoc_button.on_click(on_ad_hoc_clicked)
        box = widgets.VBox([label, adhoc_button])
        boxes.append(box)

        display(widgets.VBox(boxes))

    def create_remove_missing_value_step(self, col, reason):

        final_node = self.pipeline.find_final_node()
        final_step = self.pipeline.get_step(final_node)

        sample_output = self.doc_df.df

        remove_missing_value_step = RemoveMissingValueStep(col=col, reason=reason, sample_df=sample_output)

        def callback(remove_missing_value_step):
            if remove_missing_value_step.explanation != "" or remove_missing_value_step.codes != "":
                rename_step = self.doc_df.rename_step
                rename_step_idx = self.pipeline.steps.index(rename_step)
                rename_step_name = self.pipeline.nodes[rename_step_idx]
                self.pipeline.add_step_right_after(remove_missing_value_step, rename_step_name)
            clear_output(wait=True)
            self.display()

        callbackfunc = callback
        
        remove_missing_value_step.edit_widget(callbackfunc=callbackfunc)


    def create_regex_step(self, col, reason, unique_values):

        final_node = self.pipeline.find_final_node()
        final_step = self.pipeline.get_step(final_node)

        sample_output = self.doc_df.df

        regex_step = RegexTransformationStep(col=col, unique_values=unique_values, reason=reason, sample_df=sample_output)

        def callback(regex_step):
            if regex_step.explanation != "" or regex_step.codes != "":
                rename_step = self.doc_df.rename_step
                rename_step_idx = self.pipeline.steps.index(rename_step)
                rename_step_name = self.pipeline.nodes[rename_step_idx]
                self.pipeline.add_step_right_after(regex_step, rename_step_name)
            clear_output(wait=True)
            self.display()

        callbackfunc = callback

        regex_step.edit_widget(callbackfunc=callbackfunc)

    def create_ad_hoc_step(self):

        final_node = self.pipeline.find_final_node()
        final_step = self.pipeline.get_step(final_node)
        sample_output = final_step.get_sample_output()

        add_hoc_step = TransformationStep(name="Ad hoc", sample_df=sample_output)

        def callback(add_hoc_step):
            if add_hoc_step.explanation != "" or add_hoc_step.codes != "":
                add_hoc_step.rename_based_on_explanation()
                self.pipeline.add_step_to_final(add_hoc_step)
            clear_output(wait=True)
            self.display()

        callbackfunc = callback

        add_hoc_step.edit_widget(callbackfunc=callbackfunc)
    
    def print_codes(self):
        self.pipeline.print_codes()

    def generate_pipeline(self):
        return self.pipeline

def replace_nan(df):
    for col in df.columns:
        df[col] = df[col].fillna("")
    return df

def embed_string(string, engine=None):

    try:
        response = call_embed(string)
        embeddings = response['data'][0]['embedding']
        return embeddings
    except Exception as e:
        print(f"An error occurred while embedding: {e}")
        return None

def initialize_output_csv(df, output_csv_address, label='label'):
    try:
        df_output = pd.read_csv(output_csv_address)
    except FileNotFoundError:
        df_output = get_unique_labels_with_ids(df, label='label')
        df_output.to_csv(output_csv_address, index=False)
    return df_output

def find_first_nan_index(df, column_name):
    """
    Finds the first index in a DataFrame column that is NaN.
    
    :param df: pandas DataFrame object
    :param column_name: String name of the column to search for NaN
    :return: The index of the first NaN value, or None if no NaN found
    """
    nan_index = df[column_name].isna().idxmax()
    if pd.isna(df[column_name][nan_index]):
        return nan_index
    else:
        return None

def embed_labels(df, output_csv_address, chunk_size=1000, label='label'):
    df_output = initialize_output_csv(df, output_csv_address, label='label')

    start_index = find_first_nan_index(df_output, 'embedding')

    if start_index is None:
        print("All labels already embedded.")
        return df_output

    pbar = tqdm(total=len(df_output), desc="Embedding Labels", unit="label")

    pbar.update(start_index)

    for chunk_start in range(start_index, len(df_output), chunk_size):
        chunk_end = min(chunk_start + chunk_size, len(df_output))
        labels_chunk = df_output[label][chunk_start:chunk_end]

        for i, label_value in enumerate(labels_chunk):
            if pd.isna(df_output.at[chunk_start + i, 'embedding']):
                embeddings = embed_string(label_value)
                if embeddings is not None:
                    df_output.at[chunk_start + i, 'embedding'] = embeddings
            pbar.update(1)

        df_output.to_csv(output_csv_address, index=False)

    pbar.close()
    
    print("All labels embedded and CSV updated.")

    return df_output

def get_unique_labels(df, label='label'):
    unique_labels = df[label].dropna().unique()
    return unique_labels


def parse_json_col(df, col='embedding'):
    df[col] = df[col].apply(ast.literal_eval)
    return df


def load_embedding(df, label_embedding='embedding', dim=1536):
    
    if not isinstance(df[label_embedding].iloc[0], list):
        df = parse_json_col(df, col=label_embedding)

    embeddings_array = np.array(list(df[label_embedding]), dtype=np.float32)
    index = faiss.IndexFlatL2(dim)
    index.add(embeddings_array)
    return index

def adhoc_search(string, index, topk=10):
    adhoc_embed = embed_string(string)
    D, I = index.search(np.array([adhoc_embed], dtype=np.float32), topk) 
    return D, I

def df_search(df, index, label_embedding='embedding', topk=10):

    if not isinstance(df[label_embedding].iloc[0], list):
        df = parse_json_col(df, col=label_embedding)

    embeddings = list(df[label_embedding])
    D, I = index.search(np.array(embeddings, dtype=np.float32), topk) 
    return D, I

def flatten_append(df, std_df, D, I, topk=10):
    df = df.loc[df.index.repeat(topk)].reset_index(drop=True)
    df['D_values'] = D.flatten()
    df['I_values'] = I.flatten()
    df['rank'] = (df.index % topk) + 1

    for col in std_df.columns:
        df[col] = std_df.iloc[df['I_values'].values][col].values

    return df

def display_matches(reference_df, 
                    input_df, 
                    I, 
                    exclude_columns=['label', 'index_ids', 'embedding'],
                    label_col = 'label'):

    
    current_page = 0

    def create_html_content(page_no):
        label = input_df[label_col][page_no]
        results_indices = I[page_no]
        results = reference_df.iloc[results_indices][label_col]
        
        results_html = "<ul>" + "".join(f"<li>{result}</li>" for result in results) + "</ul>"
        html_content = f"<b>Input Label</b>: {df_row_to_column_value(input_df, idx=page_no, exclude_columns=exclude_columns).to_html()}<br><b>Top 10 Matches</b>:{results_html}"
        return html_content

    def update_html_display(page_no):
        html_display.value = create_html_content(page_no)
        page_label.value = f'Page {page_no + 1} of {len(input_df)}'
    
    def on_prev_clicked(b):
        nonlocal current_page
        if current_page > 0:
            current_page -= 1
            update_html_display(current_page)

    def on_next_clicked(b):
        nonlocal current_page
        if current_page < len(input_df) - 1:
            current_page += 1
            update_html_display(current_page)
    
    html_display = widgets.HTML(value=create_html_content(current_page))
    
    btn_prev = widgets.Button(description='Previous Page')
    btn_next = widgets.Button(description='Next Page')
    
    btn_prev.on_click(on_prev_clicked)
    btn_next.on_click(on_next_clicked)

    page_label = widgets.Label(value=f'Page {current_page + 1} of {len(input_df)}')
    
    navigation_bar = widgets.HBox([btn_prev, page_label, btn_next])
    
    display(navigation_bar, html_display)


def check_functional_dependency(df, determinant, dependent):
    groups = df.groupby(list(determinant))[dependent].nunique()
    is_functionally_dependent = (groups == 1).all()
    return is_functionally_dependent


def get_unique_labels_with_ids(df, label = 'label'):
    df_cleaned = df.dropna(subset=['label'])

    grouped = df_cleaned.groupby('label')

    label_id_df = grouped.apply(lambda x: x.index.tolist()).reset_index(name='index_ids')

    dependent_columns = []

    for column in df_cleaned.columns:
        if column != 'label':
            if grouped[column].nunique().eq(1).all():
                dependent_columns.append(column)
    
    if dependent_columns:
        attributes_data = grouped[dependent_columns].first().reset_index()
        label_id_df = pd.merge(label_id_df, attributes_data, on='label', how='left')

    label_id_df['embedding'] = None

    return label_id_df


def df_row_to_column_value(df, idx=0, exclude_columns=[]):
    """
    Create a DataFrame of [column, value] pairs from a specified row index,
    excluding specified columns.

    Parameters:
    - df: The input DataFrame.
    - idx: The index of the row to use.
    - exclude_columns: A set or list of columns to exclude.

    Returns:
    - A new DataFrame with two columns: 'Column' and 'Value'.
    """

    df_reduced = df.drop(columns=exclude_columns, errors='ignore')

    if idx not in df_reduced.index:
        raise IndexError(f"Index {idx} is out of bounds for the DataFrame.")

    row_transposed = df_reduced.iloc[idx].transpose()

    new_df = pd.DataFrame(row_transposed)
    new_df.reset_index(inplace=True)
    new_df.columns = ['Column', 'Value']

    return new_df

def extract_json_code_safe(s):
    s_stripped = s.strip()
    if (s_stripped.startswith('{') and s_stripped.endswith('}')) or \
       (s_stripped.startswith('[') and s_stripped.endswith(']')):
        return s_stripped
    return extract_json_code(s_stripped)

def extract_json_code(s):
    import re
    pattern = r"```json(.*?)```"
    match = re.search(pattern, s, re.DOTALL)
    return match.group(1).strip() if match else None


def entity_relation_match(input_df, I, refernece_df, attributes=None, label = "label", match="matches"):
    
    if match not in input_df:
        input_df[match] = None

    if attributes is None:
        attributes = input_df.columns.tolist()
        attributes.remove("label")
        attributes.remove("index_ids")
        attributes.remove("embedding")

    for idx in range(len(input_df)):
        print(f" Working on the row {idx+1} ...")

        if input_df[match][idx] is not None:
            continue

        input_desc = ""
        for attribute in attributes:
            input_desc += (attribute + ": " + input_df.iloc[idx][attribute] + "\n")

        refernece_desc = ""
        for i, output in enumerate(refernece_df[label].iloc[I[idx]]): 
            refernece_desc += (str(i+1) + ". " + output + "\n")

        template = f"""Your goal is to build relations between input and reference entities.

The input entity has the following attributes:
{input_desc}
Below are reference entities:
{refernece_desc}
Do the following:
1. Read input entity attributes and guess what it is about.

2. Go through each output entity. Describe what it is and reason its relation.
For instance, given the input entity "small car":
if the same entity then EXACT_MATCH. 
    E.g., "small automobile"
else if has assumptions that is clearly wrong then CONFLICTED_ASSUMPTION
    E.g., "big car" is wrong because input entity clearly specify size as "small"
else if additional assumptions that can't be verified then ADDITIONAL_ASSUMPTION
    E.g., "electronic car" is additional battery assumption can't be verified
else if it is general super class then GENERAL 
    E.g., "small vehicle" and "car" are general category of "small car"
else it is irrelavent entity then NOT_RELATED
    E.g., "cloth" is a irrelavent

The list is prioritized. Choose the first one that applies."""

        messages = [{"role": "user", "content": template}]

        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        

        reponded_content = response['choices'][0]['message']['content']

        messages=[
                {"role": "user", 
                "content": template
                },
                {"role": "assistant",
                "content": reponded_content
                },
                {"role": "user", 
"content": """First, criticize your answer and point out the mistakes.
For output entities classifed as EXACT_MATCH, are they making additional, potentially incorrect, or fewer assumptions? If so, correct them as ADDITIONAL_ASSUMPTION/CONFLICTED_ASSUMPTION/GENERAL. 
For output entities classifed as GENERAL, are they making additional, potentially incorrect assumptions? If so, correct them as ADDITIONAL_ASSUMPTION/CONFLICTED_ASSUMPTION.
For output entities classifed as NOT_RELATED, are the entities similar but only part of the properties are different? If so, correct them as CONFLICTED_ASSUMPTION.
Next, provide your corrected answer as json:
```json
{       
        "Input Entity Guess": "...",
        "EXACT_MATCH": {
                "entity": [...],
                "reason": "The input entity is ... which matches ..."
        },
        "CONFLICTED_ASSUMPTION": {
                "entity": [...],
                "reason": "The (what specific) details are conflicted"
        },
        "ADDITIONAL_ASSUMPTION": {
                "entity": [...],
                "reason": "The (what specific) details are not mentioned"
        },
        "GENERAL": {
                "entity": [...],
                "reason": "..."
        }
        (DON'T include NOT_RELATED!)
}
```
If no matched entity, return empty entity list and reason string.
"""},]

        response = call_gpt4(messages, temperature=0.1, top_p=0.1)

        json_code = extract_json_code_safe(response['choices'][0]['message']['content'])
        json_var = json.loads(json_code)
        input_df.at[idx, match] = json_var


def generate_html_from_json_entity(json_var):
    html_output = f"<p>&#x1F913; <b>This input entity is about:</b> <i>{json_var['Input Entity Guess']}</i><p>"
    if json_var['EXACT_MATCH']['entity']:
        html_output += "<p>&#x1F600; We find <u>exactly matched</u> entities:</p>"
        html_output += "<ul>"
        for entity in json_var['EXACT_MATCH']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['EXACT_MATCH']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['EXACT_MATCH']['reason']}</i></p>"
        return html_output + "</body></html>"
    
    html_output += "<p>&#x1F641; We can't find <u>exactly matched</u> entities.</p>"

    if json_var['GENERAL']['entity']:
        html_output += "<p>&#x1F600; We find entities that are <u>more general</u>:</p>"
        html_output += "<ul>"
        for entity in json_var['GENERAL']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['GENERAL']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['GENERAL']['reason']}</i></p>"

    if json_var['ADDITIONAL_ASSUMPTION']['entity']:
        html_output += "<p>&#x1F600; We find entities but with <u>additional details</u> that need verification:</p>"
        html_output += "<ul>"
        for entity in json_var['ADDITIONAL_ASSUMPTION']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['ADDITIONAL_ASSUMPTION']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['ADDITIONAL_ASSUMPTION']['reason']}</i></p>"

    if json_var['CONFLICTED_ASSUMPTION']['entity']:
        html_output += "<p>&#x1F600; We find entities that are <u>similar, but details are different</u>:</p>"
        html_output += "<ul>"
        for entity in json_var['CONFLICTED_ASSUMPTION']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['CONFLICTED_ASSUMPTION']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['CONFLICTED_ASSUMPTION']['reason']}</i></p>"

    return html_output


def generate_entity_page(df, i=0, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col="matches"):

    if isinstance(df[match_col].iloc[0], str):
        df = parse_json_col(df, col=match_col)
    
    input_data_html = df_row_to_column_value(df, idx=i, exclude_columns=exclude_columns).to_html(index=False)
    json_var = df[match_col][i]
    solution_output_html = generate_html_from_json_entity(json_var)

    return f"""<h1>Input Data</h1>
{input_data_html}
<h1>Match Result</h1>
{solution_output_html}"""

def display_entity_matches(df, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col="matches"):
    if isinstance(df[match_col].iloc[0], str):
        df = parse_json_col(df, col=match_col)

    current_page = 0

    def create_html_content(page_no):
        return generate_entity_page(df, page_no, exclude_columns, match_col)

    def update_html_display(page_no):
        html_display.value = create_html_content(page_no)
        page_label.value = f'Page {page_no + 1} of {len(df)}'
    
    def on_prev_clicked(b):
        nonlocal current_page
        if current_page > 0:
            current_page -= 1
            update_html_display(current_page)

    def on_next_clicked(b):
        nonlocal current_page
        if current_page < len(df) - 1:
            current_page += 1
            update_html_display(current_page)
    
    html_display = widgets.HTML(value=create_html_content(current_page))
    
    btn_prev = widgets.Button(description='Previous Page')
    btn_next = widgets.Button(description='Next Page')
    
    btn_prev.on_click(on_prev_clicked)
    btn_next.on_click(on_next_clicked)

    page_label = widgets.Label(value=f'Page {current_page + 1} of {len(df)}')
    
    navigation_bar = widgets.HBox([btn_prev, page_label, btn_next])
    
    display(navigation_bar, html_display)

def generate_html_from_json(json_var):
    html_output = f"<p>&#x1F913; <b>This input entity is about:</b> <i>{json_var['Input Entity Guess']}</i><p>"
    if json_var['EXACT_MATCH']['entity']:
        html_output += "<p>&#x1F600; We find <u>exactly matched</u> entities:</p>"
        html_output += "<ul>"
        for entity in json_var['EXACT_MATCH']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['EXACT_MATCH']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['EXACT_MATCH']['reason']}</i></p>"
        return html_output + "</body></html>"
    
    html_output += "<p>&#x1F641; We can't find <u>exactly matched</u> entities.</p>"

    if json_var['GENERAL']['entity']:
        html_output += "<p>&#x1F600; We find entities that are <u>more general</u>:</p>"
        html_output += "<ul>"
        for entity in json_var['GENERAL']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['GENERAL']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['GENERAL']['reason']}</i></p>"

    if json_var['ADDITIONAL_ASSUMPTION']['entity']:
        html_output += "<p>&#x1F600; We find entities but with <u>additional details</u> that need verification:</p>"
        html_output += "<ul>"
        for entity in json_var['ADDITIONAL_ASSUMPTION']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['ADDITIONAL_ASSUMPTION']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['ADDITIONAL_ASSUMPTION']['reason']}</i></p>"

    if json_var['CONFLICTED_ASSUMPTION']['entity']:
        html_output += "<p>&#x1F600; We find entities that are <u>similar, but details are different</u>:</p>"
        html_output += "<ul>"
        for entity in json_var['CONFLICTED_ASSUMPTION']['entity']:
            html_output += f"<li>{entity}</li>"
        html_output += "</ul>"
        if json_var['CONFLICTED_ASSUMPTION']['reason']:
            html_output += f"<p><b>Reason:</b> <i>{json_var['CONFLICTED_ASSUMPTION']['reason']}</i></p>"

    return html_output



def generate_page(df, i=0, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):
    input_data_html = df_row_to_column_value(df, idx=i, exclude_columns=exclude_columns).to_html(index=False)
    json_var = df[match_col][i]
    solution_output_html = generate_html_from_json(json_var)

    return f"""<h1>Input Data</h1>
{input_data_html}
<h1>Match Result</h1>
{solution_output_html}"""

def write_report_to_file(df, output_html_address, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):

    full_html = generate_report(df, exclude_columns=exclude_columns)

    with open(output_html_address, 'w') as f:
        f.write(full_html)

def print_report(df, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):
    
    full_html = generate_report(df, exclude_columns=exclude_columns)

    display(HTML(full_html))

def generate_report(df, exclude_columns=['label', 'index_ids', 'embedding','matches'], match_col='matches'):
    middle_html = ""

    for i in range(len(df)):
        input_data_html = df_row_to_column_value(df, idx=i, exclude_columns=exclude_columns).to_html(index=False)
        json_var = df[match_col][i]
        solution_output_html = generate_html_from_json(json_var)

        middle_html += f"""<h1>{i+1}</h1>
{generate_page(df, i, exclude_columns)}"""

    title_html = f'<div style="display: flex; align-items: center;">' \
        f'<img src="data:image/png;base64,{cocoon_icon_64}" alt="cocoon icon" width=50 style="margin-right: 10px;">' \
        f'<div style="margin: 0; padding: 0;">' \
        f'<h1 style="margin: 0; padding: 0;">Table Standardization</h1>' \
        f'<p style="margin: 0; padding: 0;">Powered by cocoon</p>' \
        f'</div>' \
        f'</div><hr>'


    full_html = f"""<!DOCTYPE html>
<html>
<head>
    <title>Cocoon Standardization</title>
    <head>
        <style>
    /* CSS for the overall appearance: fonts, spacing, etc. */
    body {{
        font-family: "Arial";
        margin: 40px;
        background-color: #d9ead3;  /* Matching the light green background of the icon */
    }}

    .container {{
        max-width: 900px;  /* Setting a maximum width for the content */
        margin: 0 auto;   /* Centering the container */
        background-color: #ffffff; /* White background for the main content */
        padding: 20px;
        border-radius: 5px; /* Rounded corners */
        box-shadow: 0 0 10px rgba(0,0,0,0.1); /* A subtle shadow to lift the container */
    }}

    h1 {{
        font-size: 24px;
        color: #274e13; /* Dark green color matching the icon */
        padding-bottom: 10px;
        margin-bottom: 20px;
    }}

    h2 {{
        font-size: 20px;
        margin-top: 20px;
        margin-bottom: 15px;
    }}

    ul, ol {{
        padding-left: 20px;
    }}

    li {{
        margin-bottom: 10px;    }}

    p {{        margin-bottom: 20px;
        text-align: justify; /* To align the text on both sides */
    }}

    /* CSS specific to the table elements */
    table {{
        font-family: Arial, sans-serif;
        border-collapse: collapse; /* Ensures there are no double borders */
        width: 100%;
        table-layout: auto;
    }}

    th, td {{
        border: 1px solid #dddddd; /* Light grey borders */
        text-align: left;
        padding: 8px; /* Make text not touch the borders */
    }}

    th {{
        background-color: #b6d7a8; /* Light green background matching the icon */
    }}

    tr:nth-child(even) {{
        background-color: #d9ead3; /* Light green background matching the icon */
    }}

    tr:hover {{
        background-color: #c9d9b3; /* A slightly darker green for hover effect */
    }}
    </style>
</head>
<body>
    
    <div class="container">
        {title_html}
        {middle_html}
    </div>
</body>
</html>
"""
    return full_html